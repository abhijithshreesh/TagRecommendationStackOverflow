,Body,body_without_stop_words,stemmed_words,Tags,New_Tags
0,"<p>What does ""backprop"" mean? I've Googled it, but it's showing backpropagation.</p>

<p>Is the ""backprop"" term basically the same as ""backpropagation"" or does it have a different meaning?</p>
", backprop  mean  googled  showing backpropagation    backprop  term basically  backpropagation  different meaning  ,backprop mean googl show backpropag backprop term basic backpropag differ mean,"neural-networks,definitions,terminology","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,backpropagation,image-recognition,tensorflow,algorithm,training,ai-design,natural-language,classification,game-ai,computer-vision"
1,"<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>
",increasing noise data help improve learning ability network  make difference depend problem solved  affect generalization process overall  ,increas nois data help improv learn abil network make differ depend problem solv affect general process overal,generalization,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,classification,tensorflow,image-recognition,game-ai,algorithm,genetic-algorithms,deep-network,computer-vision"
3,"<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>
",writing algorithm  know many neurons need per single layer  methods finding optimal number  rule thumb  ,write algorithm know mani neuron need per singl layer method find optim number rule thumb,"deep-network,neurons","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,algorithm,reinforcement-learning,ai-design,classification,game-ai,tensorflow,image-recognition,genetic-algorithms,backpropagation,recurrent-neural-networks,artificial-neuron"
4,"<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p>

<p>Is this possible?</p>

<hr>

<p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=""http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/"" rel=""nofollow"">this</a>, but don't understand it. </p>
",lego mindstorms ev  wondering way could start coding bot python rather default drag drop system  mindstorm considered ai   possible     goal write basic walking program python  bot ev rstorm  searched found  understand   ,lego mindstorm ev wonder way could start code bot python rather default drag drop system mindstorm consid ai possibl goal write basic walk program python bot ev rstorm search found understand,mindstorms,"machine-learning,neural-networks,deep-learning,ai-design,algorithm,reinforcement-learning,game-ai,convolutional-neural-networks,philosophy,tensorflow,image-recognition,natural-language,classification,genetic-algorithms,nlp"
5,"<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=""http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition"" rel=""nofollow"">Wikipedia</a>)</p>

<p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p>
",intelligent agent definition intelligence states agent intelligent acts maximize expected value performance measure based past experience knowledge   paraphrased wikipedia   mean humans intelligent  think make mistakes imply maximizing expected value performance measure  ,intellig agent definit intellig state agent intellig act maxim expect valu perform measur base past experi knowledg paraphras wikipedia mean human intellig think make mistak impli maxim expect valu perform measur,"philosophy,intelligent-agent,terminology","neural-networks,machine-learning,deep-learning,reinforcement-learning,ai-design,philosophy,game-ai,tensorflow,convolutional-neural-networks,multi-agent-systems,game-theory,q-learning,algorithm,image-recognition,intelligent-agent"
6,"<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>

<blockquote>
  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>
</blockquote>

<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>

<p>What are the adverse consequences of the so called <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow"">Technological Singularity</a>? </p>
",quote stephen hawking headlines quite time      artificial intelligence could wipe humanity gets clever humans like ants    say  put simply layman terms  possible threats ai  know ai dangerous still promoting  banned   adverse consequences called technological singularity   ,quot stephen hawk headlin quit time artifici intellig could wipe human get clever human like ant say put simpli layman term possibl threat ai know ai danger still promot ban advers consequ call technolog singular,intelligent-agent,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,strong-ai,convolutional-neural-networks,algorithm,image-recognition,game-theory,training,research,tensorflow"
9,"<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>
",new  like know simple words  fuzzy logic concept  help  used  ,new like know simpl word fuzzi logic concept help use,"deep-network,fuzzy-logic","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,natural-language,algorithm,training,tensorflow,game-ai,classification,image-recognition,nlp,backpropagation"
12,"<p>In particular, an embedded computer (limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p>

<p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but ""higher than wider"", with the registration split over two rows.</p>

<p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p>

<p>Due to the limited resources and need for rapid, realtime processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p>

<p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p>
",particular  embedded computer  limited resources  analyzes live video stream traffic camera  trying pick good frames contain license plate numbers passing cars  plate located  frame handed ocr library extract registration use   country two types license plates common use   rectangular  typical  square   actually  somewhat rectangular  higher wider   registration split two rows    types  let us disregard  small percent usually belong vehicles lie outside interest    due limited resources need rapid  realtime processing  maximum size network  number cells connections  system handle fixed   would better split two smaller networks  recognizing one type registration plates  larger single network handle two types better  ,particular embed comput limit resourc analyz live video stream traffic camera tri pick good frame contain licens plate number pass car plate locat frame hand ocr librari extract registr use countri two type licens plate common use rectangular typic squar actual somewhat rectangular higher wider registr split two row type let us disregard small percent usual belong vehicl lie outsid interest due limit resourc need rapid realtim process maximum size network number cell connect system handl fix would better split two smaller network recogn one type registr plate larger singl network handl two type better,"neural-networks,image-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,game-ai,algorithm,image-recognition,tensorflow,classification,training,genetic-algorithms,computer-vision,backpropagation"
14,"<p>The <a href=""https://en.wikipedia.org/wiki/Turing_test"">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=""https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">artificial general intelligence</a> (strong AI)?</p>
",turing test first test artificial intelligence bit outdated  total turing test aims modern test requires much sophisticated system  techniques use identify artificial intelligence  weak ai  artificial general intelligence  strong ai   ,ture test first test artifici intellig bit outdat total ture test aim modern test requir much sophist system techniqu use identifi artifici intellig weak ai artifici general intellig strong ai,"turing-test,strong-ai,intelligent-agent,weak-ai","machine-learning,neural-networks,deep-learning,ai-design,philosophy,strong-ai,game-ai,convolutional-neural-networks,reinforcement-learning,tensorflow,classification,algorithm,research,training,image-recognition"
15,"<p>What is the ""early stopping"" and what are the advantages using this method? How does it help exactly.</p>
", early stopping  advantages using method  help exactly  ,earli stop advantag use method help exact,"generalization,definitions","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,game-ai,algorithm,tensorflow,training,classification,image-recognition,backpropagation,natural-language,linear-regression"
16,"<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>
",heard idea technological singularity  relate artificial intelligence   theoretical point artificial intelligence machines progressed point grow learn beyond humans growth takes   would know reach point  ,heard idea technolog singular relat artifici intellig theoret point artifici intellig machin progress point grow learn beyond human growth take would know reach point,"self-learning,singularity","machine-learning,neural-networks,deep-learning,ai-design,philosophy,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,image-recognition,self-learning,genetic-algorithms,singularity,strong-ai,definitions"
20,"<p>I'm worrying that my network has become too complex. I don't want to end up with half of the network doing nothing but just take up space and resources.</p>

<p>So, what are the techniques for detecting and preventing overfitting to avoid such problems?</p>
",worrying network become complex  want end half network nothing take space resources    techniques detecting preventing overfitting avoid problems  ,worri network becom complex want end half network noth take space resourc techniqu detect prevent overfit avoid problem,"deep-network,overfitting,optimization","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,image-recognition,tensorflow,algorithm,classification,training,game-ai,genetic-algorithms,deep-network,backpropagation"
25,"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  </p>

<ol>
<li><p>What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  </p></li>
<li><p>Are there examples where this is already happening to a degree today?  </p></li>
<li><p>Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  </p>

<p>Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p></li>
</ol>
",seen emotional intelligence defined capacity aware  control  express one emotions  handle interpersonal relationships judiciously empathetically      strategies artificial intelligence begin tackle problem develop emotional intelligence computers    examples already happening degree today    computer passes turing test necessarily express emotional intelligence would seen obvious computer     perhaps early programs pass test represented young people  presumably lower emotional intelligence   ,seen emot intellig defin capac awar control express one emot handl interperson relationship judici empathet strategi artifici intellig begin tackl problem develop emot intellig comput exampl alreadi happen degre today comput pass ture test necessarili express emot intellig would seen obvious comput perhap earli program pass test repres young peopl presum lower emot intellig,"turing-test,emotional-intelligence","neural-networks,machine-learning,deep-learning,ai-design,philosophy,convolutional-neural-networks,algorithm,reinforcement-learning,image-recognition,game-ai,classification,training,genetic-algorithms,definitions,strong-ai"
27,"<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>
",since human intelligence presumably function natural genetic algorithm nature  using genetic algorithm computer example artificial intelligence    differ   perhaps expressing artificial intelligence depending upon scale algorithm evolves  ,sinc human intellig presum function natur genet algorithm natur use genet algorithm comput exampl artifici intellig differ perhap express artifici intellig depend upon scale algorithm evolv,"self-learning,genetic-algorithms","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,philosophy,algorithm,convolutional-neural-networks,genetic-algorithms,game-ai,image-recognition,training,classification,nlp,strong-ai"
33,"<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>
",two terms seem related  especially application computer science software engineering   one subset another   one tool used build system   differences significant  ,two term seem relat especi applic comput scienc softwar engin one subset anoth one tool use build system differ signific,"machine-learning,terminology","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,game-ai,algorithm,image-recognition,classification,training,tensorflow,computer-vision,backpropagation,philosophy"
34,"<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>
",aspects quantum computers   help develop artificial intelligence  ,aspect quantum comput help develop artifici intellig,quantum-computing,"machine-learning,neural-networks,philosophy,deep-learning,ai-design,strong-ai,algorithm,definitions,image-recognition,game-ai,reinforcement-learning,convolutional-neural-networks,training,nlp,research"
35,"<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>
",believe markov chain sequence events subsequent event depends probabilistically current event   examples application markov chain used create artificial intelligence   would genetic algorithm example markov chain since generation depends upon state prior generation  ,believ markov chain sequenc event subsequ event depend probabilist current event exampl applic markov chain use creat artifici intellig would genet algorithm exampl markov chain sinc generat depend upon state prior generat,"genetic-algorithms,markov-chain,probabilistic","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,game-ai,genetic-algorithms,training,philosophy,classification,image-recognition,tensorflow,natural-language"
38,"<p>What purpose does the ""dropout"" method serve and how does it improve the overall performance of the neural network?</p>
",purpose  dropout  method serve improve overall performance neural network  ,purpos dropout method serv improv overal perform neural network,"deep-network,overfitting,performance","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,classification,training,ai-design,backpropagation,game-ai,recurrent-neural-networks,tensorflow,genetic-algorithms,deep-network,image-recognition"
39,"<p>Can an AI program have an IQ?</p>

<p>In other words, can the IQ of an AI program be measured?</p>

<p>Like how humans can do an IQ test.</p>
",ai program iq   words  iq ai program measured   like humans iq test  ,ai program iq word iq ai program measur like human iq test,intelligence-testing,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,natural-language,strong-ai,algorithm,nlp,game-theory,research,tensorflow,reinforcement-learning,convolutional-neural-networks"
40,"<p>Why anybody would want to use the ""hidden layers""? How they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>
",anybody would want use  hidden layers   enhance learning ability network comparison network  linear models   ,anybodi would want use hidden layer enhanc learn abil network comparison network linear model,hidden-layers,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,tensorflow,ai-design,classification,game-ai,training,image-recognition,backpropagation,keras,algorithm,genetic-algorithms"
44,"<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>
",research artificial intelligence first begin   called artificial intelligence another name  ,research artifici intellig first begin call artifici intellig anoth name,history,"machine-learning,neural-networks,philosophy,deep-learning,ai-design,reinforcement-learning,strong-ai,definitions,research,singularity,convolutional-neural-networks,tensorflow,ai-community,algorithm,image-recognition"
48,"<p>How would you estimate the generalisation error? What are the methods of achieving this?</p>
",would estimate generalisation error  methods achieving  ,would estim generalis error method achiev,"deep-network,generalization","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,classification,algorithm,training,game-ai,philosophy,backpropagation,game-theory,image-recognition,genetic-algorithms"
50,"<p>I've implemented <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow"">the reinforcement learning alogrithm</a> for an agent to play <a href=""https://github.com/admonkey/snappybird"" rel=""nofollow"">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p>

<p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the nnet on an existing q-table would work, but I would like to not use a q-table at all if possible.</p>
",implemented reinforcement learning alogrithm agent play snappy bird  shameless cheap ripoff flappy bird  utilizing q table storing history future lookups  works eventually achieves perfect convergence enough training   possible implement neural network function approximation order accomplish purpose q table  obviously storage concern q table  seem ever train neural net alone  perhaps training nnet existing q table would work  would like use q table possible  ,implement reinforc learn alogrithm agent play snappi bird shameless cheap ripoff flappi bird util q tabl store histori futur lookup work eventu achiev perfect converg enough train possibl implement neural network function approxim order accomplish purpos q tabl obvious storag concern q tabl seem ever train neural net alon perhap train nnet exist q tabl would work would like use q tabl possibl,"neural-networks,reinforcement-learning","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,q-learning,ai-design,tensorflow,training,classification,game-ai,image-recognition,algorithm,genetic-algorithms,deep-network"
52,"<p>I read that in the spring of 2016 a computer <a href=""https://en.wikipedia.org/wiki/Computer_Go"" rel=""nofollow"">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>
",read spring      computer go program finally able beat professional human first time   milestone reached  represent significant advance artificial intelligence techniques matter even processing power applied problem   methods used program successful go playing program  methods considered artificial intelligence  ,read spring comput go program final abl beat profession human first time mileston reach repres signific advanc artifici intellig techniqu matter even process power appli problem method use program success go play program method consid artifici intellig,game-theory,"neural-networks,machine-learning,deep-learning,ai-design,philosophy,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,image-recognition,genetic-algorithms,classification,training,tensorflow,natural-language"
56,"<p>Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?</p>
",first coined term artificial intelligence  published research paper first use term  ,first coin term artifici intellig publish research paper first use term,history,"neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,philosophy,image-recognition,game-ai,training,algorithm,classification,research,tensorflow,genetic-algorithms"
58,"<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>

<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>

<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>

<p>What are other issues currently facing AI development?</p>
",background computer engineering working developing better algorithms mimic human thought   one favorites analogical modeling applied language processing decision making   however  research  realize complicated ai   tried tackle many problems field  sometimes find reinventing wheel trying solve problem already proven unsolvable  ie  halting problem    help furthering ai  want better understand current obstacles hindering progress field   example  time space complexity machine learning algorithms super polynomial means even fast computers  take program complete  even still  algorithms may fast desktop computer dealing small data set  increasing size data  algorithm becomes intractable   issues currently facing ai development  ,background comput engin work develop better algorithm mimic human thought one favorit analog model appli languag process decis make howev research realiz complic ai tri tackl mani problem field sometim find reinvent wheel tri solv problem alreadi proven unsolv ie halt problem help ai want better understand current obstacl hinder progress field exampl time space complex machin learn algorithm super polynomi mean even fast comput take program complet even still algorithm may fast desktop comput deal small data set increas size data algorithm becom intract issu current face ai develop,machine-learning,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,image-recognition,tensorflow,philosophy,training,classification,genetic-algorithms,computer-vision"
61,"<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p>

<p>How do you know you need more than 2? For what kind of problems you would need them (as example)?</p>
",read problems solved     hidden layers   know need    kind problems would need  example   ,read problem solv hidden layer know need kind problem would need exampl,"deep-network,hidden-layers","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,algorithm,reinforcement-learning,classification,image-recognition,game-ai,genetic-algorithms,backpropagation,tensorflow,training,recurrent-neural-networks"
62,"<p>What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:</p>

<ol>
<li>Beating a human at the game of chess</li>
<li>Convincing a human that a person was conversing with them (passing the Turing test)</li>
<li>Beating a human at Jeopardy game show</li>
<li>Beating a human at the game of go.</li>
</ol>

<p>Were there milestones that were considered major in the field before the 1990s?</p>
",first areas research artificial intelligence early successes   recently    beating human game chess convincing human person conversing  passing turing test  beating human jeopardy game show beating human game go    milestones considered major field      ,first area research artifici intellig earli success recent beat human game chess convinc human person convers pass ture test beat human jeopardi game show beat human game go mileston consid major field,history,"neural-networks,machine-learning,game-ai,deep-learning,reinforcement-learning,philosophy,ai-design,game-theory,tensorflow,combinatorial-games,convolutional-neural-networks,research,chess,gaming,image-recognition"
65,"<p>Why somebody would use SAT solvers (<a href=""https://en.wikipedia.org/wiki/Boolean_satisfiability_problem"" rel=""nofollow"">Boolean satisfiability problem</a>) to solve their real world problems?</p>

<p>Are there any examples of the real uses of this model?</p>
",somebody would use sat solvers  boolean satisfiability problem  solve real world problems   examples real uses model  ,somebodi would use sat solver boolean satisfi problem solv real world problem exampl real use model,models,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,algorithm,classification,image-recognition,tensorflow,training,game-ai,keras,genetic-algorithms,deep-network"
66,"<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p>
",designs genetic algorithms  classified differently different names  leverage models epigenetics evolution  pros cons designs  vast insufficiencies wide open questions usefulness   ,design genet algorithm classifi differ differ name leverag model epigenet evolut pros con design vast insuffici wide open question use,genetic-algorithms,"machine-learning,neural-networks,deep-learning,reinforcement-learning,convolutional-neural-networks,tensorflow,classification,ai-design,algorithm,image-recognition,training,keras,game-ai,genetic-algorithms,computer-vision"
68,"<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>

<p><a href=""https://youtu.be/py5byOOHZM8?t=815"">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>
",convolutional neural network used pattern recognition problem domain pre existing images  say representing abstract data graphically  would always less efficient   developer says current development could go limit outside image recognition   ,convolut neural network use pattern recognit problem domain pre exist imag say repres abstract data graphic would alway less effici develop say current develop could go limit outsid imag recognit,"deep-network,neural-networks,image-recognition,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,ai-design,reinforcement-learning,training,classification,algorithm,tensorflow,game-ai,computer-vision,genetic-algorithms,keras"
72,"<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>
",heard terms strong ai weak ai used   well defined terms subjective ones   generally defined  ,heard term strong ai weak ai use well defin term subject one general defin,"strong-ai,weak-ai,terminology","machine-learning,neural-networks,ai-design,deep-learning,game-ai,convolutional-neural-networks,reinforcement-learning,philosophy,strong-ai,algorithm,game-theory,tensorflow,training,image-recognition,genetic-algorithms"
73,"<p>As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The ""driver"" (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?</p>

<p>So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?</p>
",ai gains capabilities  becomes prevalent society  legal system encounter questions encountered   example  self driving car involved accident controlled ai  fault    driver   really passenger   programmer  made ai  ai    cutting edge terms kinds issues intersection law artificial intelligence  ,ai gain capabl becom preval societi legal system encount question encount exampl self drive car involv accid control ai fault driver realli passeng programm made ai ai cut edg term kind issu intersect law artifici intellig,legal,"machine-learning,neural-networks,ai-design,philosophy,deep-learning,game-ai,strong-ai,algorithm,reinforcement-learning,training,image-recognition,research,game-theory,genetic-algorithms,convolutional-neural-networks"
75,"<p>I know that language of <strong><code>Lisp</code></strong> was used early on when working on artificial intelligence problems.  Is it still being used today for significant work?  If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>
",know language lisp used early working artificial intelligence problems   still used today significant work    new language taken place common one used work ai today  ,know languag lisp use earli work artifici intellig problem still use today signific work new languag taken place common one use work ai today,"history,programming-languages,lisp","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,game-ai,algorithm,natural-language,tensorflow,image-recognition,training,classification,philosophy,genetic-algorithms"
77,"<p>What are the specific requirements of the Turing Test?</p>

<ul>
<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>
<li>Must there always be two participants to the conversation (one human and one computer) or can there be more</li>
<li>Are placebo tests (where there is not actually a computer involbed) allowed or encouraged?</li>
<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>
</ul>
",specific requirements turing test    requirements must evaluator fulfill order qualified give test  must always two participants conversation  one human one computer  placebo tests  actually computer involbed  allowed encouraged  multiple evaluators  decision need unanimous among evaluators order machine passed test   ,specif requir ture test requir must evalu fulfil order qualifi give test must alway two particip convers one human one comput placebo test actual comput involb allow encourag multipl evalu decis need unanim among evalu order machin pass test,turing-test,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,classification,game-ai,algorithm,tensorflow,image-recognition,philosophy,training,game-theory,python"
78,"<p>I believe that statistical AI uses inductive thought processes.  For example, deducing a trend from a pattern.  What are some examples of successfully applying statistical AI to real world problems.</p>
",believe statistical ai uses inductive thought processes   example  deducing trend pattern   examples successfully applying statistical ai real world problems  ,believ statist ai use induct thought process exampl deduc trend pattern exampl success appli statist ai real world problem,statistical-ai,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,convolutional-neural-networks,reinforcement-learning,algorithm,philosophy,image-recognition,strong-ai,classification,training,genetic-algorithms,tensorflow"
79,"<p>How do the basic components <a href=""https://en.wikipedia.org/wiki/Optimality_theory"" rel=""nofollow"">optimality theory</a> apply to artificial intelligence?</p>

<p>How is optimality theory related to neural network research?</p>
",basic components optimality theory apply artificial intelligence   optimality theory related neural network research  ,basic compon optim theori appli artifici intellig optim theori relat neural network research,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,philosophy,training,classification,algorithm,tensorflow,game-ai,research,backpropagation,recurrent-neural-networks"
81,"<p>Some programs do exhaustive searches for a solution while others do heuristic searches.  For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas in go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>

<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI?  If so, is the chess playing computer beating a human professional seen as a meaningful milestone?</p>
",programs exhaustive searches solution others heuristic searches   example  chess  search best next move tends exhaustive nature whereas go  search best next move tends heuristic nature due much larger search space   technique brute force exhaustive searching good answer considered ai generally required heuristic algorithms used deemed ai    chess playing computer beating human professional seen meaningful milestone  ,program exhaust search solut heurist search exampl chess search best next move tend exhaust natur wherea go search best next move tend heurist natur due much larger search space techniqu brute forc exhaust search good answer consid ai general requir heurist algorithm use deem ai chess play comput beat human profession seen meaning mileston,"gaming,search,chess,heuristics","machine-learning,neural-networks,deep-learning,ai-design,game-ai,reinforcement-learning,algorithm,philosophy,convolutional-neural-networks,genetic-algorithms,training,image-recognition,combinatorial-games,tensorflow,chess"
83,"<p>How is a neural network having the ""deep"" adjective actually distinguished from other similar networks?</p>
",neural network  deep  adjective actually distinguished similar networks  ,neural network deep adject actual distinguish similar network,"neural-networks,deep-network,comparison","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,classification,tensorflow,training,backpropagation,ai-design,deep-network,recurrent-neural-networks,game-ai,image-recognition,genetic-algorithms"
85,"<p>What is the effectiveness of pre-training of unsupervised deep learning?</p>

<p>Does unsupervised deep learning actually work?</p>
",effectiveness pre training unsupervised deep learning   unsupervised deep learning actually work  ,effect pre train unsupervis deep learn unsupervis deep learn actual work,"deep-learning,unsupervised-learning","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,tensorflow,training,classification,ai-design,q-learning,game-ai,deep-network,keras,image-recognition,self-learning"
87,"<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? Is this considered AI or just smart?</p>
",search engines considered ai way analyze search remember  send ads searched recently  considered ai smart  ,search engin consid ai way analyz search rememb send ad search recent consid ai smart,search,"machine-learning,neural-networks,ai-design,game-ai,deep-learning,philosophy,algorithm,strong-ai,reinforcement-learning,genetic-algorithms,game-theory,research,convolutional-neural-networks,ai-community,combinatorial-games"
88,"<p>The following <a href=""http://www.evolvingai.org/fooling"" rel=""noreferrer"">page</a>/<a href=""http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf"" rel=""noreferrer"">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>

<p><a href=""https://i.stack.imgur.com/7pgrH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7pgrH.jpg"" alt=""Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images""></a></p>

<p><a href=""https://i.stack.imgur.com/pBm48.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pBm48.png"" alt=""Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels""></a></p>

<p>How this is possible? Can you please explain ideally in plain English?</p>
",following page study demonstrates deep neural networks easily fooled giving high confidence predictions unrecognisable images  e g       possible  please explain ideally plain english  ,follow page studi demonstr deep neural network easili fool give high confid predict unrecognis imag e g possibl pleas explain ideal plain english,"convolutional-neural-networks,image-recognition,deep-network,computer-vision","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,classification,ai-design,tensorflow,training,computer-vision,game-ai,algorithm,deep-network,keras"
90,"<p>In a feedforward neural network the inputs are fed directly to the outputs via a series of <strong>weights</strong>.</p>

<p>What purpose do the weights serve and how are they significant in this neural network?</p>
",feedforward neural network inputs fed directly outputs via series weights   purpose weights serve significant neural network  ,feedforward neural network input fed direct output via seri weight purpos weight serv signific neural network,untagged,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,backpropagation,training,reinforcement-learning,ai-design,classification,tensorflow,game-ai,artificial-neuron,recurrent-neural-networks,genetic-algorithms,algorithm"
92,"<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p>

<hr>

<p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p>
",pretty sure noob question  deep network  popular tag ai  reason      please note  asking distinguish deep network neural network  simply asking definition deep network  ,pretti sure noob question deep network popular tag ai reason pleas note ask distinguish deep network neural network simpli ask definit deep network,deep-network,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,game-ai,tensorflow,training,classification,deep-network,backpropagation,genetic-algorithms,image-recognition,recurrent-neural-networks"
98,"<p>I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.  What are some examples of successfully applying Classical AI to real world problems.</p>
",believe classical ai uses deductive thought processes  example  given set constraints  deduce conclusion   examples successfully applying classical ai real world problems  ,believ classic ai use deduct thought process exampl given set constraint deduc conclus exampl success appli classic ai real world problem,classical-ai,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,reinforcement-learning,convolutional-neural-networks,algorithm,philosophy,classification,image-recognition,training,genetic-algorithms,strong-ai,tensorflow"
99,"<p>In <a href=""https://youtu.be/oSdPmxRCWws?t=30"">this video</a> an expert says, ""One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.""</p>

<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>
",video expert says   one way thinking intelligence  specifically regard artificial intelligence   optimization process    intelligence always thought optimization process  artificial intelligence always modeled optimization problem  pattern recognition  mischaracterizing  ,video expert say one way think intellig specif regard artifici intellig optim process intellig alway thought optim process artifici intellig alway model optim problem pattern recognit mischaracter,"optimization,agi","neural-networks,machine-learning,deep-learning,ai-design,philosophy,reinforcement-learning,convolutional-neural-networks,algorithm,tensorflow,keras,training,classification,game-ai,definitions,image-recognition"
103,"<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>
",specific advantages declarative languages make applicable ai imperative languages   declarative languages easily languages styles find difficult kind problem  ,specif advantag declar languag make applic ai imper languag declar languag easili languag style find difficult kind problem,declarative-programming,"machine-learning,neural-networks,ai-design,deep-learning,natural-language,game-ai,algorithm,nlp,convolutional-neural-networks,philosophy,reinforcement-learning,programming-languages,language-processing,genetic-algorithms,image-recognition"
104,"<p>In years past, GOFAI (Good Old Fashioned AI) was heavily based on ""rules"" and <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">symbolic computation</a> based on rules.  Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical / probabilistic approaches leading to the current wave of interest in ""machine learning"".</p>

<p>It seems though, that the symbolic / rule based approach probably still has application. So, could one ""learn"" rules using a probabilistic <a href=""https://en.wikipedia.org/wiki/Rule_induction"" rel=""nofollow"">rule induction</a> method, and then layer symbolic computation on top?  If so, how could the whole process be made truly two-way, so that something ""learned"" from processing rules, can be fed back into how the system learns rules? </p>
",years past  gofai  good old fashioned ai  heavily based  rules  symbolic computation based rules   unfortunately  approach ran stumbling blocks  world moved heavily towards statistical   probabilistic approaches leading current wave interest  machine learning    seems though  symbolic   rule based approach probably still application   could one  learn  rules using probabilistic rule induction method  layer symbolic computation top    could whole process made truly two way  something  learned  processing rules  fed back system learns rules   ,year past gofai good old fashion ai heavili base rule symbol comput base rule unfortun approach ran stumbl block world move heavili toward statist probabilist approach lead current wave interest machin learn seem though symbol rule base approach probabl still applic could one learn rule use probabilist rule induct method layer symbol comput top could whole process made truli two way someth learn process rule fed back system learn rule,"gofai,symbolic-computing","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,game-ai,algorithm,genetic-algorithms,classification,training,image-recognition,tensorflow,philosophy,backpropagation"
105,"<p>Obviously driverless cars aren't perfect, so imagine that the Google car (as an example) got into difficult situation.</p>

<p>Here are a few examples of unfortunate situations caused by set of events:</p>

<ul>
<li>the car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>
<li>avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>
<li>killing animal on the street in favour of human being,</li>
<li>changing lanes to crash into another car to avoid killing a dog,</li>
</ul>

<p>And here are few dilemmas:</p>

<ul>
<li>Does the algorithm recognize the difference between a human being and an animal?</li>
<li>Does the size of the human being or animal matter?</li>
<li>Does it count how many passengers it has vs. people in the front?</li>
<li>Does it ""know"" when babies/children are on board?</li>
<li>Does it take into the account the age (e.g. killing the older first)?</li>
</ul>

<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>

<p>Related articles:</p>

<ul>
<li><a href=""https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/"">Why Self-Driving Cars Must Be Programmed to Kill</a></li>
<li><a href=""https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/"">How to Help Self-Driving Cars Make Ethical Decisions</a></li>
</ul>
",obviously driverless cars perfect  imagine google car  example  got difficult situation   examples unfortunate situations caused set events    car heading toward crowd    people crossing road  cannot stop time  avoid killing    people hitting wall  killing passengers   avoiding killing rider motorcycle considering probability survival greater passenger car  killing animal street favour human  changing lanes crash another car avoid killing dog    dilemmas    algorithm recognize difference human animal  size human animal matter  count many passengers vs  people front   know  babies children board  take account age  e g  killing older first     would algorithm decide technical perspective  aware  counting probability kills    killing people avoid destruction    related articles    self driving cars must programmed kill help self driving cars make ethical decisions  ,obvious driverless car perfect imagin googl car exampl got difficult situat exampl unfortun situat caus set event car head toward crowd peopl cross road stop time avoid kill peopl hit wall kill passeng avoid kill rider motorcycl consid probabl surviv greater passeng car kill anim street favour human chang lane crash anoth car avoid kill dog dilemma algorithm recogn differ human anim size human anim matter count mani passeng vs peopl front know babi children board take account age e g kill older first would algorithm decid technic perspect awar count probabl kill kill peopl avoid destruct relat articl self drive car must program kill help self drive car make ethic decis,"algorithm,self-driving,decision-theory,ethics","neural-networks,machine-learning,deep-learning,ai-design,algorithm,reinforcement-learning,convolutional-neural-networks,image-recognition,game-ai,philosophy,training,genetic-algorithms,classification,tensorflow,self-driving"
106,"<p>Which deep neural network is used in <a href=""https://en.wikipedia.org/wiki/Google_self-driving_car"" rel=""nofollow"">Google's driverless cars</a> to analyse the surroundings? Is this information is open?</p>
",deep neural network used google driverless cars analyse surroundings  information open  ,deep neural network use googl driverless car analys surround inform open,"deep-network,algorithm,self-driving","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,image-recognition,game-ai,training,tensorflow,classification,backpropagation,genetic-algorithms,deep-network,recurrent-neural-networks"
107,"<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e tanh(z) = 2*sigma(z) - 1). </p>

<p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p>

<p>I realize that in some cases (like when estimating probabilities) outputs in the range of [0,1] are more convenient than outputs that range from [-1,1]. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p>
",two common activation functions used deep learning hyperbolic tangent function sigmoid activation function  understand hyperbolic tangent rescaling translation sigmoid function  e tanh z      sigma z          significant difference two activation functions  particular  one preferable   realize cases  like estimating probabilities  outputs range       convenient outputs range         want know differences convenience distinguish two activation functions  ,two common activ function use deep learn hyperbol tangent function sigmoid activ function understand hyperbol tangent rescal translat sigmoid function e tanh z sigma z signific differ two activ function particular one prefer realiz case like estim probabl output rang conveni output rang want know differ conveni distinguish two activ function,"deep-network,neural-networks,machine-learning,hidden-layers","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,tensorflow,algorithm,training,game-ai,classification,backpropagation,keras,image-recognition,q-learning"
112,"<p><a href=""https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic"">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>

<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>
",fuzzy logic logic every statement real truth value       fuzzy logic used creating ai  useful certain decision problems involving multiple inputs  give example ai uses  ,fuzzi logic logic everi statement real truth valu fuzzi logic use creat ai use certain decis problem involv multipl input give exampl ai use,fuzzy-logic,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,tensorflow,classification,training,image-recognition,philosophy,genetic-algorithms,backpropagation"
114,"<p>In <a href=""http://users.ox.ac.uk/~jrlucas/Godel/mmg.html"" rel=""nofollow"">Minds, Machines and Gdel</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using Gdel's incompleteness theorem. </p>

<p>As I understand it, he states that since the computer is an algorithm and hence a formal system, Gdel's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p>
",minds  machines g del         j  r  lucas shows human mathematician represented algorithmic automaton  turing machine  computer equivalent church turing thesis   using g del incompleteness theorem    understand  states since computer algorithm hence formal system  g del incompleteness theorem applies  human mathematician also work formal axiom system prove theorem  apply well   ,mind machin g del j r luca show human mathematician repres algorithm automaton ture machin comput equival church ture thesi use g del incomplet theorem understand state sinc comput algorithm henc formal system g del incomplet theorem appli human mathematician also work formal axiom system prove theorem appli well,"philosophy,incompleteness-theorems","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,philosophy,convolutional-neural-networks,image-recognition,game-ai,training,tensorflow,genetic-algorithms,computer-vision,classification"
117,"<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence.</p>

<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where  a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese, but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules, but does not understand what (s)he is communicating.</p>

<p>Does the chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>
",back college  complexity theory teacher stated artificial intelligence contradiction terms  could calculated mechanically  argued  intelligence   seems variant chinese room argument  argument metaphor   person put room full chinese books  person understand word chinese  slipped messages chinese door  person use books  contain transformation rules  answer messages  person apply transformation rules  understand  communicating   chinese room argument hold  argue artificial intelligence merely clever algorithmics  ,back colleg complex theori teacher state artifici intellig contradict term could calcul mechan argu intellig seem variant chines room argument argument metaphor person put room full chines book person understand word chines slip messag chines door person use book contain transform rule answer messag person appli transform rule understand communic chines room argument hold argu artifici intellig mere clever algorithm,philosophy,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,philosophy,algorithm,convolutional-neural-networks,natural-language,nlp,game-ai,training,genetic-algorithms,tensorflow,image-recognition"
124,"<p>What are the main differences between <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Deep Boltzmann Machines</a> (DBM) recurrent neural network and <a href=""https://en.wikipedia.org/wiki/Deep_belief_network"" rel=""nofollow"">Deep Belief Network</a> (which is based on RBMs)?</p>
",main differences deep boltzmann machines  dbm  recurrent neural network deep belief network  based rbms   ,main differ deep boltzmann machin dbm recurr neural network deep belief network base rbms,boltzmann-machine,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,classification,game-ai,tensorflow,image-recognition,backpropagation,deep-network,recurrent-neural-networks,genetic-algorithms"
129,"<p>I'd like to learn more about the differences between <a href=""https://en.wikipedia.org/wiki/Cellular_automaton#Related_automata"" rel=""nofollow noreferrer"">related automata</a> which can be based on hexagonal cells instead of squares (rule 34/2), like in <a href=""https://en.wikipedia.org/wiki/CoDi"" rel=""nofollow noreferrer"">CoDi model</a> which uses spiking neural network (SNN). </p>

<p>Is using a plane tiled with regular <a href=""https://en.wikipedia.org/wiki/Hexagonal_tiling"" rel=""nofollow noreferrer"">hexagons</a> more efficient and reliable than using square cells? What is the difference and how do I know which one to use in which scenario?</p>

<hr>

<p>In other words, the more efficiently flexible that it grows, the more difficult scenarios it can be used for (for me, hexagonal implicates more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or they're both on the same level? In general, I'd like to learn the differences between them to know when I should use one over the other. </p>
",like learn differences related automata based hexagonal cells instead squares  rule        like codi model uses spiking neural network  snn     using plane tiled regular hexagons efficient reliable using square cells  difference know one use scenario     words  efficiently flexible grows  difficult scenarios used   hexagonal implicates possibilities  send share signal tiles   maybe one modern  level  general  like learn differences know use one   ,like learn differ relat automata base hexagon cell instead squar rule like codi model use spike neural network snn use plane tile regular hexagon effici reliabl use squar cell differ know one use scenario word effici flexibl grow difficult scenario use hexagon implic possibl send share signal tile mayb one modern level general like learn differ know use one,efficiency,"neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,game-ai,training,tensorflow,algorithm,image-recognition,classification,backpropagation,genetic-algorithms,natural-language"
133,"<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>

<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>

<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>
",ultraintelligent machine machine surpass intellectual activities human  machine often used science fiction machine brings mankind end    machine executed using algorithm  church turing thesis  algorithm executed modern computer executed turing machine  however  human easily simulate turing machine  mean machine surpass intellectual activities  since also execute algorithm   argument likely flawed  since intuition tells  ultraintelligent machines possible  however  clear flaw  note argument   ,ultraintellig machin machin surpass intellectu activ human machin often use scienc fiction machin bring mankind end machin execut use algorithm church ture thesi algorithm execut modern comput execut ture machin howev human easili simul ture machin mean machin surpass intellectu activ sinc also execut algorithm argument like flaw sinc intuit tell ultraintellig machin possibl howev clear flaw note argument,"philosophy,ultraintelligent-machine","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,convolutional-neural-networks,image-recognition,philosophy,game-ai,training,singularity,nlp,tensorflow,definitions"
138,"<p>From Wikipedia:</p>

<blockquote>
  <p>AIXI ['ai?k?si?] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>
</blockquote>

<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>

<p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>
",wikipedia      aixi   ai k si   theoretical mathematical formalism artificial general intelligence  combines solomonoff induction sequential decision theory  aixi first proposed marcus hutter         results proved hutter      book universal artificial intelligence       albeit non computable  approximations possible  aixitl  finding approximations aixi could objective way solving ai   question  aixi really big deal artificial general intelligence research  thought central concept field   publications subject  maybe aware   ,wikipedia aixi ai k si theoret mathemat formal artifici general intellig combin solomonoff induct sequenti decis theori aixi first propos marcus hutter result prove hutter book univers artifici intellig albeit non comput approxim possibl aixitl find approxim aixi could object way solv ai question aixi realli big deal artifici general intellig research thought central concept field public subject mayb awar,"models,agi","machine-learning,neural-networks,deep-learning,ai-design,philosophy,reinforcement-learning,game-ai,strong-ai,convolutional-neural-networks,algorithm,image-recognition,research,nlp,training,classification"
139,"<p>In what ways can connectionist artificial intelligence (neural networks) be integrated with <em>Good Old-Fashioned A.I.</em> (<em>GOFAI</em>)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the <a href=""http://wiki.opencog.org/w/DestinOpenCog"" rel=""nofollow"">OpenCog + Destin integration</a>.</p>
",ways connectionist artificial intelligence  neural networks  integrated good old fashioned   gofai   instance  could deep neural networks integrated knowledge bases logical inference  one example seems opencog   destin integration  ,way connectionist artifici intellig neural network integr good old fashion gofai instanc could deep neural network integr knowledg base logic infer one exampl seem opencog destin integr,"neural-networks,classical-ai,gofai,symbolic-computing","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,classification,game-ai,image-recognition,tensorflow,backpropagation,genetic-algorithms,philosophy,algorithm"
140,"<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p>
",proved recurrent neural net rational weights super turing machine  achieve practice   ,prove recurr neural net ration weight super ture machin achiev practic,"neural-networks,hypercomputation,recurrent-neural-networks","neural-networks,machine-learning,deep-learning,backpropagation,convolutional-neural-networks,training,reinforcement-learning,artificial-neuron,ai-design,game-theory,classification,image-recognition,philosophy,game-ai,algorithm"
141,"<p>Given the proven <a href=""https://en.wikipedia.org/wiki/Halting_problem"">halting problem</a> for <a href=""https://en.wikipedia.org/wiki/Turing_machine"">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>
",given proven halting problem turing machines  infer limits ability strong artificial intelligence  ,given proven halt problem ture machin infer limit abil strong artifici intellig,halting-problem,"machine-learning,neural-networks,deep-learning,philosophy,ai-design,algorithm,reinforcement-learning,definitions,strong-ai,ultraintelligent-machine,singularity,convolutional-neural-networks,image-recognition,game-ai,genetic-algorithms"
143,"<p>By default using <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> technique you can creating a dreamlike image out of two different images.</p>

<p>Is it possible to easily enhance this technique to generate one image out from three?</p>
",default using deepdream technique creating dreamlike image two different images   possible easily enhance technique generate one image three  ,default use deepdream techniqu creat dreamlik imag two differ imag possibl easili enhanc techniqu generat one imag three,"convolutional-neural-networks,deepdream","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,classification,reinforcement-learning,computer-vision,tensorflow,training,ai-design,algorithm,game-ai,keras,python"
144,"<p>Consider these neural style algorithms which produce some art work:</p>

<ul>
<li><a href=""https://github.com/alexjc/neural-doodle"">Neural Doodle</a></li>
<li><a href=""https://github.com/jcjohnson/neural-style"">neural-style</a></li>
</ul>

<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>

<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>

<p>Here are few user comments (<a href=""https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/"">How ANYONE can create Deep Style images</a>):</p>

<ul>
<li><blockquote>
  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>
</blockquote></li>
<li><blockquote>
  <p>Having a memory issue, though. I'm using the ""g2.8xlarge"" instance type.</p>
</blockquote></li>
</ul>
",consider neural style algorithms produce art work    neural doodle neural style   generating images slow take huge amounts memory  method optimizing algorithm   mechanism technical limitation behind  realtime processing   user comments  anyone create deep style images        anything    x    talking days heavy crunching insane amount ram      tried     pixel image still crashed   gigs memory    gigs swap  vm space swapfile  plus takes several hours potentially days cpu rendering      tried     x      gig ram     gig swap still dying lack memory      memory issue  though  using  g   xlarge  instance type    ,consid neural style algorithm produc art work neural doodl neural style generat imag slow take huge amount memori method optim algorithm mechan technic limit behind realtim process user comment anyon creat deep style imag anyth x talk day heavi crunch insan amount ram tri pixel imag still crash gig memori gig swap vm space swapfil plus take sever hour potenti day cpu render tri x gig ram gig swap still die lack memori memori issu though use g xlarg instanc type,"performance,neural-doodle,deepdreaming","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,tensorflow,ai-design,classification,keras,algorithm,training,game-ai,computer-vision,q-learning"
145,"<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p>
",autoencoders used supervised learning without adding output layer  simply feed concatenated input output vector training  reconstruct output part input part inference  output part would treated missing values inference imputation would applied  ,autoencod use supervis learn without ad output layer simpli feed concaten input output vector train reconstruct output part input part infer output part would treat miss valu infer imput would appli,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,classification,algorithm,backpropagation,training,game-ai,image-recognition,keras,recurrent-neural-networks"
146,"<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>

<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>

<p>Example 1 (<code>2+2</code>):</p>

<ul>
<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input 3: <code>2</code>; Expected output: <code>4</code></li>
<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input 3: <code>10</code>; Expected output: <code>0</code></li>
<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input 3: <code>5</code>; Expected output: <code>25</code></li>
<li>and so</li>
</ul>

<p>The above can be extended to more sophisticated examples.</p>

<p>Is that possible? If so, what kind of network can learn/achieve that?</p>
",aware neural networks probably designed  however asking hypothetically  possible train deep neural network  similar  solve math equations   given   inputs   st number  operator sign represented number                                 nd number  training network give valid results   example            input       input           input       expected output    input        input           input        expected output    input       input           input       expected output       extended sophisticated examples   possible   kind network learn achieve  ,awar neural network probabl design howev ask hypothet possibl train deep neural network similar solv math equat given input st number oper sign repres number nd number train network give valid result exampl input input input expect output input input input expect output input input input expect output extend sophist exampl possibl kind network learn achiev,"neural-networks,math","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,tensorflow,reinforcement-learning,classification,training,backpropagation,algorithm,keras,game-ai,genetic-algorithms,image-recognition"
148,"<p>From Wikipedia:</p>

<blockquote>
  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>
</blockquote>

<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>
",wikipedia      mirror neuron neuron fires animal acts animal observes action performed another    mirror neurons related imitation learning  useful feature missing current real world  implementations  instead learning input output examples  supervised learning  rewards  reinforcement learning   agent mirror neurons would able learn simply observing agents  translating movements coordinate system  subject regarding computational models  ,wikipedia mirror neuron neuron fire anim act anim observ action perform anoth mirror neuron relat imit learn use featur miss current real world implement instead learn input output exampl supervis learn reward reinforc learn agent mirror neuron would abl learn simpli observ agent translat movement coordin system subject regard comput model,"neural-networks,models","neural-networks,machine-learning,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,tensorflow,algorithm,game-ai,classification,training,q-learning,image-recognition,backpropagation,keras"
149,"<p>If I have a paragraph I want to summarize, for example:</p>

<blockquote>
  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p>
</blockquote>

<p>Better summarized as:</p>

<blockquote>
  <p>They shopped at the mall today and bought some clothes.</p>
</blockquote>

<p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p>
",paragraph want summarize  example      ponzo fila went mall day  walked long  stopping shops  went many shops  first  buy anything  going number shops  eventually bought shirt  pair pants    better summarized      shopped mall today bought clothes    best ai strategy automate process  one   would dependent first external information resource would inform algorithm  problem inherently contextual  ,paragraph want summar exampl ponzo fila went mall day walk long stop shop went mani shop first buy anyth go number shop eventu bought shirt pair pant better summar shop mall today bought cloth best ai strategi autom process one would depend first extern inform resourc would inform algorithm problem inher contextu,"algorithm,natural-language,pattern-recognition","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,classification,image-recognition,genetic-algorithms,philosophy,game-theory,training,natural-language"
151,"<p>What happens if you apply the same <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">deep dream technique</a> which produces ""dream"" visuals, but to media streams such as audio files?</p>

<p>Does changing image functions into audio and enhancing the logic would work, or it won't work or doesn't make any sense?</p>

<p>My goal is to create ""dream"" like audio based on the two samples.</p>
",happens apply deep dream technique produces  dream  visuals  media streams audio files   changing image functions audio enhancing logic would work  work make sense   goal create  dream  like audio based two samples  ,happen appli deep dream techniqu produc dream visual media stream audio file chang imag function audio enhanc logic would work work make sens goal creat dream like audio base two sampl,"convolutional-neural-networks,deepdreaming","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,image-recognition,algorithm,classification,game-ai,training,tensorflow,computer-vision,genetic-algorithms,game-theory"
158,"<p>In <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.</p>

<blockquote>
  <p>The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.</p>
</blockquote>

<p>How this is even possible?</p>

<p>How exactly convolutional neural networks have anything to do with human visual cortex?</p>
",deepdream wikipedia page suggested dreamlike images created convolutional neural network may related visual cortex works humans tripping      imagery lsd  psilocybin induced hallucinations suggestive functional resemblance artificial neural networks particular layers visual cortex    even possible   exactly convolutional neural networks anything human visual cortex  ,deepdream wikipedia page suggest dreamlik imag creat convolut neural network may relat visual cortex work human trip imageri lsd psilocybin induc hallucin suggest function resembl artifici neural network particular layer visual cortex even possibl exact convolut neural network anyth human visual cortex,"convolutional-neural-networks,deepdream,computer-vision,deepdreaming","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,ai-design,game-ai,classification,tensorflow,training,backpropagation,recurrent-neural-networks,computer-vision,genetic-algorithms"
159,"<p>This 2014 <a href=""https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb"" rel=""nofollow"">article</a> saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.</p>

<p><strong>Why did they have to use a quantum computer</strong> to do that?</p>

<p>Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?</p>

<p>If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?</p>
",     article saying chinese team physicists trained quantum computer recognise handwritten characters   use quantum computer   fun demonstration  recognising handwritten characters difficult standard  non quantum  computers algorithms cannot   standard computers achieve thing  benefits using quantum computers standard methods  ,articl say chines team physicist train quantum comput recognis handwritten charact use quantum comput fun demonstr recognis handwritten charact difficult standard non quantum comput algorithm standard comput achiev thing benefit use quantum comput standard method,"quantum-computing,handwritten-characters,ocr","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,algorithm,training,ai-design,tensorflow,image-recognition,game-ai,classification,computer-vision,deep-network,philosophy"
162,"<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>
",possible time future  ais able initiatively develop  rather passively developed humanity  ,possibl time futur ai abl initi develop rather passiv develop human,"neural-networks,machine-learning,unsupervised-learning,self-learning","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,strong-ai,algorithm,convolutional-neural-networks,tensorflow,training,game-theory,image-recognition,genetic-algorithms"
168,"<p>I have been wondering since a while ago about the <a href=""https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences"" rel=""nofollow noreferrer"">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>

<p>We hear from time to time about <a href=""https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london"" rel=""nofollow noreferrer"">Leonardo</a> being a genius or <a href=""https://www.youtube.com/watch?v=xUHQ2ybTejU"" rel=""nofollow noreferrer"">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>

<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>

<hr>

<p><a href=""https://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented"">Related question - How could emotional intelligence be implemented?</a></p>
",wondering since ago multiple intelligences could fit field artificial intelligence whole   hear time time leonardo genius bach musical intelligence  persons commonly said   intelligent  multiple intelligences speak cooking dancing chatting well  e  coping everyday tasks  least interpretation    approaches incorporating multiple intelligences ai     related question   could emotional intelligence implemented  ,wonder sinc ago multipl intellig could fit field artifici intellig whole hear time time leonardo genius bach music intellig person common said intellig multipl intellig speak cook danc chat well e cope everyday task least interpret approach incorpor multipl intellig ai relat question could emot intellig implement,"emotional-intelligence,new-ai","machine-learning,neural-networks,deep-learning,philosophy,ai-design,reinforcement-learning,strong-ai,game-ai,convolutional-neural-networks,algorithm,definitions,genetic-algorithms,research,natural-language,singularity"
169,"<p>Which is the preferred algorithm to build word vector for a given language?</p>
",preferred algorithm build word vector given language  ,prefer algorithm build word vector given languag,"algorithm,nlp,wordvector","machine-learning,neural-networks,natural-language,deep-learning,algorithm,reinforcement-learning,nlp,ai-design,classification,language-processing,image-recognition,learning-algorithms,recurrent-neural-networks,genetic-algorithms,game-theory"
170,"<p>How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?</p>
",decide optimum number layers created implementing neural network  feedforward  back propagation rnn   ,decid optimum number layer creat implement neural network feedforward back propag rnn,"neural-networks,hidden-layers","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,backpropagation,reinforcement-learning,tensorflow,game-ai,classification,ai-design,recurrent-neural-networks,training,genetic-algorithms,image-recognition,algorithm"
172,"<p>I am interested in the <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow"">emergence</a> of properties in <a href=""https://en.wikipedia.org/wiki/Agent-based_model#Theory"" rel=""nofollow"">agents</a>, and, more generally in robotics.</p>

<p>I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like <em>before</em> and <em>after</em>. I know, for example, that there is work on the emergence of <a href=""http://www.scholarpedia.org/article/Kohonen_network"" rel=""nofollow"">spatial representation</a> (similar to <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow"">knn</a>), or even <a href=""https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf"" rel=""nofollow"">communication</a>* but time seems to be a tricky concept. </p>

<p>This has everything to do with the <em>platform</em>, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time <em>looks like</em> in humans, or if it is even present in other living beings.</p>

<p><strong>Is there some work on the (emergence of the) representation of <em>time</em> in artificial agents?</strong></p>

<hr>

<p>*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?</p>
",interested emergence properties agents   generally robotics   wondering work emergence time related concepts  low level representation notions like  know  example  work emergence spatial representation  similar knn   even communication  time seems tricky concept    everything platform  e  way representation would coded  tend favour ways meaning somehow mimic natural  well  yes  human structures  like brain  neuroscientist know sense time looks like humans  even present living beings   work  emergence  representation time artificial agents      remember watching really cool    actually creepy video robots cannot find anymore  anyone link hand  ,interest emerg properti agent general robot wonder work emerg time relat concept low level represent notion like know exampl work emerg spatial represent similar knn even communic time seem tricki concept everyth platform e way represent would code tend favour way mean somehow mimic natur well yes human structur like brain neuroscientist know sens time look like human even present live work emerg represent time artifici agent rememb watch realli cool actual creepi video robot find anymor anyon link hand,"knowledge-representation,time,embodied-cognition","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,philosophy,image-recognition,classification,training,tensorflow,nlp,natural-language"
174,"<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>
",proposed extensions go beyond turing machine solve halting problem  would proposed extensions value advance strong artificial intelligence   example  quantum computing go beyond definition turing machine resolve halting problem  help creating strong ai  ,propos extens go beyond ture machin solv halt problem would propos extens valu advanc strong artifici intellig exampl quantum comput go beyond definit ture machin resolv halt problem help creat strong ai,"quantum-computing,halting-problem,strong-ai","machine-learning,neural-networks,deep-learning,ai-design,philosophy,reinforcement-learning,algorithm,game-ai,convolutional-neural-networks,image-recognition,genetic-algorithms,classification,tensorflow,strong-ai,training"
179,"<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=""https://www.youtube.com/watch?v=WnzlbyTZsQY"" rel=""nofollow"">'I am not a robot. I am a unicorn' case?</a></p>

<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>
",first ai able carry conversation  real responses  famous  robot  unicorn  case    real response  constitutes sort personalized answer specific input user  ,first ai abl carri convers real respons famous robot unicorn case real respons constitut sort person answer specif input user,"history,turing-test,natural-language","neural-networks,machine-learning,ai-design,deep-learning,game-ai,reinforcement-learning,convolutional-neural-networks,algorithm,philosophy,classification,nlp,image-recognition,tensorflow,strong-ai,training"
184,"<p>This question stems from quite a few ""informal"" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>

<p>There may be several arguments for survival. What would be the most prominent?</p>
",question stems quite  informal  sources  movies like       space odyssey ex machina  books like destination void  frank herbert   others suggest general intelligence wants survive  even learn importance   may several arguments survival  would prominent  ,question stem quit inform sourc movi like space odyssey ex machina book like destin void frank herbert suggest general intellig want surviv even learn import may sever argument surviv would promin,agi,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,classification,image-recognition,tensorflow,philosophy,game-ai,training,nlp,research"
185,"<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p>

<p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p>
",identifying sarcasm considered one difficult open ended problems domain ml nlp    considerable research done front  yes  accuracy like  please also explain nlp model briefly  ,identifi sarcasm consid one difficult open end problem domain ml nlp consider research done front yes accuraci like pleas also explain nlp model briefli,"natural-language,research","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,classification,algorithm,keras,image-recognition,training,research,computer-vision,nlp"
189,"<p>I'd like to know more about <a href=""https://ai.stackexchange.com/q/26/8"">implementing emotional intelligence</a>.</p>

<p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p>

<p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p>

<p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p>

<p>How this possibly can be achieved? Are there any examples?</p>
",like know implementing emotional intelligence   given implementing chat bot like introduce levels curiosity measure whether user text input interesting   high level would mean bot asking questions following topic  lower level curiosity makes bot asking questions changing topics   less interesting content could mean bot see opportunity learn something new understand topic want talk  low quality    possibly achieved  examples  ,like know implement emot intellig given implement chat bot like introduc level curios measur whether user text input interest high level would mean bot ask question follow topic lower level curios make bot ask question chang topic less interest content could mean bot see opportun learn someth new understand topic want talk low qualiti possibl achiev exampl,"emotional-intelligence,chat-bots","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,classification,training,image-recognition,philosophy,tensorflow,nlp,genetic-algorithms"
190,"<p>I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.</p>

<p>Given the following <code>hello.c</code> file (as example):</p>

<pre><code>#include &lt;stdio.h&gt;
int main() {
  printf(""Hello World!"");
}
</code></pre>

<p>Then after compilation (<code>gcc hello.c</code>) I've got the binary file like:</p>

<pre><code>$ hexdump -C a.out | head
00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|
00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|
00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|
00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|
00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|
00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|
00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|
$ wc -c hello.c a.out 
  60 hello.c
8432 a.out
</code></pre>

<p>For the learning dataset I assume I'll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.</p>

<p>My concerns are:</p>

<ul>
<li>do my algorithm needs to be aware about the header file, or it's ""smart"" enough to figure it out,</li>
<li>if it needs to know about the header, how do I tell my algorithm 'here is the header file',</li>
<li>what should be input/output mapping (whether some section to section or file to file),</li>
<li>do I need to divide my source code into some sections,</li>
<li>do I need to know exactly how decompilers work or AI can figure it out for me,</li>
<li>or should I've two networks, one for header, another for body it-self,</li>
<li>or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)</li>
</ul>

<p>How would you tackle this?</p>
",would like learn whether possible write program decompiles executable binary  object file  c source  asking exactly    rather achieved   given following hello c file  example     include  lt stdio h gt  int main       printf  hello world         compilation  gcc hello c  got binary file like     hexdump  c   head           cf fa ed fe                                                                     f          b                                                                                            f  f                       h     pageze                f                                              ro                                                                                                                                                                                                                                                                      f  f                                                text              wc  c hello c       hello c        learning dataset assume thousands source code files along binary representation  algorithm learn moving parts certain changes   concerns    algorithm needs aware header file   smart  enough figure  needs know header  tell algorithm  header file   input output mapping  whether section section file file   need divide source code sections  need know exactly decompilers work ai figure  two networks  one header  another body self  separate networks  one logical component  e g  byte  c tag  etc     would tackle  ,would like learn whether possibl write program decompil execut binari object file c sourc ask exact rather achiev given follow hello c file exampl includ lt stdio h gt int main printf hello world compil gcc hello c got binari file like hexdump c head cf fa ed fe f b f f h pagez f ro f f text wc c hello c hello c learn dataset assum thousand sourc code file along binari represent algorithm learn move part certain chang concern algorithm need awar header file smart enough figur need know header tell algorithm header file input output map whether section section file file need divid sourc code section need know exact decompil work ai figur two network one header anoth bodi self separ network one logic compon e g byte c tag etc would tackl,"algorithm,deep-learning","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,algorithm,reinforcement-learning,tensorflow,game-theory,training,classification,decision-theory,game-ai,image-recognition,genetic-algorithms"
192,"<p>Text summarization is a long-standing research problem that was <em>""ignited""</em> by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.</p>

<p>Is summarization problem solvable using AI (neural networks to be precise)? </p>
",text summarization long standing research problem  ignited  luhn       however  half century later  still came nowhere close  solving problem  abstractive summarization   reason might researchers resorting statistical  sometimes linguistic  methods find  amp  extract salient parts text   summarization problem solvable using ai  neural networks precise    ,text summar long stand research problem ignit luhn howev half centuri later still came nowher close solv problem abstract summar reason might research resort statist sometim linguist method find amp extract salient part text summar problem solvabl use ai neural network precis,"neural-networks,natural-language,text-summarization","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,algorithm,game-ai,classification,training,image-recognition,genetic-algorithms,tensorflow,philosophy,computer-vision"
194,"<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p>

<p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p>

<p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p>

<p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p>
",like know common file format efficient terms simplicity storage space storing state artificial neural network   talking memory storage  file storage  data loaded later   first guess would xml  millions connections weights would generate huge amount data  another thing would dump object instances binary file using export serialize functions  disadvantage file common language specific   common file format standards used exporting huge artificial neural network file loaded another program   one  ,like know common file format effici term simplic storag space store state artifici neural network talk memori storag file storag data load later first guess would xml million connect weight would generat huge amount data anoth thing would dump object instanc binari file use export serial function disadvantag file common languag specif common file format standard use export huge artifici neural network file load anoth program one,storage,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,tensorflow,image-recognition,algorithm,classification,game-ai,genetic-algorithms,backpropagation,natural-language"
195,"<p>What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?</p>
",ai techniques ibm use watson platform  specifically natural language analysis  ,ai techniqu ibm use watson platform specif natur languag analysi,"algorithm,natural-language,watson,lexical-recognition","machine-learning,neural-networks,ai-design,deep-learning,game-ai,reinforcement-learning,natural-language,convolutional-neural-networks,algorithm,strong-ai,philosophy,tensorflow,training,image-recognition,nlp"
196,"<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p>

<p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p>
",investigating possibility storing semantic lexical connections  relationships words phrases dependencies  strength  part speech  language  etc   order provide analysis input text   assume already done   avoid reinventing wheel  efficient method store manage data common format already researched tested  ,investig possibl store semant lexic connect relationship word phrase depend strength part speech languag etc order provid analysi input text assum alreadi done avoid reinvent wheel effici method store manag data common format alreadi research test,"algorithm,models,research,storage,lexical-recognition","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,classification,natural-language,tensorflow,training,algorithm,nlp,game-ai,image-recognition,keras"
198,"<p>Which objective and measurable tests have been developed to test the intelligence of AI? </p>

<p>The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.</p>

<p>I am looking for other, more modern tests. </p>
",objective measurable tests developed test intelligence ai    classical test turing test  objective criteria measurable since measured percentage jury fooled ai   looking  modern tests   ,object measur test develop test intellig ai classic test ture test object criteria measur sinc measur percentag juri fool ai look modern test,intelligence-testing,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,tensorflow,convolutional-neural-networks,classification,game-ai,strong-ai,training,image-recognition,nlp,research,python"
202,"<p>I'm interested in implementing a program for natural language processing (aka <a href=""https://en.wikipedia.org/wiki/ELIZA"" rel=""nofollow noreferrer"">ELIZA</a>).</p>

<p>Assuming that I'm already <a href=""https://ai.stackexchange.com/q/212/8"">storing semantic-lexical connections</a> between the words and its strength.</p>

<p>What are the methods of dealing with words which have very distinct meaning?</p>

<p>Few examples:</p>

<ul>
<li><p>'Are we on the same page?'</p>

<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>
<li><p>'I'm living in Reading.'</p>

<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>
<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>

<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>
</ul>

<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>

<p>For example:</p>

<ul>
<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>
<li>Detecting whether the word is part of phrase.</li>
<li>Detecting word for multiple meaning.</li>
</ul>

<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>
",interested implementing program natural language processing  aka eliza    assuming already storing semantic lexical connections words strength   methods dealing words distinct meaning   examples     page     page  context document page  part phrase   living reading     reading  city  noun   verb  otherwise make sense  checking capital letter would work specific example  work  like  make  either verb noun    read something facebook wall  want know     facebook wall  nothing wall    general  algorithm distinguish word meaning recognise word within context   example    detecting word different type speech  recognise whether verb noun  detecting whether word part phrase  detecting word multiple meaning    possible approaches solve problem order  identify correct sense word context  ,interest implement program natur languag process aka eliza assum alreadi store semant lexic connect word strength method deal word distinct mean exampl page page context document page part phrase live read read citi noun verb otherwis make sens check capit letter would work specif exampl work like make either verb noun read someth facebook wall want know facebook wall noth wall general algorithm distinguish word mean recognis word within context exampl detect word differ type speech recognis whether verb noun detect whether word part phrase detect word multipl mean possibl approach solv problem order identifi correct sens word context,"nlp,lexical-recognition","machine-learning,neural-networks,deep-learning,ai-design,natural-language,convolutional-neural-networks,reinforcement-learning,algorithm,classification,image-recognition,game-ai,nlp,genetic-algorithms,training,tensorflow"
204,"<p>Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).</p>

<p>So how does this model learn?</p>
",unsupervised learning involve target values  basically targets likely inputs  words  involves target values    model learn  ,unsupervis learn involv target valu basic target like input word involv target valu model learn,"models,unsupervised-learning","neural-networks,machine-learning,deep-learning,reinforcement-learning,tensorflow,convolutional-neural-networks,ai-design,q-learning,classification,keras,game-ai,training,algorithm,backpropagation,natural-language"
205,"<p>Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I'm referring to this state of affairs as 'multipolar,' where instead of there being one world leader that's far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There's not only one academic center of AI research worth mentioning, there might be particularly hot companies but there's not only one worth mentioning, and so on.)</p>

<p>But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I'm referring to as ""monolithic."" Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can't be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)</p>

<p>It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, <em>how we would know</em> which path seems more likely.</p>

<p>(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)</p>
",currently  many different organizations cutting edge ai research  innovations shared freely  time lag  others kept private  referring state affairs  multipolar   instead one world leader far ahead everyone else  many competitors mentioned breath   one academic center ai research worth mentioning  might particularly hot companies one worth mentioning     could imagine instead one institution mattered comes ai  company  university  research group  non profit   referring  monolithic   maybe access tools resources one else access  maybe attract best brightest way gives unsurmountable competitive edge  maybe returns research compound way means early edges overcome  maybe sort government coercion preventing competitors popping   industries  network first mover effects might good examples would expect industry monolithic instead multipolar    seems like able use insights social sciences like economics organizational design history science order figure  path seems likely  would know path seems likely    example  may able measure much returns research compound  sense one organization coming insight meaning organization likely come next relevant insight  knowing number makes easier figure boundary two trajectories located   ,current mani differ organ cut edg ai research innov share freeli time lag kept privat refer state affair multipolar instead one world leader far ahead everyon els mani competitor mention breath one academ center ai research worth mention might particular hot compani one worth mention could imagin instead one institut matter come ai compani univers research group non profit refer monolith mayb access tool resourc one els access mayb attract best brightest way give unsurmount competit edg mayb return research compound way mean earli edg overcom mayb sort govern coercion prevent competitor pop industri network first mover effect might good exampl would expect industri monolith instead multipolar seem like abl use insight social scienc like econom organiz design histori scienc order figur path seem like would know path seem like exampl may abl measur much return research compound sens one organ come insight mean organ like come next relev insight know number make easier figur boundari two trajectori locat,"research,ai-community","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,image-recognition,classification,tensorflow,training,philosophy,genetic-algorithms,research"
208,"<p>One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as ""nanobots swimming around our brains and bodies"" or ""electrodes connected to our skulls""?</p>
",one compelling applications ai would augmenting human biological intelligence  currently proposed methods aside vague notions  nanobots swimming around brains bodies   electrodes connected skulls   ,one compel applic ai would augment human biolog intellig current propos method asid vagu notion nanobot swim around brain bodi electrod connect skull,cyborg,"neural-networks,machine-learning,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,convolutional-neural-networks,algorithm,game-theory,strong-ai,genetic-algorithms,image-recognition,classification,training"
209,"<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p>

<p>Which AI or neural network would be more suitable for this task? </p>

<p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p>
",given list fixed numbers mathematical constant pi  possible train ai attempt predict next numbers   ai neural network would suitable task    especially one work without memorizing entire training set  one attempt find patterns statistical association  ,given list fix number mathemat constant pi possibl train ai attempt predict next number ai neural network would suitabl task especi one work without memor entir train set one attempt find pattern statist associ,"training,math,recurrent-neural-networks","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,tensorflow,training,classification,game-ai,algorithm,image-recognition,genetic-algorithms,philosophy,computer-vision"
211,"<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p>

<p>What are the fundamental differences between these two types?</p>
",main differences two types feedforward networks multilayer perceptrons  mlp  radial basis function  rbf    fundamental differences two types  ,main differ two type feedforward network multilay perceptron mlp radial basi function rbf fundament differ two type,"comparison,mlp","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,game-ai,classification,algorithm,image-recognition,training,tensorflow,genetic-algorithms,computer-vision,backpropagation"
216,"<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>

<p>So are neural networks and its variants the only way to reach ""true"" artificial intelligence? </p>
",according knowledge current artificial intelligence study uses kind neural network variants  good example would deepmind alphago believe deep neural network  vision cnn  text  music ordered features rnn  etc  machine learning application neural networks  support vector machines  random forest  regression methods  etc  available applications    neural networks variants way reach  true  artificial intelligence   ,accord knowledg current artifici intellig studi use kind neural network variant good exampl would deepmind alphago believ deep neural network vision cnn text music order featur rnn etc machin learn applic neural network support vector machin random forest regress method etc avail applic neural network variant way reach true artifici intellig,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,classification,training,game-ai,image-recognition,tensorflow,backpropagation,algorithm,genetic-algorithms,recurrent-neural-networks"
220,"<p>I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?</p>
",interested hardware implementation anns  artificial neural networks   popular existing technology implementations form microchips purpose designed run artificial neural networks  example  chip optimised application like image recognition something similar  ,interest hardwar implement ann artifici neural network popular exist technolog implement form microchip purpos design run artifici neural network exampl chip optimis applic like imag recognit someth similar,"image-recognition,hardware","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,ai-design,reinforcement-learning,classification,training,tensorflow,game-ai,backpropagation,computer-vision,deep-network,genetic-algorithms"
223,"<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>

<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>

<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>
",noticed questions site mention genetic algorithms made realize really know much   heard term  something ever used  much idea work good  know involve sort evolution randomly changing values   give short explanation  preferably including sort practical example illustrates basic principles  ,notic question site mention genet algorithm made realiz realli know much heard term someth ever use much idea work good know involv sort evolut random chang valu give short explan prefer includ sort practic exampl illustr basic principl,genetic-algorithms,"neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,algorithm,game-ai,image-recognition,tensorflow,classification,training,genetic-algorithms,philosophy,backpropagation"
224,"<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This ""puzzle"" aspect of detective novels is part of the attraction.</p>

<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>
An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>

<p>Has an AI ever been able to solve a detective mystery?</p>
",detective novels  point often reader gets enough information solve crime   puzzle  aspect detective novels part attraction   often difficulty humans keep track variables   events  items  motivations  ai would easier time keeping track details  would rely real world knowledge prevent making crazy mistakes  example  stated character took train  ai would need know method transportation   changes location property agent time   ai ever able solve detective mystery  ,detect novel point often reader get enough inform solv crime puzzl aspect detect novel part attract often difficulti human keep track variabl event item motiv ai would easier time keep track detail would reli real world knowledg prevent make crazi mistak exampl state charact took train ai would need know method transport chang locat properti agent time ai ever abl solv detect mysteri,"natural-language,problem-solving,world-knowledge","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,philosophy,algorithm,image-recognition,training,tensorflow,classification,game-theory,genetic-algorithms"
229,"<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p>

<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>

<p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p>

<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p>
",      seymour papert marvin minsky showed perceptrons could learn xor function     solved backpropagation network least one hidden layer  type network learn xor function   believe taught every problem could learnt backpropagation neural network multiple hidden layers  could also learnt backpropagation neural network single hidden layer   although possible nonlinear activation function required    however  unclear limits backpropagation neural networks  patterns cannot learnt backpropgation neural network  ,seymour papert marvin minski show perceptron could learn xor function solv backpropag network least one hidden layer type network learn xor function believ taught everi problem could learnt backpropag neural network multipl hidden layer could also learnt backpropag neural network singl hidden layer although possibl nonlinear activ function requir howev unclear limit backpropag neural network pattern learnt backpropg neural network,"neural-networks,machine-learning,backpropagation,learning-theory","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,backpropagation,classification,game-ai,training,algorithm,image-recognition,genetic-algorithms,recurrent-neural-networks"
230,"<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p>

<p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p>

<p>I therefore have two questions:</p>

<ol>
<li>Practitioners - What do you find to be the main obstacles to
applying DL 'out of the box' to your problem? </li>
<li>Researchers - What
techniques do you use (or have developed) that might help address
practical issues? Are they within DL or do they offer an
alternative approach?</li>
</ol>
",last    years  rise fall rise popularity neural nets acted something  barometer  ai research   clear questions site people interested applying deep learning  dl  wide variety difficult problems   therefore two questions    practitioners   find main obstacles applying dl  box  problem   researchers   techniques use  developed  might help address practical issues  within dl offer alternative approach   ,last year rise fall rise popular neural net act someth baromet ai research clear question site peopl interest appli deep learn dl wide varieti difficult problem therefor two question practition find main obstacl appli dl box problem research techniqu use develop might help address practic issu within dl offer altern approach,deep-learning,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,image-recognition,classification,philosophy,training,genetic-algorithms,tensorflow,research"
231,"<p>Is it possible for <em>unsupervised learning</em> to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?</p>
",possible unsupervised learning learn high level  class specific features given unlabelled images  example detecting human animal faces    ,possibl unsupervis learn learn high level class specif featur given unlabel imag exampl detect human anim face,"image-recognition,unsupervised-learning","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,reinforcement-learning,image-recognition,classification,ai-design,tensorflow,training,algorithm,computer-vision,game-ai,philosophy,self-learning"
240,"<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p>

<ul>
<li>Dendrites - acts as the input vector,</li>
<li>Soma - acts as the summation function,</li>
<li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li>
</ul>

<p>I've checked <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer"">Deep learning</a> wiki page, but I couldn't find any references to dendrites, soma or axons.</p>

<p>So my question is, which type of artificial neural network implements or can mimic such model most closely?</p>
",wikipedia page read basic structure artificial neuron  model biological neurons  consist    dendrites   acts input vector  soma   acts summation function  axon   gets signal summation behavior occurs inside soma    checked deep learning wiki page  find references dendrites  soma axons   question  type artificial neural network implements mimic model closely  ,wikipedia page read basic structur artifici neuron model biolog neuron consist dendrit act input vector soma act summat function axon get signal summat behavior occur insid soma check deep learn wiki page find refer dendrit soma axon question type artifici neural network implement mimic model close,artificial-neuron,"neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,tensorflow,keras,classification,training,game-ai,algorithm,backpropagation,image-recognition,q-learning"
254,"<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=""https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based"" rel=""nofollow"">BCI/EEG devices</a>?</p>

<p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p>

<p>If so, did any of those studies show some degree of success?</p>
",studies attempted use ai algorithms detect human thoughts emotions based brain activity  using bci eeg devices    mean simple guesses whether person happy angry  object  e g  banana  car  thinking    studies show degree success  ,studi attempt use ai algorithm detect human thought emot base brain activ use bci eeg devic mean simpl guess whether person happi angri object e g banana car think studi show degre success,"research,signal-processing","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,image-recognition,tensorflow,training,philosophy,classification,genetic-algorithms,computer-vision"
265,"<p>Has there been any attempts to deploy AI with blockchain technology? </p>

<p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p>
",attempts deploy ai blockchain technology    decentralized examples ai networks central point control ai nodes acting independently  according codified set rules  creating  validating storing shared decentralized database many locations around world  ,attempt deploy ai blockchain technolog decentr exampl ai network central point control ai node act independ accord codifi set rule creat valid store share decentr databas mani locat around world,untagged,"neural-networks,machine-learning,ai-design,deep-learning,game-ai,philosophy,convolutional-neural-networks,reinforcement-learning,strong-ai,genetic-algorithms,algorithm,training,image-recognition,tensorflow,research"
268,"<p>In their famous book entitled ""<em>Perceptrons: An Introduction to Computational Geometry</em>"", Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p>

<p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p>
",famous book entitled  perceptrons  introduction computational geometry   minsky papert show perceptron solve xor problem  contributed first ai winter  resulting funding cuts neural networks  however  know multilayer perceptron solve xor problem easily   backprop known time  know manually building multilayer perceptrons  minsky  amp  papert know multilayer perceptrons could solve xor time wrote book  albeit knowing train  ,famous book entitl perceptron introduct comput geometri minski papert show perceptron solv xor problem contribut first ai winter result fund cut neural network howev know multilay perceptron solv xor problem easili backprop known time know manual build multilay perceptron minski amp papert know multilay perceptron could solv xor time wrote book albeit know train,"neural-networks,history","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,algorithm,classification,game-ai,tensorflow,image-recognition,philosophy,genetic-algorithms,computer-vision"
269,"<p>According to wikipedia <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial general intelligence(AGI)</a></p>

<blockquote>
  <p>Artificial general intelligence (AGI) is the intelligence of a
  (hypothetical) machine that could successfully perform any
  intellectual task that a human being can. </p>
</blockquote>

<p>According to below image todays artifical intellgence is same as that of a lizards.</p>

<p><a href=""https://i.stack.imgur.com/gddKB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gddKB.jpg"" alt=""enter image description here""></a></p>

<p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=""http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/"" rel=""nofollow noreferrer"">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p>
",according wikipedia artificial general intelligence agi      artificial general intelligence  agi  intelligence    hypothetical  machine could successfully perform   intellectual task human     according image todays artifical intellgence lizards     lets assume  within       years humans successful creating agi agis  agi intelligence emotions humans according wikipedia definition perform intellectual task human  destroy agi without consent  considered murder  ,accord wikipedia artifici general intellig agi artifici general intellig agi intellig hypothet machin could success perform intellectu task human accord imag today artif intellg lizard let assum within year human success creat agi agi agi intellig emot human accord wikipedia definit perform intellectu task human destroy agi without consent consid murder,ethics,"machine-learning,neural-networks,philosophy,deep-learning,ai-design,image-recognition,convolutional-neural-networks,game-ai,strong-ai,reinforcement-learning,algorithm,agi,singularity,classification,definitions"
270,"<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>
",deep mind published lot works deep learning last years  state art respective tasks  much work actually reproduced ai community  instance  neural turing machine paper seems hard reproduce  according researchers  ,deep mind publish lot work deep learn last year state art respect task much work actual reproduc ai communiti instanc neural ture machin paper seem hard reproduc accord research,"neural-networks,deep-learning,research","neural-networks,machine-learning,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,game-ai,tensorflow,training,philosophy,classification,genetic-algorithms,research,algorithm,image-recognition"
273,"<p>Geoffrey Hinton has been researching something he calls ""capsules theory"" in neural networks. What is this and how does it work?</p>
",geoffrey hinton researching something calls  capsules theory  neural networks  work  ,geoffrey hinton research someth call capsul theori neural network work,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,classification,backpropagation,tensorflow,recurrent-neural-networks,image-recognition,game-ai,genetic-algorithms,deep-network"
274,"<p>During my research, I've stumbled upon ""complex-valued neural networks"", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p>
",research  stumbled upon  complex valued neural networks   neural networks work complex valued inputs  probably weights   advantages  simply applications  kind neural network real valued neural networks  ,research stumbl upon complex valu neural network neural network work complex valu input probabl weight advantag simpli applic kind neural network real valu neural network,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,backpropagation,tensorflow,ai-design,classification,training,game-ai,image-recognition,recurrent-neural-networks,genetic-algorithms,deep-network"
275,"<p>The <a href=""http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/"" rel=""noreferrer"">author</a> claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p>
",author claims guiding evolution novelty alone  without explicit goals  solve problems even better using explicit goals  words  using novelty measure fitness function genetic algorithm works better goal directed fitness function  possible  ,author claim guid evolut novelti alon without explicit goal solv problem even better use explicit goal word use novelti measur fit function genet algorithm work better goal direct fit function possibl,genetic-algorithms,"neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,algorithm,genetic-algorithms,game-ai,tensorflow,training,classification,image-recognition,natural-language,deep-network"
276,"<p>Quote from this <a href=""https://ai.meta.stackexchange.com/a/46/8"">Eric's meta post</a> about modelling and implementation:</p>

<blockquote>
  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them ""computable"", as in runnable on a computer).</p>
</blockquote>

<p>If they're not the same, what is the difference?</p>

<p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p>

<p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p>
",quote eric meta post modelling implementation      exactly  although strongly related  difficult lesson learn among mathematicians early programmers  notably    mathematical proofs demand lot non trivial programming work make  computable   runnable computer      difference   say talking ai implementation  modelling  suggested easy task  draw line talk   asking general  specifically site  posted question meta ,quot eric meta post model implement exact although strong relat difficult lesson learn among mathematician earli programm notabl mathemat proof demand lot non trivial program work make comput runnabl comput differ say talk ai implement model suggest easi task draw line talk ask general specif site post question meta,"models,comparison,implementation","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,tensorflow,game-ai,keras,training,philosophy,classification,algorithm,image-recognition,computer-vision"
280,"<p>Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?</p>

<p>In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network.</p>
",given pictures multiple features faces  single ai algorithm detect  better reliability preferred use separate instances   words talking attempt finding possible human faces picture single neural network  ,given pictur multipl featur face singl ai algorithm detect better reliabl prefer use separ instanc word talk attempt find possibl human face pictur singl neural network,"deep-network,algorithm,image-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,game-ai,image-recognition,algorithm,training,classification,tensorflow,philosophy,genetic-algorithms,natural-language"
281,"<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p>

<p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p>

<p><em><sup>1</sup></em> <a href=""http://www.developer.com/lang/php/creating-neural-networks-in-php.html"" rel=""nofollow noreferrer"">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=""https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there"">https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p>
",read information  attempts build neural networks php programming language  personally think php right language probably high level language  assume low level language way suitable ai terms performance scalability    good logical reason use php language write ai     http   www developer com lang php creating neural networks php html https   stackoverflow com questions         artificial intelligence projects php  ,read inform attempt build neural network php program languag person think php right languag probabl high level languag assum low level languag way suitabl ai term perform scalabl good logic reason use php languag write ai http www develop com lang php creat neural network php html https stackoverflow com question artifici intellig project php,"neural-networks,programming-languages","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,game-ai,natural-language,philosophy,training,nlp,image-recognition,classification,tensorflow,genetic-algorithms"
283,"<p>I've found <a href=""http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2"" rel=""nofollow"">this old scientific paper from 1988</a> about introduction of AI into nuclear power fields.</p>

<p>Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?</p>

<p>Especially applications to the core, like cooling systems and other components which can be affected in negative way.</p>
",found old scientific paper      introduction ai nuclear power fields   still dangers application algorithm  nuclear power plants human life risk algorithm fail   especially applications core  like cooling systems components affected negative way  ,found old scientif paper introduct ai nuclear power field still danger applic algorithm nuclear power plant human life risk algorithm fail especi applic core like cool system compon affect negat way,applications,"machine-learning,neural-networks,deep-learning,ai-design,algorithm,reinforcement-learning,game-ai,philosophy,image-recognition,convolutional-neural-networks,genetic-algorithms,nlp,research,training,classification"
285,"<p>How likely AI can fully replace pilots on commercial flights (including take off, landing and parking)?</p>

<p>Since we've self-driving cars already, is it likely to happen to commercial planes as well?</p>
",likely ai fully replace pilots commercial flights  including take  landing parking    since self driving cars already  likely happen commercial planes well  ,like ai fulli replac pilot commerci flight includ take land park sinc self drive car alreadi like happen commerci plane well,untagged,"neural-networks,machine-learning,ai-design,deep-learning,algorithm,game-ai,convolutional-neural-networks,training,image-recognition,philosophy,reinforcement-learning,research,tensorflow,classification,strong-ai"
290,"<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>

<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>
",much processing power needed emulate human brain  specifically  neural simulation  communication neurons processing certain data real time   understand may bit speculation possible accurate  sure data available research studies attempted estimate based current understanding human brain  ,much process power need emul human brain specif neural simul communic neuron process certain data real time understand may bit specul possibl accur sure data avail research studi attempt estim base current understand human brain,"hardware,neuromorphic-computing","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,classification,game-ai,philosophy,training,image-recognition,algorithm,tensorflow,computer-vision,artificial-neuron"
294,"<p><strong>The Situation:</strong>
A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.</p>

<p>Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. <a href=""http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed"" rel=""nofollow"">Breaking distance</a> at this speed is about 24m.</p>

<p><strong>The Question:</strong> Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.</p>

<p><strong>Possible Answers:</strong> The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (<em>thinking distance</em>).</p>

<p>Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans <em>thinking distance</em>). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.</p>

<p>Lastly and most unlikely, the car will not alter its course and proceed to drive forward.</p>

<p><sup>Do not attempt to do it to check;)</sup></p>
",situation  self driving car traveling maximum speed     mph     km h   middle empty street ability change lanes sides  two passengers  one front another back   someone jumps side road directly path car  collision would occur    meters  breaking distance speed     question  known current implementation google car ai would react  currently matter speculation  step step explanation ai decisioning process would preferred   possible answers  car could activate brakes immediately  coming halt quickly possible  would sooner human could stop  people require time recognize possibility collision  physically slam brake   thinking distance    alternatively  car could continue traveling forward  processing situation   similar humans thinking distance   person may continue move  either way  still danger hit  case  car may decide change lanes attempt pass around person   lastly unlikely  car alter course proceed drive forward   attempt check   ,situat self drive car travel maximum speed mph km h middl empti street abil chang lane side two passeng one front anoth back someon jump side road direct path car collis would occur meter break distanc speed question known current implement googl car ai would react current matter specul step step explan ai decis process would prefer possibl answer car could activ brake immedi come halt quick possibl would sooner human could stop peopl requir time recogn possibl collis physic slam brake think distanc altern car could continu travel forward process situat similar human think distanc person may continu move either way still danger hit case car may decid chang lane attempt pass around person last unlik car alter cours proceed drive forward attempt check,"self-driving,decision-theory","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,game-ai,algorithm,convolutional-neural-networks,philosophy,image-recognition,training,genetic-algorithms,tensorflow,game-theory,classification"
296,"<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p>

<p>In games like Age of Empires, for example.</p>
",artificial intelligence present many games  current older games  intelligence understand  mean  behave like human game  allowing play  ai plays   games like age empires  example  ,artifici intellig present mani game current older game intellig understand mean behav like human game allow play ai play game like age empir exampl,gaming,"neural-networks,machine-learning,deep-learning,game-ai,reinforcement-learning,ai-design,philosophy,tensorflow,convolutional-neural-networks,game-theory,combinatorial-games,algorithm,gaming,q-learning,research"
299,"<p><a href=""https://cs.stackexchange.com/a/60535/54605"">At a related question in Computer Science SE</a>, a user told:</p>

<blockquote>
  <p>Neural networks typically require a large training set.</p>
</blockquote>

<p>Is there a way to define the boundaries of the ""optimal"" size of a training set in general case?</p>

<p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p>

<p>Is there such a method that can be applicable for an already defined neural network topology? </p>
",related question computer science se  user told      neural networks typically require large training set    way define boundaries  optimal  size training set general case   learning fuzzy logic  heard rules thumb involved examining mathematical composition problem using define number fuzzy sets   method applicable already defined neural network topology   ,relat question comput scienc se user told neural network typic requir larg train set way defin boundari optim size train set general case learn fuzzi logic heard rule thumb involv examin mathemat composit problem use defin number fuzzi set method applic alreadi defin neural network topolog,"neural-networks,training,optimization","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,training,ai-design,tensorflow,classification,game-ai,image-recognition,algorithm,genetic-algorithms,deep-network,backpropagation"
308,"<p>How important is true (non-<a href=""https://en.wikipedia.org/wiki/Pseudorandomness"" rel=""noreferrer"" title=""pseudo"">pseudo</a>) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?</p>
",important true  non pseudo  randomness artificial intelligence designs  chance pseudo randomness could barrier successful designs  ,import true non pseudo random artifici intellig design chanc pseudo random could barrier success design,ai-design,"neural-networks,machine-learning,deep-learning,reinforcement-learning,tensorflow,ai-design,philosophy,q-learning,convolutional-neural-networks,classification,algorithm,keras,game-ai,training,definitions"
309,"<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p>

<p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p>

<p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p>
",complex ai learns lexical semantic content meaning  collection words  structure dependencies  watson takes terabytes disk space   lets assume deepqa like ai consumed whole wikipedia size   g took amount structured unstructured stored content   learning another   g different encyclopedia  different topics language  take amount data  ai reuse existing structured take less half  like       additional space  ,complex ai learn lexic semant content mean collect word structur depend watson take terabyt disk space let assum deepqa like ai consum whole wikipedia size g took amount structur unstructur store content learn anoth g differ encyclopedia differ topic languag take amount data ai reus exist structur take less half like addit space,"watson,storage","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,training,tensorflow,natural-language,image-recognition,philosophy,classification,computer-vision"
310,"<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p>

<p>In other words, how does it know which precise answer it needs to return?</p>
",simple explanation watson finds scores evidence gathering massive evidence analyzing data   words  know precise answer needs return  ,simpl explan watson find score evid gather massiv evid analyz data word know precis answer need return,watson,"machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,natural-language,algorithm,training,classification,nlp,tensorflow,image-recognition,computer-vision,python"
323,"<p>Isaac Asimov's famous <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>

<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified"">modified the First Law</a>, <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added"">added a Fourth (or Zeroth) Law</a>, or even <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws"">removed all Laws altogether</a>.</p>

<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>
",isaac asimov famous three laws robotics originated context asimov science fiction stories  stories  three laws serve safety measure  order avoid untimely manipulated situations exploding havoc   often  asimov narratives would find way break  leading writer make several modifications laws  instance  stories  modified first law  added fourth  zeroth  law  even removed laws altogether   however  easy argue  popular culture  even field ai research  laws robotics taken quite seriously  ignoring side problem different  subjective  mutually exclusive interpretations laws  arguments proving laws intrinsically flawed design   alternatively  strong enough use reality  likewise  better  stricter security heuristics set designed purpose  ,isaac asimov famous three law robot origin context asimov scienc fiction stori stori three law serv safeti measur order avoid untim manipul situat explod havoc often asimov narrat would find way break lead writer make sever modif law instanc stori modifi first law ad fourth zeroth law even remov law altogeth howev easi argu popular cultur even field ai research law robot taken quit serious ignor side problem differ subject mutual exclus interpret law argument prove law intrins flaw design altern strong enough use realiti likewis better stricter secur heurist set design purpos,asimovs-laws,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,image-recognition,philosophy,training,classification,genetic-algorithms,tensorflow,game-theory"
326,"<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=""https://ai.stackexchange.com/q/92/8"">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>

<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>

<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>

<p>Any suggestions or research has been done?</p>
",modern techniques generating textual captcha  person needs type right text  challenges easily fool ai visual obfuscation methods  time human solve without struggle   example talking plain ability recognising text embedded image  without considering external plugins like flash java  image classification  etc   typing text written something similar   guess adding noise  gradient  rotating letters changing colours reliable methods  since quickly broken   suggestions research done  ,modern techniqu generat textual captcha person need type right text challeng easili fool ai visual obfusc method time human solv without struggl exampl talk plain abil recognis text embed imag without consid extern plugin like flash java imag classif etc type text written someth similar guess ad nois gradient rotat letter chang colour reliabl method sinc quick broken suggest research done,"image-recognition,research,ocr","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,ai-design,image-recognition,reinforcement-learning,classification,algorithm,game-ai,training,computer-vision,tensorflow,philosophy,nlp"
329,"<p>Can an AI program have an EQ (Emotional intelligence or emotional quotient)?</p>

<p>In other words, can the EQ of an AI program be measured?</p>

<p>If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?</p>
",ai program eq  emotional intelligence emotional quotient    words  eq ai program measured   eq problematic measure iq  least standard applicaple humans ai programs   case  ,ai program eq emot intellig emot quotient word eq ai program measur eq problemat measur iq least standard applicapl human ai program case,"emotional-intelligence,intelligence-testing","machine-learning,neural-networks,ai-design,philosophy,game-ai,deep-learning,strong-ai,natural-language,algorithm,game-theory,genetic-algorithms,research,nlp,reinforcement-learning,terminology"
330,"<p>I have heard about this concept in a Reddit post about Alpha Go. I have tried to go through the paper and the article, but could not really make sense of the algorithm.</p>

<p>So, can someone give an easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p>
",heard concept reddit post alpha go  tried go paper article  could really make sense algorithm    someone give easy understand explanation monte carlo search algorithm work used building game playing ai bots  ,heard concept reddit post alpha go tri go paper articl could realli make sens algorithm someon give easi understand explan mont carlo search algorithm work use build game play ai bot,"gaming,monte-carlo-search","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,game-ai,algorithm,convolutional-neural-networks,genetic-algorithms,tensorflow,image-recognition,philosophy,training,classification,combinatorial-games"
332,"<p>DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.</p>

<p>Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?</p>

<p>Is this even slightly feasible? </p>
",dnns typically used classify things  course  let go wild sounds tell think sounds good  like think training class made  perhaps comparing output existing song  could get nn basic concept music   timing would issue  sure feasible  strongly weighted input attached hidden layers perhaps  use bias   even slightly feasible   ,dnns typic use classifi thing cours let go wild sound tell think sound good like think train class made perhap compar output exist song could get nn basic concept music time would issu sure feasibl strong weight input attach hidden layer perhap use bias even slight feasibl,"deep-network,machine-learning","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,training,classification,algorithm,game-ai,image-recognition,backpropagation,keras,genetic-algorithms"
334,"<p>How do I avoid my gradient descent algorithm into falling into the ""local minima"" trap while backpropogating on my neural network?</p>

<p>Are there any methods which help me avoid it?</p>
",avoid gradient descent algorithm falling  local minima  trap backpropogating neural network   methods help avoid  ,avoid gradient descent algorithm fall local minima trap backpropog neural network method help avoid,"neural-networks,backpropagation,gradient-descent","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,backpropagation,ai-design,algorithm,classification,training,game-ai,genetic-algorithms,tensorflow,image-recognition,recurrent-neural-networks"
335,"<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>

<p>Is this technique beneficial for examining neural networks?</p>
",neural network directed weighted graph  represented  sparse  matrix  expose elegant properties network   technique beneficial examining neural networks  ,neural network direct weight graph repres spars matrix expos eleg properti network techniqu benefici examin neural network,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,backpropagation,training,tensorflow,classification,ai-design,recurrent-neural-networks,game-ai,genetic-algorithms,image-recognition,deep-network"
345,"<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p>

<p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p>

<p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p>
",would ethical implement ai self defence public walking robots exposed dangers violence crime robbery  parts   damage abduction   would pros cons ai behavior  realistic  taken account obvious reasons   like pushing back somebody somebody start pushing first  ai say  pushed first   running away crowded street case algorithm detect risk abduction  ,would ethic implement ai self defenc public walk robot expos danger violenc crime robberi part damag abduct would pros con ai behavior realist taken account obvious reason like push back somebodi somebodi start push first ai say push first run away crowd street case algorithm detect risk abduct,"ethics,decision-theory,robots","machine-learning,neural-networks,ai-design,deep-learning,reinforcement-learning,game-ai,algorithm,philosophy,convolutional-neural-networks,training,image-recognition,genetic-algorithms,strong-ai,game-theory,research"
348,"<p>Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through <a href=""https://watson-api-explorer.mybluemix.net/"" rel=""nofollow"">API</a>?</p>

<p><sup>Something similar happened in the future in <a href=""https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)"" rel=""nofollow""><strong>The Time Machine</strong> movie from 2002</a>.</sup></p>

<p>Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.</p>

<p>What are the pros and cons of such a change?</p>
",risk near future replacing encyclopedias watson like ai knowledge accessible everybody api   something similar happened future time machine movie        obviously maintaining    million articles keeping date consistent could beyond brain power thousands active editors  mention thousands encyclopedias including paperback version large number books used universities needs updated every year huge number people   pros cons change  ,risk near futur replac encyclopedia watson like ai knowledg access everybodi api someth similar happen futur time machin movi obvious maintain million articl keep date consist could beyond brain power thousand activ editor mention thousand encyclopedia includ paperback version larg number book use univers need updat everi year huge number peopl pros con chang,watson,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,philosophy,training,image-recognition,classification,tensorflow,genetic-algorithms,nlp"
350,"<p>I've watched the <a href=""https://www.youtube.com/watch?v=LY7x2Ihqjmc"" rel=""nofollow"">Sunspring</a> video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.</p>

<p>What was the mechanism of creating such screenplay?</p>

<p>On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?</p>
",watched sunspring video make sense  lot nonsense monologues   mainly created jetson ai   mechanism creating screenplay   criteria trained  goal motivation terms training criteria defining text make sense  missed  bad  possibly could improved  ,watch sunspr video make sens lot nonsens monologu main creat jetson ai mechan creat screenplay criteria train goal motiv term train criteria defin text make sens miss bad possibl could improv,algorithm,"machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,tensorflow,reinforcement-learning,training,game-ai,classification,image-recognition,philosophy,algorithm,computer-vision,keras"
352,"<p>This <a href=""http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/"" rel=""nofollow noreferrer"">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p>

<p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p>

<p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=""https://ai.stackexchange.com/q/154/8"">math</a> equations, predicting <a href=""https://ai.stackexchange.com/q/225/8"">pseudo-random lists</a>, <a href=""https://ai.stackexchange.com/q/168/8"">fluid mechanics</a>, guessing encryption algorithms, or <a href=""https://ai.stackexchange.com/q/205/8"">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p>

<p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than ""deep"" architectures cannot?</p>
",article suggests deep learning designed produce universal algorithm cannot used create complex systems   first requires huge amounts computing power  time effort train algorithm right way adding extra layers really help solve complex problems cannot easily predicted   secondly tasks extremely difficult impossible solve using dnn  like solving math equations  predicting pseudo random lists  fluid mechanics  guessing encryption algorithms  decompiling unknown formats  simple mapping input output   asking  alternative learning algorithms powerful deep architectures general purpose problem solving  solve variety problems   deep  architectures cannot  ,articl suggest deep learn design produc univers algorithm use creat complex system first requir huge amount comput power time effort train algorithm right way ad extra layer realli help solv complex problem easili predict second task extrem difficult imposs solv use dnn like solv math equat predict pseudo random list fluid mechan guess encrypt algorithm decompil unknown format simpl map input output ask altern learn algorithm power deep architectur general purpos problem solv solv varieti problem deep architectur,"deep-network,comparison,architecture","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,algorithm,ai-design,tensorflow,classification,training,game-ai,image-recognition,genetic-algorithms,backpropagation,deep-network"
357,"<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p>

<p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=""https://opendata.stackexchange.com/q/3553/3082"">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p>

<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>
",research study application ai chemistry predict output certain chemical reactions   example  train ai current compounds  substances  structures products chemical reactions existing dataset  basically produce   give task find create gold silver group available substances  algorithm find chemical reactions  successfully predicting new one dataset  gives results  maybe gold good example  practical scenario would creation drugs cheaper create using much simpler processes synthesizing substances first time drug industries   successful research attempting achieve using deep learning algorithms  ,research studi applic ai chemistri predict output certain chemic reaction exampl train ai current compound substanc structur product chemic reaction exist dataset basic produc give task find creat gold silver group avail substanc algorithm find chemic reaction success predict new one dataset give result mayb gold good exampl practic scenario would creation drug cheaper creat use much simpler process synthes substanc first time drug industri success research attempt achiev use deep learn algorithm,"deep-learning,research,prediction","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,classification,training,tensorflow,image-recognition,genetic-algorithms,philosophy,game-theory"
358,"<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>

<p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p>

<p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p>
",assume want solve issue neural network either fit already existing topologies  perceptron  konohen  etc  simply aware existence unable understand mechanics rely instead   deconstruct problem find corresponding neural network topology  mean size certain layers  number  type activation functions  number direction connections    beginner  yet realized topologies   least perceptrons  hard impossible understand inner mechanics neurons hidden layers express mathematically meaningful context  ,assum want solv issu neural network either fit alreadi exist topolog perceptron konohen etc simpli awar exist unabl understand mechan reli instead deconstruct problem find correspond neural network topolog mean size certain layer number type activ function number direct connect beginn yet realiz topolog least perceptron hard imposs understand inner mechan neuron hidden layer express mathemat meaning context,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,tensorflow,ai-design,classification,algorithm,game-ai,genetic-algorithms,backpropagation,image-recognition,training,recurrent-neural-networks"
359,"<p>For example there is <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow"">the MNIST database</a> which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.</p>

<p>Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?</p>
",example mnist database used test artificial neural network  ann   however challenging  hierarchical systems convolutional neural networks manages get error rate      percent   similar  especially challenging tasks dataset used benchmark tests challenge ai fairly reliable possible pass  aan struggling achieve lower error rate  ,exampl mnist databas use test artifici neural network ann howev challeng hierarch system convolut neural network manag get error rate percent similar especi challeng task dataset use benchmark test challeng ai fair reliabl possibl pass aan struggl achiev lower error rate,"image-recognition,deep-learning,datasets","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,tensorflow,classification,game-ai,backpropagation,image-recognition,algorithm,genetic-algorithms,keras"
360,"<p>This <a href=""http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf"" rel=""nofollow"">study</a> (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks </p>

<p>Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?</p>
",study  pages      shows attempt recognizing traffic signs lower error rates using multi column deep neural networks   google cars using similar techniques predicting signs using dnn  using method  ,studi page show attempt recogn traffic sign lower error rate use multi column deep neural network googl car use similar techniqu predict sign use dnn use method,"deep-network,image-recognition,self-driving,classification,cars","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,training,game-ai,image-recognition,classification,algorithm,backpropagation,deep-network,genetic-algorithms"
361,"<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=""https://ai.stackexchange.com/q/237/8"">ANN on microchips</a>, but brain simulations.</p>
",like know whether attempts simulate whole brain  talking ann microchips  brain simulations  ,like know whether attempt simul whole brain talk ann microchip brain simul,neuromorphic-computing,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,philosophy,reinforcement-learning,algorithm,game-theory,training,game-ai,image-recognition,artificial-neuron,classification,nlp"
363,"<p>On <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"">the wikipedia page</a> about AI, we can read:</p>

<blockquote>
  <p>Optical character recognition is no longer perceived as an exemplar of ""artificial intelligence"" having become a routine technology.</p>
</blockquote>

<p>On the other hand, the <a href=""https://en.wikipedia.org/wiki/MNIST_database"">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=""https://en.wikipedia.org/wiki/MNIST_database#Classifiers"">Classifiers</a>).</p>

<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>
",wikipedia page ai  read      optical character recognition longer perceived exemplar  artificial intelligence  become routine technology    hand  mnist database handwritten digits especially designed training testing neural networks error rates  see  classifiers    quote state ocr longer exemplar ai  ,wikipedia page ai read optic charact recognit longer perceiv exemplar artifici intellig becom routin technolog hand mnist databas handwritten digit especi design train test neural network error rate see classifi quot state ocr longer exemplar ai,ocr,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,tensorflow,reinforcement-learning,training,game-ai,classification,philosophy,image-recognition,strong-ai,algorithm,backpropagation"
364,"<p><a href=""https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test"" rel=""nofollow"">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>

<ul>
<li>Is Earth a planet?</li>
<li>Is the sun bigger than my foot?</li>
<li>Do people sometimes lie?</li>
<li>etc.</li>
</ul>

<p>Have any AI attempted and passed this test to date?</p>
",mist quantiative test humanness  consisting    k propositions    earth planet  sun bigger foot  people sometimes lie  etc    ai attempted passed test date  ,mist quantiat test human consist k proposit earth planet sun bigger foot peopl sometim lie etc ai attempt pass test date,"history,intelligence-testing,turing-test,chat-bots","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,tensorflow,strong-ai,convolutional-neural-networks,research,classification,game-theory,training,algorithm,nlp"
368,"<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p>

<p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=""https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"" rel=""nofollow"">friendly</a>? Has there any research been done towards this?</p>
",possible normal code prove correct using mathematical techniques  often done ensure parts bug free    also prove piece code ai software cause never turn us  e  ai friendly  research done towards  ,possibl normal code prove correct use mathemat techniqu often done ensur part bug free also prove piec code ai softwar caus never turn us e ai friend research done toward,friendly-ai,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,philosophy,algorithm,tensorflow,image-recognition,strong-ai,genetic-algorithms,research,training"
371,"<p>In <a href=""http://arxiv.org/pdf/1606.00652.pdf"" rel=""nofollow"">this paper</a>, a proposal is given for what death could mean for Artificial Intelligence. </p>

<p>What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. </p>
",paper  proposal given death could mean artificial intelligence    mean using english  understand mathematical notation useful giving precise definition  like understand definition really means   ,paper propos given death could mean artifici intellig mean use english understand mathemat notat use give precis definit like understand definit realli mean,"research,definitions,death","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,algorithm,game-ai,tensorflow,image-recognition,training,classification,philosophy,natural-language,deep-network"
377,"<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>
I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>
",measure power machine number operation per second frequency processor  units similar iq humans exist ai  asking unit give countable result something different turing test give binary result  ,measur power machin number oper per second frequenc processor unit similar iq human exist ai ask unit give countabl result someth differ ture test give binari result,"machine-learning,classification,intelligence-testing","machine-learning,neural-networks,deep-learning,ai-design,game-ai,convolutional-neural-networks,reinforcement-learning,philosophy,classification,algorithm,training,image-recognition,tensorflow,strong-ai,game-theory"
382,"<p>In the mid 1980s, Rodney Brooks famously created the foundations of ""the new AI"". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p>

<p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p>

<p>My question takes the form of a thought experiment and asks: ""Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?""</p>
",mid      rodney brooks famously created foundations  new ai   central claim symbolist approach  good old fashioned ai   gofai  failed attempting  cream cognition top   embodied cognition required  e  built bottom  hierarchy competances   e g  basic locomotion    wandering around    actively foraging  etc   imagine ai researchers would agree  embodied cognition  perspective  least tacitly  supplanted gofai mainstream   question takes form thought experiment asks      aspects  embodied  relaxed omitted lose something essential agi   ,mid rodney brook famous creat foundat new ai central claim symbolist approach good old fashion ai gofai fail attempt cream cognit top embodi cognit requir e built bottom hierarchi compet e g basic locomot wander around activ forag etc imagin ai research would agre embodi cognit perspect least tacit supplant gofai mainstream question take form thought experi ask aspect embodi relax omit lose someth essenti agi,"agi,gofai,embodied-cognition","machine-learning,neural-networks,ai-design,deep-learning,game-ai,reinforcement-learning,philosophy,convolutional-neural-networks,algorithm,strong-ai,genetic-algorithms,research,natural-language,game-theory,image-recognition"
383,"<p>In other words, which existing reinforcement method learns in fewest episodes? <a href=""http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf"" rel=""nofollow"">R-Max</a> comes to mind, but its very old and I'd like to know if there is something better now.</p>
",words  existing reinforcement method learns fewest episodes  r max comes mind  old like know something better  ,word exist reinforc method learn fewest episod r max come mind old like know someth better,"algorithm,research,reinforcement-learning","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,tensorflow,algorithm,q-learning,natural-language,game-ai,classification,training,image-recognition,nlp"
387,"<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=""https://en.wikipedia.org/wiki/Ex_Machina_(film)""><em>Ex Machina</a></em> or <em><a href=""https://en.wikipedia.org/wiki/I,_Robot_(film)"">I, Robot</em></a> movies?</p>

<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>
",research teams attempted create already created ai robot close intelligent found ex machina  robot movies   talking full awareness  artificial make decisions physical intellectual tasks human  ,research team attempt creat alreadi creat ai robot close intellig found ex machina robot movi talk full awar artifici make decis physic intellectu task human,"research,agi,robots","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,algorithm,strong-ai,convolutional-neural-networks,image-recognition,research,genetic-algorithms,game-theory,nlp"
390,"<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p>

<p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p>
", humans  following multiple processes  e g  reading listening music  memorize information less focused sources worse efficiency main concentration   things exist case artificial intelligences  doubt  example neural networks obtain features  may wrong  ,human follow multipl process e g read listen music memor inform less focus sourc wors effici main concentr thing exist case artifici intellig doubt exampl neural network obtain featur may wrong,structured-data,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,philosophy,classification,image-recognition,game-ai,training,tensorflow,genetic-algorithms,algorithm,backpropagation"
392,"<p>How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows <a href=""http://science.sciencemag.org/content/345/6198/795.abstract"" rel=""nofollow"">programmable self-assembly in a thousand-robot swarm</a> (see <a href=""http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/"" rel=""nofollow"">article</a> &amp; <a href=""https://vimeo.com/103329200"" rel=""nofollow"">video</a>) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.</p>

<p>Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!</p>
",swarm small robots  like kilobots  walking close achieve collaboration without bumping  example  one study shows programmable self assembly thousand robot swarm  see article  amp  video  moving without gps like system measuring distances neighbours  achieved  robots slow   way similar robots achieve much efficient quicker assembly using complex techniques coordination  walking around clock wise  guess easiest way   mean using sophisticated way  waiting half day     h  create simple star shape using thousand robot swarm way long  ,swarm small robot like kilobot walk close achiev collabor without bump exampl one studi show programm self assembl thousand robot swarm see articl amp video move without gps like system measur distanc neighbour achiev robot slow way similar robot achiev much effici quicker assembl use complex techniqu coordin walk around clock wise guess easiest way mean use sophist way wait half day h creat simpl star shape use thousand robot swarm way long,"robots,multi-agent-systems","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,algorithm,training,game-ai,image-recognition,tensorflow,classification,backpropagation,nlp,philosophy"
393,"<p>On Watson wiki page we can read:</p>

<blockquote>
  <p>In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.</p>
</blockquote>

<p>How exactly such AI can help doctors to diagnose the diseases?</p>
",watson wiki page read      healthcare  watson natural language  hypothesis generation  evidence based learning capabilities allow function clinical decision support system use medical professionals    exactly ai help doctors diagnose diseases  ,watson wiki page read healthcar watson natur languag hypothesi generat evid base learn capabl allow function clinic decis support system use medic profession exact ai help doctor diagnos diseas,"watson,healthcare","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,natural-language,training,tensorflow,image-recognition,philosophy,genetic-algorithms,classification"
395,"<p>Recently White House published the article: <a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow"">Preparing for the Future of Artificial Intelligence</a> which says that government is working to leverage AI for public good and toward a more effective government.</p>

<p>I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?</p>
",recently white house published article  preparing future artificial intelligence says government working leverage ai public good toward effective government   especially interested ai help computational sustainability  environmental management earth ecosystem biological conservation  ,recent white hous publish articl prepar futur artifici intellig say govern work leverag ai public good toward effect govern especi interest ai help comput sustain environment manag earth ecosystem biolog conserv,biology,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,strong-ai,reinforcement-learning,research,algorithm,convolutional-neural-networks,genetic-algorithms,terminology,image-recognition,game-theory"
397,"<p>When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a superintelligence or not?</p>
",ai narrow domain chess outperform world human masters chess  make superintelligence  ,ai narrow domain chess outperform world human master chess make superintellig,"definitions,deep-blue","machine-learning,neural-networks,ai-design,game-ai,deep-learning,philosophy,chess,strong-ai,game-theory,reinforcement-learning,algorithm,combinatorial-games,image-recognition,gaming,convolutional-neural-networks"
398,"<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p>

<p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p>

<ul>
<li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li>
<li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li>
</ul>
",suppose goal collaborate create advanced ai  instance one resembles human project would frontier ai research  kind skills would need   talking specific things like university program complete enter competent field  things thought  exemplify mean    computer sciences  obviously ai built computers  hurt know computers work  low level stuff machine specific things seem essential  may wrong course  psychology  ai resembles human beings  knowledge human cognition would probably useful  although imagine neurology cellular level complicated psychological quirks typical human beings like oedipus complex would relevant   may wrong   ,suppos goal collabor creat advanc ai instanc one resembl human project would frontier ai research kind skill would need talk specif thing like univers program complet enter compet field thing thought exemplifi mean comput scienc obvious ai built comput hurt know comput work low level stuff machin specif thing seem essenti may wrong cours psycholog ai resembl human knowledg human cognit would probabl use although imagin neurolog cellular level complic psycholog quirk typic human like oedipus complex would relev may wrong,research,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,algorithm,image-recognition,training,game-theory,research,genetic-algorithms,classification"
399,"<p><a href=""https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence"" rel=""nofollow"">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p>

<p>What are these exactly?</p>
",white house published information ai requests mentions  important research gaps ai must addressed advance field benefit public    exactly  ,white hous publish inform ai request mention import research gap ai must address advanc field benefit public exact,"research,ethics,social,reasoning","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,strong-ai,research,tensorflow,reinforcement-learning,game-theory,classification,image-recognition,history,algorithm"
401,"<p>Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?</p>
",methods artificial intelligence use recursion  solve certain issue keep working calculating  ,method artifici intellig use recurs solv certain issu keep work calcul,math,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,philosophy,game-ai,algorithm,tensorflow,training,classification,image-recognition,deep-network,genetic-algorithms"
411,"<p>The Von Neumann's <a href=""https://en.wikipedia.org/wiki/Minimax_theorem"" rel=""nofollow"">Minimax theorem</a> gives the conditions that make the <a href=""https://en.wikipedia.org/wiki/Max%E2%80%93min_inequality"" rel=""nofollow"">max-min inequality</a> an equality.</p>

<p>I understand the max-min inequality, basically <code>min(max(f))&gt;=max(min(f))</code>.</p>

<p>The Von Neumann's theorem states that, for the inequality to become an equality <code>f(.,y)</code> should always be convex for given y and <code>f(x,.)</code> should always be concave for given x, which also makes sense.</p>

<p><a href=""https://www.youtube.com/watch?v=m-EewaiFhF0&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp&amp;index=61"" rel=""nofollow"">This video</a> says that for a zero-sum perfect information game, the Von Neumann's theorem always holds, so that minimax always equal to maximin, which I did not quite follow.</p>

<p><strong>Questions</strong><br>
Why zero-sum perfect information games satisfy the conditions of Von Neumann's theorem?<br>
If we relax the rules to be non-zero-sum or non-perfect information, how would the conditions change?</p>
",von neumann minimax theorem gives conditions make max min inequality equality   understand max min inequality  basically min max f   gt  max min f     von neumann theorem states  inequality become equality f    always convex given f x    always concave given x  also makes sense   video says zero sum perfect information game  von neumann theorem always holds  minimax always equal maximin  quite follow   questions zero sum perfect information games satisfy conditions von neumann theorem  relax rules non zero sum non perfect information  would conditions change  ,von neumann minimax theorem give condit make max min inequ equal understand max min inequ basic min max f gt max min f von neumann theorem state inequ becom equal f alway convex given f x alway concav given x also make sens video say zero sum perfect inform game von neumann theorem alway hold minimax alway equal maximin quit follow question zero sum perfect inform game satisfi condit von neumann theorem relax rule non zero sum non perfect inform would condit chang,"game-theory,search,minimax","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,tensorflow,ai-design,algorithm,game-ai,q-learning,keras,game-theory,image-recognition,classification,nlp"
416,"<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=""http://arxiv.org/pdf/1410.6142v3.pdf"" rel=""noreferrer"">the ""Lovelace Test 2.0""</a>, after being inspired by the <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""noreferrer"">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p>

<p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p>

<blockquote>
  <p>The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:</p>
  
  <ul>
  <li><p>a must create an artifact o of type t;</p></li>
  <li><p>o must conform to a set of constraints C where ci ? C is
  any criterion expressible in natural language;</p></li>
  <li><p>a human evaluator h, having chosen t and C, is satisfied
  that o is a valid instance of t and meets C; and</p></li>
  <li><p>a human referee r determines the combination of t and C
  to not be unrealistic for an average human.</p></li>
  </ul>
</blockquote>

<p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p>

<p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p>
",october       dr  mark riedl published approach testing ai intelligence  called  lovelace test       inspired original lovelace test  published        mark believed original lovelace test would impossible pass  therefore  suggested weaker  practical version   lovelace test     makes assumption ai intelligent  must exhibit creativity  paper      lovelace     test follows  artificial agent challenged follows          must create artifact type    must conform set constraints c ci   c   criterion expressible natural language    human evaluator h  chosen c  satisfied   valid instance meets c    human referee r determines combination c   unrealistic average human       since possible human evaluator come pretty easy constraints ai beat  human evaluator expected keep coming complex constraints ai ai fails  point lovelace test     compare creativity different ais  provide definite dividing line  intelligence   nonintelligence  like turing test would   however  curious whether test actually used academic setting  seen thought experiment moment  lovelace test seems easy apply academic settings  need develop measurable constraints use test artificial agent   also may subjective  humans disagree merits certain constraints  whether creative artifact produced ai actually meets final result   ,octob dr mark riedl publish approach test ai intellig call lovelac test inspir origin lovelac test publish mark believ origin lovelac test would imposs pass therefor suggest weaker practic version lovelac test make assumpt ai intellig must exhibit creativ paper lovelac test follow artifici agent challeng follow must creat artifact type must conform set constraint c ci c criterion express natur languag human evalu h chosen c satisfi valid instanc meet c human refere r determin combin c unrealist averag human sinc possibl human evalu come pretti easi constraint ai beat human evalu expect keep come complex constraint ai ai fail point lovelac test compar creativ differ ai provid definit divid line intellig nonintellig like ture test would howev curious whether test actual use academ set seen thought experi moment lovelac test seem easi appli academ set need develop measur constraint use test artifici agent also may subject human disagre merit certain constraint whether creativ artifact produc ai actual meet final result,"history,intelligence-testing","machine-learning,neural-networks,deep-learning,ai-design,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,game-theory,tensorflow,algorithm,training,classification,strong-ai,image-recognition"
418,"<p>Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?</p>

<p>For example for classification of type of animals based on the training from huge dataset.</p>
",convolutional neural network leading type feed forward artificial neural network image recognition  used real time image recognition videos  frame frame   takes much processing  assuming written c like language    example classification type animals based training huge dataset  ,convolut neural network lead type feed forward artifici neural network imag recognit use real time imag recognit video frame frame take much process assum written c like languag exampl classif type anim base train huge dataset,"convolutional-neural-networks,classification,performance,real-time","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,classification,ai-design,tensorflow,training,reinforcement-learning,game-ai,computer-vision,algorithm,keras,backpropagation"
422,"<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>

<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>

<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>
",purpose learning like classify likeliness tweet aggressive language    wondering approach problem  guess need first train neural network huge dataset text aggressive language  brings question would get data first place   feels bit like chicken egg problem wonder would approach problem  ,purpos learn like classifi likeli tweet aggress languag wonder approach problem guess need first train neural network huge dataset text aggress languag bring question would get data first place feel bit like chicken egg problem wonder would approach problem,"classification,datasets","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,classification,training,algorithm,tensorflow,image-recognition,game-ai,genetic-algorithms,natural-language,nlp"
425,"<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>
So are they actual AI programs or not?</p>

<p>(By ""question"" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>
",siri cortana communicate pretty much like humans  unlike google mainly gives us search results asked questions  setting alarms reminders   siri cortana provide us answer  way person would  actual ai programs     question  mean academic related question asking routes  temperature  rather opinion based question    ,siri cortana communic pretti much like human unlik googl main give us search result ask question set alarm remind siri cortana provid us answer way person would actual ai program question mean academ relat question ask rout temperatur rather opinion base question,"emotional-intelligence,intelligence-testing,natural-language,applications","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,game-ai,philosophy,nlp,classification,training,image-recognition,natural-language,research"
426,"<p>The question is pretty much the title.</p>

<p>Basically what is the difference between AI and robots?</p>
",question pretty much title   basically difference ai robots  ,question pretti much titl basic differ ai robot,"comparison,robots","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,image-recognition,strong-ai,algorithm,nlp,research,training,natural-language"
435,"<p>With typical machine learning you would usually use a training data-set to create a model of some kind, and a testing data-set to then test the newly created model. For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data. You would then take the prediction that the model returned and compare that to the actual data in the testing set. How would a validation set be used here?</p>

<p>With nearest neighbor you would use the training data to create an n-dimensional space that has all the features of the training set. You would then use this space to classify the features in the testing data. Again you would compare these predictions to the actual value of the data. How would a validation set help here as well?</p>
",typical machine learning would usually use training data set create model kind  testing data set test newly created model  something like linear regression model created training data equation would use predict outcome set features testing data  would take prediction model returned compare actual data testing set  would validation set used   nearest neighbor would use training data create n dimensional space features training set  would use space classify features testing data  would compare predictions actual value data  would validation set help well  ,typic machin learn would usual use train data set creat model kind test data set test newli creat model someth like linear regress model creat train data equat would use predict outcom set featur test data would take predict model return compar actual data test set would valid set use nearest neighbor would use train data creat n dimension space featur train set would use space classifi featur test data would compar predict actual valu data would valid set help well,"machine-learning,linear-regression","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,training,classification,keras,image-recognition,algorithm,game-ai,computer-vision,linear-regression"
438,"<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>

<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>
",reinforcement learning  mean class machine learning algorithms deepq  etc  mind general concept learning based rewards punishment    possible create strong ai rely learning reinforcement  reinforcement learning requirement artificial intelligence  existence rewards punishment imply existence favorable unfavorable world states  must intelligence general artificial intelligence particular way classifying world states favorable unfavorable    ,reinforc learn mean class machin learn algorithm deepq etc mind general concept learn base reward punish possibl creat strong ai reli learn reinforc reinforc learn requir artifici intellig exist reward punish impli exist favor unfavor world state must intellig general artifici intellig particular way classifi world state favor unfavor,"philosophy,reinforcement-learning","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,philosophy,convolutional-neural-networks,tensorflow,game-ai,q-learning,algorithm,classification,self-learning,training,genetic-algorithms"
439,"<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=""https://en.wikipedia.org/wiki/Not_safe_for_work"" rel=""nofollow"">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p>

<p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p>
",example  search engine companies want classify image searches   categories  already   nsfw  nudity  porn  brutality  safe view pictures   artificial neural networks achieve  success rate  easily mistaken  ,exampl search engin compani want classifi imag search categori alreadi nsfw nuditi porn brutal safe view pictur artifici neural network achiev success rate easili mistaken,"image-recognition,classification,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,classification,reinforcement-learning,ai-design,tensorflow,algorithm,training,game-ai,computer-vision,genetic-algorithms,backpropagation"
440,"<p>Do scientists or research experts know from the kitchen what is happening inside complex ""deep"" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>

<p>For example this <a href=""https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"" rel=""nofollow noreferrer"">study</a> says:</p>

<blockquote>
  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>
</blockquote>

<p>So does this mean that scientists actually don't know how complex convolutional network models work?</p>
",scientists research experts know kitchen happening inside complex  deep  neural network least millions connections firing instant  understand process behind  e g  happening inside works exactly   subject debate   example study says      however clear understanding perform well  might improved    mean scientists actually know complex convolutional network models work  ,scientist research expert know kitchen happen insid complex deep neural network least million connect fire instant understand process behind e g happen insid work exact subject debat exampl studi say howev clear understand perform well might improv mean scientist actual know complex convolut network model work,"neural-networks,deep-learning,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,classification,training,game-ai,image-recognition,backpropagation,algorithm,genetic-algorithms,computer-vision"
441,"<p>Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in <a href=""https://cs.stanford.edu/~acoates/stl10/"" rel=""nofollow"">STL-10 dataset</a>: 96x96 pixels and color)?</p>

<p>Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?</p>
",way estimate big neural network would training session         unlabeled images unsupervised learning  like stl    dataset    x   pixels color    storage space  could vary guess based implementation   specifically many neurons could  could estimate  e g  thousand  millions   depends   figures estimated  ,way estim big neural network would train session unlabel imag unsupervis learn like stl dataset x pixel color storag space could vari guess base implement specif mani neuron could could estim e g thousand million depend figur estim,"deep-network,image-recognition,deep-learning,datasets","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,image-recognition,classification,training,algorithm,game-ai,keras,computer-vision,genetic-algorithms"
442,"<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can ""ask"" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>

<p>What are the current successful approaches to that type of problem?</p>
",example like train neural network recognize type actions  e g  commercial movies real life videos    ask  network video movie  frames  somebody driving car  kissing  eating  scared talking phone   current successful approaches type problem  ,exampl like train neural network recogn type action e g commerci movi real life video ask network video movi frame somebodi drive car kiss eat scare talk phone current success approach type problem,"image-recognition,training,action-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,training,image-recognition,classification,algorithm,game-ai,computer-vision,genetic-algorithms,recurrent-neural-networks"
445,"<p>I'm playing with an LSTM to generate text. In particular, this one:</p>

<p><a href=""https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py"" rel=""nofollow"">https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py</a></p>

<p>It works on quite a big demo text set from Nietzsche and says</p>

<blockquote>
  <p>If you try this script on new data, make sure your corpus
  has at least ~100k characters. ~1M is better.</p>
</blockquote>

<p>This pops up a couple of questions.</p>

<p>A.) If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern.</p>

<p>E.g.</p>

<p><em>I like blue sky with white clouds</em></p>

<p><em>I like yellow fields with some trees</em></p>

<p><em>I like big cities with lots of bars</em></p>

<p>...</p>

<p>Would it then be reasonable to use a much much smaller dataset?</p>

<p>B.) If the dataset really needs to be that big. What if I just repeat the text over and over to reach the recommended minimum? If that would work though, I'd be wondering how that is any different from just taking more iterations of learning with the same shorter text?</p>

<p>Obviously I can play with these two questions myself and in fact I am experimenting with it. One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low ( ~0.04) quite fast but the predicted text just turns out as gibberish.</p>

<p>My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not? But then again I wonder if more iterations or duplicating the content would actually help.</p>

<p>I'm trying to experiment with these questions myself so please don't think I'm just too lazy and are aiming for others to do the work. I'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things.</p>
",playing lstm generate text  particular  one   https   raw githubusercontent com fchollet keras master examples lstm text generation py  works quite big demo text set nietzsche says     try script new data  make sure corpus   least     k characters    better    pops couple questions     want ai limited vocabulary generate text short sentences following basic pattern   e g   like blue sky white clouds  like yellow fields trees  like big cities lots bars       would reasonable use much much smaller dataset   b   dataset really needs big  repeat text reach recommended minimum  would work though  wondering different taking iterations learning shorter text   obviously play two questions fact experimenting  one thing already figured shorter text following basic pattern get low          quite fast predicted text turns gibberish   naive explanation would enough samples proof whether gibberish actually makes sense  wonder iterations duplicating content would actually help   trying experiment questions please think lazy aiming others work  looking experienced people give better understanding mechanics influence things  ,play lstm generat text particular one https raw githubusercont com fchollet kera master exampl lstm text generat py work quit big demo text set nietzsch say tri script new data make sure corpus least k charact better pop coupl question want ai limit vocabulari generat text short sentenc follow basic pattern e g like blue sky white cloud like yellow field tree like big citi lot bar would reason use much much smaller dataset b dataset realli need big repeat text reach recommend minimum would work though wonder differ take iter learn shorter text obvious play two question fact experi one thing alreadi figur shorter text follow basic pattern get low quit fast predict text turn gibberish naiv explan would enough sampl proof whether gibberish actual make sens wonder iter duplic content would actual help tri experi question pleas think lazi aim work look experienc peopl give better understand mechan influenc thing,"datasets,lstm","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,image-recognition,training,classification,tensorflow,natural-language,genetic-algorithms,nlp"
446,"<p>For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.</p>

<p>Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?</p>
",example would like implement transparent ai rts game offer ai api  like old games   like use image recognition algorithm detecting objects talks another algorithm responsible logic   given like use two neural networks  approaches setup communication  exporting result findings first algorithm  e g  using cnn  list features found screen  use input another network  complex  need two networks  ,exampl would like implement transpar ai rts game offer ai api like old game like use imag recognit algorithm detect object talk anoth algorithm respons logic given like use two neural network approach setup communic export result find first algorithm e g use cnn list featur found screen use input anoth network complex need two network,"neural-networks,convolutional-neural-networks,gaming","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,game-ai,image-recognition,algorithm,tensorflow,training,classification,genetic-algorithms,computer-vision,backpropagation"
448,"<p>Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.</p>
",successful attempts replace poor guide dogs used blind people ai achieve similar rate success  guess dogs could easily distracted reliable every situation  probably takes less time train ai  dog  ,success attempt replac poor guid dog use blind peopl ai achiev similar rate success guess dog could easili distract reliabl everi situat probabl take less time train ai dog,"neural-networks,applications","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,tensorflow,training,philosophy,algorithm,image-recognition,classification,strong-ai,computer-vision"
449,"<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>

<p>References: <a href=""http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121"" rel=""nofollow"">Sky News article</a>, <a href=""http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s"" rel=""nofollow"">The Verge</a>.</p>
",know tesla autopilot mistaken empty sky high sided lorry resulted fatal crash involving car self drive mode  ai fault something else  technical explanation behind happened   references  sky news article  verge  ,know tesla autopilot mistaken empti sky high side lorri result fatal crash involv car self drive mode ai fault someth els technic explan behind happen refer sky news articl verg,"self-driving,cars","neural-networks,machine-learning,ai-design,deep-learning,philosophy,algorithm,training,reinforcement-learning,game-ai,convolutional-neural-networks,strong-ai,image-recognition,research,game-theory,backpropagation"
451,"<p>For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?</p>
",benefits testing agi  using high level video game description language  vgdl  gives reliable accurate results general intelligence using arcade learning environment  ale   ,benefit test agi use high level video game descript languag vgdl give reliabl accur result general intellig use arcad learn environ ale,"agi,gaming","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,game-ai,ai-design,tensorflow,training,classification,algorithm,image-recognition,philosophy,natural-language,q-learning"
452,"<p>Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.</p>

<p>How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?</p>
",time ago playing chess challenging algorithms  go game vastly complex compared chess   playing rts game enormous branching factors limited time space  like deciding next   successful approaches problems  ,time ago play chess challeng algorithm go game vast complex compar chess play rts game enorm branch factor limit time space like decid next success approach problem,"gaming,branching-factors,real-time","neural-networks,machine-learning,deep-learning,reinforcement-learning,game-ai,ai-design,algorithm,convolutional-neural-networks,tensorflow,genetic-algorithms,combinatorial-games,q-learning,training,game-theory,chess"
453,"<p>We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One <a href=""http://www.bbc.co.uk/news/technology-36558829"" rel=""nofollow"">article</a> cite says:</p>

<blockquote>
  <p>AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.</p>
</blockquote>

<p>Have researchers already figured it out what went wrong?</p>
",read wiki page march      alphago ai lost game       lee sedol  professional go player  one article cite says      alphago lost game researchers want explore find went wrong  need figure weaknesses try improve    researchers already figured went wrong  ,read wiki page march alphago ai lost game lee sedol profession go player one articl cite say alphago lost game research want explor find went wrong need figur weak tri improv research alreadi figur went wrong,"gaming,deepmind","neural-networks,machine-learning,deep-learning,game-ai,reinforcement-learning,ai-design,convolutional-neural-networks,research,tensorflow,algorithm,philosophy,combinatorial-games,game-theory,genetic-algorithms,image-recognition"
454,"<p>Assuming we're dealing with artificial neural network (e.g. using <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convnets</a>) which was trained by large dataset of human faces.</p>

<p>Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?</p>
",assuming dealing artificial neural network  e g  using convnets  trained large dataset human faces   known issues challenges facial recognition would fail  talking covering half face  simple common things wearing glasses  hat  jewellery  face painting tattoo  successfully prevent ai recognizing face   current methods dealing challenges  ,assum deal artifici neural network e g use convnet train larg dataset human face known issu challeng facial recognit would fail talk cover half face simpl common thing wear glass hat jewelleri face paint tattoo success prevent ai recogn face current method deal challeng,"convolutional-neural-networks,facial-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,game-ai,training,image-recognition,tensorflow,classification,algorithm,philosophy,genetic-algorithms,computer-vision"
455,"<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p>

<p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p>

<p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p>
",would like know kind dataset need  prepare  training network recognize spelling mistakes individual words english text   given large database words  correct one incorrect  kind input efficient tasks  using one input per letter  syllable  whole word use different pattern syllable   input incorrect word  output correct  word need correction  input output  right approach  ,would like know kind dataset need prepar train network recogn spell mistak individu word english text given larg databas word correct one incorrect kind input effici task use one input per letter syllabl whole word use differ pattern syllabl input incorrect word output correct word need correct input output right approach,"deep-learning,datasets,language-processing","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,training,classification,algorithm,tensorflow,natural-language,backpropagation,game-ai,image-recognition,keras"
462,"<p>As I have been looking at other questions on this site (like <a href=""https://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development"">this</a>, <a href=""https://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots"">this</a>, <a href=""https://ai.stackexchange.com/questions/111/how-would-self-driving-cars-make-ethical-decisions-about-who-to-kill"">this</a>, and <a href=""https://ai.stackexchange.com/questions/1289/can-we-destroy-artificial-general-intelligence-without-its-consent"">this</a>), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we <em>can</em> create it is not rationale enough as to whether or not we <em>should</em> do it.</p>

<p>In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its <em>""will""</em>?</p>

<p>What does the creation of AI imply ethically for us as well as the AI?</p>
",looking questions site  like      thinking ethical implications creating generalized ai systems  seems whether create rationale enough whether   dealing issue ethics ai  wonder ethical implications us  system  seems extend beyond usually asked questions topic unknown territory  ethics computable  implemented programmatically  force ai system something     creation ai imply ethically us well ai  ,look question site like think ethic implic creat general ai system seem whether creat rational enough whether deal issu ethic ai wonder ethic implic us system seem extend beyond usual ask question topic unknown territori ethic comput implement programmat forc ai system someth creation ai impli ethic us well ai,ethics,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,algorithm,strong-ai,image-recognition,convolutional-neural-networks,research,training,genetic-algorithms,nlp"
467,"<p>I believe <em>artificial intelligence</em> (AI) term is overused nowadays.</p>

<p>For example people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p>

<p>What are the minimum general requirements so that we can say something is AI?</p>
",believe artificial intelligence  ai  term overused nowadays   example people see something self moving call ai  even autopilot  like cars planes  simple algorithm behind   minimum general requirements say something ai  ,believ artifici intellig ai term overus nowaday exampl peopl see someth self move call ai even autopilot like car plane simpl algorithm behind minimum general requir say someth ai,definitions,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,algorithm,reinforcement-learning,strong-ai,training,convolutional-neural-networks,image-recognition,genetic-algorithms,research,tensorflow"
468,"<p>I believe normally you can use <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p>

<p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p>
",believe normally use genetic programming sorting  however like check whether possible using ann   given unsorted text data input  neural network suitable sorting tasks  ,believ normal use genet program sort howev like check whether possibl use ann given unsort text data input neural network suitabl sort task,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,tensorflow,classification,game-ai,algorithm,image-recognition,backpropagation,genetic-algorithms,keras"
469,"<p>I've read on wiki that <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> has '<em>outstanding results</em>' in cyberterrorism prevention.</p>

<p>Further more, this <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=877981"" rel=""nofollow"">abstract</a> says:</p>

<blockquote>
  <p>Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.</p>
</blockquote>

<p>I've checked the study, but it's still not clear for me.</p>

<p>How exactly was this detection achieved from the technical perspective?</p>
",read wiki genetic programming  outstanding results  cyberterrorism prevention    abstract says      using machine coded linear genomes homologous crossover operator genetic programming  promising results achieved detecting malicious intrusions    checked study  still clear   exactly detection achieved technical perspective  ,read wiki genet program outstand result cyberterror prevent abstract say use machin code linear genom homolog crossov oper genet program promis result achiev detect malici intrus check studi still clear exact detect achiev technic perspect,"genetic-programming,cyberterrorism,security","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,algorithm,image-recognition,training,genetic-algorithms,classification,game-ai,philosophy,tensorflow,natural-language"
475,"<p>On Wikipedia, we can read about different type of <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow noreferrer"">intelligent agents</a>:</p>

<ul>
<li>abstract intelligent agents (AIA),</li>
<li>autonomous intelligent agents,</li>
<li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=""https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva"" rel=""nofollow noreferrer"">this one</a>.</li>
</ul>

<p>What are the differences between these three to avoid confusion?</p>

<hr>

<p>For example I've used term <em>virtual artificial agent</em> <a href=""https://ai.stackexchange.com/a/1512/8"">here</a> as:</p>

<blockquote>
  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p>
</blockquote>

<p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p>
",wikipedia  read different type intelligent agents    abstract intelligent agents  aia   autonomous intelligent agents  virtual intelligent agent  iva   found websites  e g  one    differences three avoid confusion     example used term virtual artificial agent      basically robot mechanical virtual artificial agent exhibit intelligent behavior  ai     basically like know terms like autonomous abstract agents used context  defined  virtual  robot definition  distinguish terms  ,wikipedia read differ type intellig agent abstract intellig agent aia autonom intellig agent virtual intellig agent iva found websit e g one differ three avoid confus exampl use term virtual artifici agent basic robot mechan virtual artifici agent exhibit intellig behavior ai basic like know term like autonom abstract agent use context defin virtual robot definit distinguish term,"definitions,intelligent-agent,comparison,terminology","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,philosophy,convolutional-neural-networks,game-ai,multi-agent-systems,algorithm,image-recognition,game-theory,intelligent-agent,training,classification"
477,"<p>On <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow"">Wikipedia</a> we can read:</p>

<blockquote>
  <p>Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.</p>
</blockquote>

<p>What was the accusation and how was Deep Blue allegedly able to cheat?</p>
",wikipedia read      kasparov accused ibm cheating demanded rematch  ibm refused retired deep blue    accusation deep blue allegedly able cheat  ,wikipedia read kasparov accus ibm cheat demand rematch ibm refus retir deep blue accus deep blue alleg abl cheat,"chess,deep-blue,challenges,game-theory","deep-learning,neural-networks,machine-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,deep-network,classification,image-recognition,knowledge-representation,chess,genetic-algorithms,getting-started,natural-language"
478,"<p>The Wikipedia page describes <a href=""https://en.wikipedia.org/wiki/AI_control_problem"" rel=""nofollow"">AI control problem</a> in very intricated way.</p>

<p>Therefore I would like to better understand it based on some simple explanation, what's going on.
Basically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.</p>

<p>I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.
I believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.</p>

<p>So what is exactly the AI Control Problem?</p>
",wikipedia page describes ai control problem intricated way   therefore would like better understand based simple explanation  going  basically want copy  amp  pastes wiki  articles written neutral point view  general way articles evolving slowly  definition suit   believe discussed nowadays government important aspects ai technology leds  believe could big problem near future  expecting hear people much better date point view   exactly ai control problem  ,wikipedia page describ ai control problem intric way therefor would like better understand base simpl explan go basic want copi amp past wiki articl written neutral point view general way articl evolv slowli definit suit believ discuss nowaday govern import aspect ai technolog led believ could big problem near futur expect hear peopl much better date point view exact ai control problem,"definitions,control-problem","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,philosophy,image-recognition,classification,genetic-algorithms,training,tensorflow,strong-ai"
483,"<p><sub>This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. </sub></p>

<hr>

<p>According to <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""noreferrer"">Wikipedia</a>,</p>

<blockquote>
  <p>Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.</p>
</blockquote>

<p>Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.</p>

<p>Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).</p>

<p>As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you'll always get the ""closest"" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?</p>
",closed beta ai  question posted user number     credit      according wikipedia      boltzmann machines seen stochastic  generative counterpart hopfield nets    recurrent neural networks trained learn bit patterns  presented partial pattern  net retrieve full complete pattern   hopfield networks proven capacity        e g  approximately     bit vectors recalled storage every      nodes  hertz         boltzmann machine stochastic  understanding would necessarily always show pattern energy difference one stored pattern another similar  stochasticity  maybe allows denser pattern storage without guarantee always get  closest  pattern terms energy difference  would true  would hopfield net able store patterns  ,close beta ai question post user number credit accord wikipedia boltzmann machin seen stochast generat counterpart hopfield net recurr neural network train learn bit pattern present partial pattern net retriev full complet pattern hopfield network proven capac e g approxim bit vector recal storag everi node hertz boltzmann machin stochast understand would necessarili alway show pattern energi differ one store pattern anoth similar stochast mayb allow denser pattern storag without guarante alway get closest pattern term energi differ would true would hopfield net abl store pattern,"neural-networks,comparison,recurrent-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,image-recognition,classification,algorithm,game-ai,tensorflow,genetic-algorithms,backpropagation,philosophy"
489,"<p>According to <a href=""http://en.wikipedia.org/wiki/Prolog"">Wikipedia</a>,</p>

<blockquote>
  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>
</blockquote>

<p>Is it still used for AI?</p>

<hr>

<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>
",according wikipedia      prolog general purpose logic programming language associated artificial intelligence computational linguistics    still used ai     based question      closed beta  author uid      ,accord wikipedia prolog general purpos logic program languag associ artifici intellig comput linguist still use ai base question close beta author uid,"history,programming-languages,prolog","machine-learning,neural-networks,deep-learning,ai-design,philosophy,game-ai,reinforcement-learning,convolutional-neural-networks,algorithm,natural-language,nlp,strong-ai,image-recognition,training,genetic-algorithms"
492,"<p>I'm a bit confused with extensive number of different <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_method"" rel=""nofollow"">Monte Carlo methods</a> such as:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo"" rel=""nofollow"">Hamiltonian/Hybrid Monte Carlo (HMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"" rel=""nofollow"">Markov chain Monte Carlo (MCMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Kinetic_Monte_Carlo"" rel=""nofollow"">Kinetic Monte Carlo (KMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method"" rel=""nofollow"">Quasi-Monte Carlo (QMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo"" rel=""nofollow"">Direct Simulation Monte Carlo (DSMC)</a>,</li>
<li>and so on.</li>
</ul>

<p>I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?</p>
",bit confused extensive number different monte carlo methods    hamiltonian hybrid monte carlo  hmc   dynamic monte carlo  dmc   markov chain monte carlo  mcmc   kinetic monte carlo  kmc   dynamic monte carlo  dmc  quasi monte carlo  qmc   direct simulation monte carlo  dsmc      ask exact differences  called monte carlo  common  used ai  e g  one used gaming  like go  image recognition  resampling   ,bit confus extens number differ mont carlo method hamiltonian hybrid mont carlo hmc dynam mont carlo dmc markov chain mont carlo mcmc kinet mont carlo kmc dynam mont carlo dmc quasi mont carlo qmc direct simul mont carlo dsmc ask exact differ call mont carlo common use ai e g one use game like go imag recognit resampl,"gaming,comparison,monte-carlo-search","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,game-ai,image-recognition,tensorflow,classification,training,algorithm,computer-vision,genetic-algorithms,game-theory"
493,"<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p>

<p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p>

<p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p>

<p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p>

<p>In layman's terms, what are the underlying principles that make a neural network work?</p>
",comes neural networks  often explained abstract task  say example detect number image  never understood going hood essentially   seems common structure directed graph  values node  nodes input nodes  values set  values subsequent nodes calculated based along edges graph values output nodes set  interpreted result   exactly value node determined  assume formula associated node takes incoming nodes input calculate value node  formula used  formula throughout network   heard network trained  assume training would process assign values coefficients formulas used determine node values  correct   layman terms  underlying principles make neural network work  ,come neural network often explain abstract task say exampl detect number imag never understood go hood essenti seem common structur direct graph valu node node input node valu set valu subsequ node calcul base along edg graph valu output node set interpret result exact valu node determin assum formula associ node take incom node input calcul valu node formula use formula throughout network heard network train assum train would process assign valu coeffici formula use determin node valu correct layman term principl make neural network work,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,tensorflow,ai-design,backpropagation,game-ai,classification,training,algorithm,image-recognition,genetic-algorithms,q-learning"
495,"<p>Ideally I'd like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.</p>

<p>For example I'm bored with some movie which I've watched thousands of time and I'd like to add some ""dreaming"" to it which is real-time filter which takes input frames, then it's processing and enhances the images through artificial neural network to achieve doodled output.</p>

<p>Doesn't have to be exactly <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I'm more interested into achieving desired real-time use.</p>

<p>What kind of techniques can achieve such efficiency?</p>
",ideally like watch movie deep dreamed real time  algorithms know slow designed real time processing   example bored movie watched thousands time like add  dreaming  real time filter takes input frames  processing enhances images artificial neural network achieve doodled output   exactly deepdream hallucinogenic technique  could much watch  h   similar ann algorithm  interested achieving desired real time use   kind techniques achieve efficiency  ,ideal like watch movi deep dream real time algorithm know slow design real time process exampl bore movi watch thousand time like add dream real time filter take input frame process enhanc imag artifici neural network achiev doodl output exact deepdream hallucinogen techniqu could much watch h similar ann algorithm interest achiev desir real time use kind techniqu achiev effici,"research,algorithm,real-time,neural-doodle","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,algorithm,image-recognition,classification,training,tensorflow,game-ai,backpropagation,computer-vision,genetic-algorithms"
496,"<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>
",employing evolutionary algorithms design train artificial neural networks advantages using conventional backpropagation algorithms  ,employ evolutionari algorithm design train artifici neural network advantag use convent backpropag algorithm,"neural-networks,comparison,backpropagation,evolutionary-algorithms","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,training,tensorflow,ai-design,algorithm,classification,image-recognition,backpropagation,game-ai,genetic-algorithms,recurrent-neural-networks"
497,"<p>Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?</p>

<p>I don't have any specific programming language in mind, but something similar to <a href=""http://pear.php.net/package/PHP_CodeSniffer"" rel=""nofollow"">PHP_CodeSniffer</a> (following <a href=""https://www.drupal.org/coding-standards"" rel=""nofollow"">these standards</a>), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?</p>
",existing approaches using artificial neural networks  ann  evolutionary algorithm  ea  detecting coding standard violations  one would suitable   specific programming language mind  something similar php codesniffer  following standards   instead using hardcoded rules  algorithm learn good techniques  sure based training data  would approach training session  suggestions  ,exist approach use artifici neural network ann evolutionari algorithm ea detect code standard violat one would suitabl specif program languag mind someth similar php codesniff follow standard instead use hardcod rule algorithm learn good techniqu sure base train data would approach train session suggest,"neural-networks,training,computer-programming","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,classification,tensorflow,algorithm,image-recognition,game-ai,genetic-algorithms,computer-vision,nlp"
498,"<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>

<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>

<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>

<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>
",genetic algorithms come attention recently trying correct improve computer opponents turn based strategy computer games   implemented simple genetic algorithm use cross  random mutation  seemed work case  started thinking   cross part genetic algorithms  mutation enough   data dump old ai site  asker uid     ,genet algorithm come attent recent tri correct improv comput oppon turn base strategi comput game implement simpl genet algorithm use cross random mutat seem work case start think cross part genet algorithm mutat enough data dump old ai site asker uid,genetic-algorithms,"machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,game-ai,convolutional-neural-networks,genetic-algorithms,tensorflow,training,image-recognition,classification,philosophy,computer-vision"
501,"<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=""http://psychologia.co/creativity-test/"" rel=""nofollow"">website</a> (for testing creativity):</p>

<blockquote>
  <p>Curiosity refers to persistent desire to learn and discover new things
  and ideas</p>

<pre><code>always looks for new and original ways of thinking,
likes to learn,
searches for alternative solutions even when traditional solutions are present and available,
enjoys reading books and watching documentaries,
wants to know how things work inside out
</code></pre>
</blockquote>

<p>Let's take <a href=""https://www.clarifai.com/demo"" rel=""nofollow"">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a ""curiosity factor"" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>

<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>
",thinking ai  question came mind  could curiosity help developing true ai  according website  testing creativity       curiosity refers persistent desire learn discover new things   ideas  always looks new original ways thinking  likes learn  searches alternative solutions even traditional solutions present available  enjoys reading books watching documentaries  wants know things work inside    let take clarifai  image video classification startup classify images video best accuracy  according   understand correctly  trained deep learning system using millions images supervised learning  algorithm  would happen somehow added  curiosity factor  ai difficulty classifying image objects  would ask human help  like curious child    curiosity makes human learn new things also helps generate new original ideas  could addition curiosity change clarifai true ai  ,think ai question came mind could curios help develop true ai accord websit test creativ curios refer persist desir learn discov new thing idea alway look new origin way think like learn search altern solut even tradit solut present avail enjoy read book watch documentari want know thing work insid let take clarifai imag video classif startup classifi imag video best accuraci accord understand correct train deep learn system use million imag supervis learn algorithm would happen somehow ad curios factor ai difficulti classifi imag object would ask human help like curious child curios make human learn new thing also help generat new origin idea could addit curios chang clarifai true ai,human-inspired,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,image-recognition,classification,algorithm,game-ai,tensorflow,training,philosophy,computer-vision,genetic-algorithms"
517,"<p>Based on this <a href=""http://www.dailymail.co.uk/sciencetech/article-3677950/Google-s-self-driving-cars-spot-cyclists-Sensors-read-hand-signals-predict-riders-behavior.html"" rel=""nofollow"">article</a>, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.</p>

<p>How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?</p>
",based article  google self driving cars spot cyclists  cars  road signs  markings  traffic lights  pedestrians   exactly identify pedestrians  based face recognition  shape  size  distance  infrared signature  ,base articl googl self drive car spot cyclist car road sign mark traffic light pedestrian exact identifi pedestrian base face recognit shape size distanc infrar signatur,"self-driving,cars,object-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,ai-design,training,tensorflow,algorithm,reinforcement-learning,keras,classification,self-driving,computer-vision,game-ai"
518,"<p>In <a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"">Hidden Obstacles for Google?s Self-Driving Cars</a> article we can read that:</p>

<blockquote>
  <p>Google?s cars can detect and respond to stop signs that aren?t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p>
  
  <p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p>
</blockquote>

<p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p>
",hidden obstacles google self driving cars article read      google cars detect respond stop signs map  feature introduced deal temporary signs used construction sites       google says cars identify almost unmapped stop signs  would remain safe miss sign vehicles always looking traffic  pedestrians obstacles    would happen car spotted somebody front  collision path  wearing shirt stop sign printed  would react stop car  ,hidden obstacl googl self drive car articl read googl car detect respond stop sign map featur introduc deal temporari sign use construct site googl say car identifi almost unmap stop sign would remain safe miss sign vehicl alway look traffic pedestrian obstacl would happen car spot somebodi front collis path wear shirt stop sign print would react stop car,"self-driving,decision-theory,cars,object-recognition","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,algorithm,image-recognition,training,tensorflow,genetic-algorithms,classification,game-ai,natural-language,self-driving"
524,"<p><sub> This is a scope experiment. </sub></p>

<hr>

<p>After Google/Tesla/whoever else is making self-driving cars finishes perfecting them, will they replace the cars with human drivers, so that there are only self-driving cars?</p>

<p>If they do, it would probably make the roads safer.</p>
", scope experiment      google tesla whoever else making self driving cars finishes perfecting  replace cars human drivers  self driving cars    would probably make roads safer  ,scope experi googl tesla whoever els make self drive car finish perfect replac car human driver self drive car would probabl make road safer,"self-driving,cars","neural-networks,machine-learning,deep-learning,ai-design,algorithm,training,reinforcement-learning,game-ai,image-recognition,philosophy,convolutional-neural-networks,self-driving,genetic-algorithms,game-theory,classification"
525,"<p>Significant AI vs human board game matches include:</p>

<ul>
<li><strong>chess</strong>: <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov"" rel=""nofollow"">Deep Blue vs Kasparov</a> in 1996,</li>
<li><strong>Go</strong>: <a href=""https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"" rel=""nofollow"">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>
</ul>

<p>which demonstrated that AI challenged and defeated professional players.</p>

<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>
",significant ai vs human board game matches include    chess  deep blue vs kasparov       go  deepmind alphago vs lee sedol         demonstrated ai challenged defeated professional players   known board games left human still win ai  mean based final outcome authoritative famous matches  still board game ai cannot beat world champion game  ,signific ai vs human board game match includ chess deep blue vs kasparov go deepmind alphago vs lee sedol demonstr ai challeng defeat profession player known board game left human still win ai mean base final outcom authorit famous match still board game ai beat world champion game,"history,challenges,game-theory","machine-learning,neural-networks,game-ai,ai-design,deep-learning,reinforcement-learning,philosophy,combinatorial-games,game-theory,tensorflow,strong-ai,gaming,convolutional-neural-networks,chess,q-learning"
527,"<p>I'm trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not.</p>

<p>Unfortunately it's not learning to recognize them correctly and I think may way of representing/encoding the game into vectors is wrong.</p>

<p>I choose a way that is easy for an human (me, in particular!) to make sense of:</p>

<pre><code>training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")
target_data = np.array([[0],[0],[1],[1]], ""float32"")
</code></pre>

<p>This basically just use an array of length 9 to represent a 3 x 3 board. The first three items represent the first row, the next three the second row and so on. The line breaks should make it obvious I guess.</p>

<p>The target data then maps the first two game states to ""no wins"" and the last two game states to ""wins"".</p>

<p>Then I wanted to create some validation data that is slightly different to see if it generalizes.</p>

<pre><code>validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")
</code></pre>

<p>Obviously, again the last two game states should be ""wins"" whereas the first two should not.</p>

<p>I tried to play with the number of neurons and learning rate but no matter what I try, my output looks pretty of. E.g.</p>

<pre><code>[[ 0.01207292]
 [ 0.98913926]
 [ 0.00925775]
 [ 0.00577191]]
</code></pre>

<p>I tend to think it's the way how I represent the game state that may be wrong but actually I have no idea :D</p>

<p>Can anyone help me out here?</p>

<p>This is the entire code that I use</p>

<pre><code>import numpy as np
from keras.models import Sequential
from keras.layers.core import Activation, Dense
from keras.optimizers import SGD

training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")

target_data = np.array([[0],[0],[1],[1]], ""float32"")

validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")

model = Sequential()
model.add(Dense(2, input_dim=9, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)

history = model.fit(training_data, target_data, nb_epoch=10000, batch_size=4, verbose=0)

print(model.predict(validation_data))
</code></pre>
",trying teach ai different pattern tic tac toe recognize wether given pattern represents win   unfortunately learning recognize correctly think may way representing encoding game vectors wrong   choose way easy human   particular   make sense   training data   np array                                                                                                                                                                                                                                                                                                                                                                                                      float     target data   np array                     float       basically use array length   represent   x   board  first three items represent first row  next three second row  line breaks make obvious guess   target data maps first two game states  wins  last two game states  wins    wanted create validation data slightly different see generalizes   validation data   np array                                                                                                                                                                                                                                                                                                                                                                                                                            float       obviously  last two game states  wins  whereas first two   tried play number neurons learning rate matter try  output looks pretty  e g                                                                  tend think way represent game state may wrong actually idea   anyone help   entire code use  import numpy np keras models import sequential keras layers core import activation  dense keras optimizers import sgd  training data   np array                                                                                                                                                                                                                                                                                                                                                                                                      float      target data   np array                     float      validation data   np array                                                                                                                                                                                                                                                                                                                                                                                                                            float      model   sequential   model add dense    input dim    activation  sigmoid    model add dense    activation  sigmoid     sgd   sgd lr      decay  e    momentum      nesterov true  model compile loss  mean squared error   optimizer sgd   history   model fit training data  target data  nb epoch        batch size    verbose     print model predict validation data    ,tri teach ai differ pattern tic tac toe recogn wether given pattern repres win unfortun learn recogn correct think may way repres encod game vector wrong choos way easi human particular make sens train data np array float target data np array float basic use array length repres x board first three item repres first row next three second row line break make obvious guess target data map first two game state win last two game state win want creat valid data slight differ see general valid data np array float obvious last two game state win wherea first two tri play number neuron learn rate matter tri output look pretti e g tend think way repres game state may wrong actual idea anyon help entir code use import numpi np kera model import sequenti kera layer core import activ dens kera optim import sgd train data np array float target data np array float valid data np array float model sequenti model add dens input dim activ sigmoid model add dens activ sigmoid sgd sgd lr decay e momentum nesterov true model compil loss mean squar error optim sgd histori model fit train data target data nb epoch batch size verbos print model predict valid data,"classification,keras","neural-networks,machine-learning,deep-learning,tensorflow,reinforcement-learning,convolutional-neural-networks,keras,ai-design,training,classification,game-ai,q-learning,image-recognition,python,algorithm"
537,"<p>Has there any research been done on how difficult certain languages are to learn for chatbots? 
For example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p>

<p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p>
",research done difficult certain languages learn chatbots   example  cleverbot knows bit dutch  german  finnish french  clearly chatbots speak languages english   english still best language  speaks often   would imagine logical constructed language  like lobjan  would easier learn natural language  like english  example    ,research done difficult certain languag learn chatbot exampl cleverbot know bit dutch german finnish french clear chatbot speak languag english english still best languag speak often would imagin logic construct languag like lobjan would easier learn natur languag like english exampl,"chat-bots,language-processing","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,natural-language,convolutional-neural-networks,algorithm,classification,nlp,image-recognition,game-ai,training,genetic-algorithms,tensorflow"
549,"<p>Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?</p>
",google  tesla  apple etc built building self driving cars  expert related area  interested knowing high level  systems techniques go self driving cars  easy make tabletop prototype  large enough accomodate needed computing power needs   ,googl tesla appl etc built build self drive car expert relat area interest know high level system techniqu go self drive car easi make tabletop prototyp larg enough accomod need comput power need,"self-driving,ai-design","neural-networks,machine-learning,deep-learning,ai-design,algorithm,training,image-recognition,convolutional-neural-networks,reinforcement-learning,philosophy,game-ai,self-driving,backpropagation,genetic-algorithms,computer-vision"
550,"<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>

<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=""http://memory-alpha.org/Borg"" rel=""nofollow"">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>

<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>
",question perhaps broad forum  hence phrasing request references   humans endowed personalities nature  clear  least  feature bug  explored science fiction various notions borg like entities  belief  narrative reasons  stories usually end humans flawed personalities winning end    experts analyzed  perhaps mathematically  design criteria ai agent weakly enforced goals  eg  maximize reproduction human case  uncertain environment  ended answer notion personality useful  philosophers science fiction writers examined question work  would happy know  ,question perhap broad forum henc phrase request refer human endow person natur clear least featur bug explor scienc fiction various notion borg like entiti belief narrat reason stori usual end human flaw person win end expert analyz perhap mathemat design criteria ai agent weak enforc goal eg maxim reproduct human case uncertain environ end answer notion person use philosoph scienc fiction writer examin question work would happi know,"philosophy,strong-ai,ai-design,human-inspired","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,philosophy,algorithm,image-recognition,game-theory,training,nlp,classification,natural-language"
555,"<p>I've found this short <a href=""http://iamtrask.github.io/2015/07/12/basic-python-network/"" rel=""nofollow"">Python code</a> which implements neural network in 11 lines of code:</p>

<pre><code>X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).T
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1
for j in xrange(60000):
    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))
    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))
    l2_delta = (y - l2)*(l2*(1-l2))
    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))
    syn1 += l1.T.dot(l2_delta)
    syn0 += X.T.dot(l1_delta)
</code></pre>

<p>I believe it may be a valid implementation of neural network, but how do I know?</p>

<p>In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?</p>

<p>In other words, I'd like to ask, what features/properties makes a valid artificial neural network?</p>
",found short python code implements neural network    lines code   x   np array                                        np array              syn      np random random            syn      np random random            j xrange             l         np exp   np dot x syn          l         np exp   np dot l  syn          l  delta      l    l     l        l  delta   l  delta dot syn      l       l        syn     l  dot l  delta      syn     x dot l  delta    believe may valid implementation neural network  know   words  creating bunch arrays compute output certain criteria call layers synapses make proper neural network   words  like ask  features properties makes valid artificial neural network  ,found short python code implement neural network line code x np array np array syn np random random syn np random random j xrang l np exp np dot x syn l np exp np dot l syn l delta l l l l delta l delta dot syn l l syn l dot l delta syn x dot l delta believ may valid implement neural network know word creat bunch array comput output certain criteria call layer synaps make proper neural network word like ask featur properti make valid artifici neural network,"neural-networks,implementation,computer-programming","neural-networks,machine-learning,deep-learning,tensorflow,convolutional-neural-networks,reinforcement-learning,backpropagation,keras,ai-design,classification,algorithm,training,q-learning,game-ai,recurrent-neural-networks"
557,"<p>I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.</p>

<p>So it can be used by <a href=""https://en.wikipedia.org/wiki/Internet_service_provider"" rel=""nofollow"">ISP</a> to detect suspicious users connected to their network.</p>
",looking research discusses misbehavior detection public internet access networks using ann approaches   used isp detect suspicious users connected network  ,look research discuss misbehavior detect public internet access network use ann approach use isp detect suspici user connect network,"neural-networks,research","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,image-recognition,tensorflow,game-ai,classification,backpropagation,algorithm,genetic-algorithms,deep-network"
558,"<p>I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like <a href=""https://en.wikipedia.org/wiki/Forcepoint"" rel=""nofollow"">Forcepoint</a>). More specifically detecting traffic patterns. I'm new to this.</p>

<p>Which learning algorithms are most suitable for this goal? <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">EA</a>, <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">GA</a>, <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow"">ANN</a> (which one) or something else?</p>
",investigating applications ai algorithms used data leakage detection prevention within intranet network  like forcepoint   specifically detecting traffic patterns  new   learning algorithms suitable goal  ea  ga  ann  one  something else  ,investig applic ai algorithm use data leakag detect prevent within intranet network like forcepoint specif detect traffic pattern new learn algorithm suitabl goal ea ga ann one someth els,learning-algorithms,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,training,tensorflow,classification,image-recognition,genetic-algorithms,backpropagation,nlp"
561,"<p>I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?</p>

<p>So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).</p>

<p>The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.</p>

<p>Which approach would you take and what are the most challenging things you can think of?</p>
",wondering  instead implementing new web browsers millions line code difficult manage  would possible use ann ga algorithm teach rendering process  page look like    input would imaging html source code  output rendered page  maybe interactive image like svg  library something  sure    training data dataset websites providing input source code rendered representation using browsers guidance expected output   approach would take challenging things think  ,wonder instead implement new web browser million line code difficult manag would possibl use ann ga algorithm teach render process page look like input would imag html sourc code output render page mayb interact imag like svg librari someth sure train data dataset websit provid input sourc code render represent use browser guidanc expect output approach would take challeng thing think,"neural-networks,implementation,computer-programming","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,image-recognition,classification,training,tensorflow,algorithm,game-ai,keras,backpropagation,genetic-algorithms"
566,"<p>I'm trying to make a conversational chatbot, so theuser inputs are quite wide ranging - beyond just ""turn lights on"". I want to detect the category of the user intents from their inputs and prepare responses.</p>

<p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p>

<p>For example if the user says ""Pasta is my favorite dish to cook"" then detect ""intent preference entity pasta"" - then I can gradually build up responses to different categories of inputs.</p>

<p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p>
",trying make conversational chatbot  user inputs quite wide ranging   beyond  turn lights   want detect category user intents inputs prepare responses   looked ms  luis api ai intents require lot training  people suggest techniques untrained intent detection   example user says  pasta favorite dish cook  detect  intent preference entity pasta    gradually build responses different categories inputs   perhaps crowd sourced intents wit ai  facebook  access could sure end users access models  ,tri make convers chatbot user input quit wide rang beyond turn light want detect categori user intent input prepar respons look ms lui api ai intent requir lot train peopl suggest techniqu untrain intent detect exampl user say pasta favorit dish cook detect intent prefer entiti pasta gradual build respons differ categori input perhap crowd sourc intent wit ai facebook access could sure end user access model,untagged,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,tensorflow,reinforcement-learning,classification,algorithm,training,game-ai,keras,image-recognition,backpropagation,computer-vision"
568,"<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=""https://en.wikipedia.org/wiki/Roomba"" rel=""nofollow"">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>

<p>Does it use some kind of <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow"">A*</a> algorithm?</p>
",domestic autonomous robotic vacuum cleaner    roomba   know working cleaned area  aka virtual map   plan travel areas explored yet   use kind  algorithm  ,domest autonom robot vacuum cleaner roomba know work clean area aka virtual map plan travel area explor yet use kind algorithm,"real-time,path-planning,robotics","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,convolutional-neural-networks,game-ai,training,image-recognition,nlp,tensorflow,classification,genetic-algorithms,backpropagation"
569,"<p>It has been <a href=""http://www.itnonline.com/content/will-fda-be-too-much-intelligent-machines"" rel=""nofollow noreferrer"">suggested</a> that machine learning algorithms (also <a href=""https://ai.stackexchange.com/q/1427/8"">Watson</a>) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.</p>

<p>How does exactly deep learning algorithms exactly can find suspicious patterns in the body?s biochemistry?</p>
",suggested machine learning algorithms  also watson  help finding disease patient images optimize scans  also deep learning algorithms show promise every type digital imaging   exactly deep learning algorithms exactly find suspicious patterns body biochemistry  ,suggest machin learn algorithm also watson help find diseas patient imag optim scan also deep learn algorithm show promis everi type digit imag exact deep learn algorithm exact find suspici pattern bodi biochemistri,"deep-learning,healthcare,learning-algorithms","machine-learning,neural-networks,deep-learning,reinforcement-learning,convolutional-neural-networks,image-recognition,algorithm,ai-design,classification,training,tensorflow,computer-vision,game-ai,genetic-algorithms,q-learning"
572,"<p>The <a href=""https://www.youtube.com/watch?v=AplG6KnOr2Q"" rel=""nofollow"">Mario Lives!</a> video (and its follow-up video, <a href=""https://www.youtube.com/watch?v=ltPj3RlN4Nw&amp;list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&amp;index=5"" rel=""nofollow"">Mario Becomes Social!</a>) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into ""emotional states"" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.</p>

<p>What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing <em>what</em> they should do (either by hardcoding in the rules, or learning the rules through machine learning)?</p>
",mario lives  video  follow video  mario becomes social   showcases ai unit able simulate emotional desicion making within virtual world  enter  emotional states  curiosity  hunger  happiness  fear  seems cool exciting  especially video game ai   confused would useful real world scenarios   would point building autonomous actors would behave based emotional states  instead simply knowing  either hardcoding rules  learning rules machine learning   ,mario live video follow video mario becom social showcas ai unit abl simul emot desicion make within virtual world enter emot state curios hunger happi fear seem cool excit especi video game ai confus would use real world scenario would point build autonom actor would behav base emot state instead simpli know either hardcod rule learn rule machin learn,emotional-intelligence,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,philosophy,tensorflow,genetic-algorithms,image-recognition,training,game-theory,classification"
573,"<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p>

<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p>

<p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p>

<p>What are the practical considerations? 
How should I encode the structure into a genome?</p>

<hr>

<p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p>
",     closed beta  asker uid       deterministic problem space  need find neural network optimal node link structure  want use genetic algorithm simulate many neural networks find best network structure problem domain   know fair amount neural networks  used genetic algorithms task like   practical considerations   encode structure genome      actually   saying   mithrandir   ,close beta asker uid determinist problem space need find neural network optim node link structur want use genet algorithm simul mani neural network find best network structur problem domain know fair amount neural network use genet algorithm task like practic consider encod structur genom actual say mithrandir,"neural-networks,genetic-algorithms","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,algorithm,training,classification,game-ai,genetic-algorithms,image-recognition,tensorflow,backpropagation,recurrent-neural-networks"
575,"<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p>
",studies checked accuracy neural network predictions greyhound racing results  compared human expert  would achieve better payoff  ,studi check accuraci neural network predict greyhound race result compar human expert would achiev better payoff,"neural-networks,research","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,classification,training,game-ai,game-theory,image-recognition,tensorflow,decision-theory,algorithm,genetic-algorithms"
578,"<p>I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.</p>

<p>So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?</p>

<p>From their <a href=""http://www.loebner.net/Prizef/loebner-prize.html"" rel=""nofollow"">website</a> this isn't clear (as some of the links doesn't load).</p>

<hr>

<p>A few highlights from previous years:</p>

<p><a href=""http://loebner.exeter.ac.uk/results/"" rel=""nofollow"">2011 Loebner Prize results</a></p>

<blockquote>
  <p>None of the AI systems fooled the judges, therefore the Turing Test has not been passed.</p>
</blockquote>

<p><a href=""http://www.paulmckevitt.com/loebner2013/scoring/loebner2013leaderboard.txt"" rel=""nofollow"">Loebner 2013 results</a>:</p>

<blockquote>
  <p>No chatbot fooled any of the 4 Judges.</p>
</blockquote>
",read loebner prize ai  pledged grand prize          gold medal first computer whose responses indistinguishable human   wondering whether chatbots fooled judges gold medal yet   website clear  links load      highlights previous years        loebner prize results     none ai systems fooled judges  therefore turing test passed    loebner      results      chatbot fooled   judges   ,read loebner prize ai pledg grand prize gold medal first comput whose respons indistinguish human wonder whether chatbot fool judg gold medal yet websit clear link load highlight previous year loebner prize result none ai system fool judg therefor ture test pass loebner result chatbot fool judg,"history,turing-test,chat-bots","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,convolutional-neural-networks,reinforcement-learning,strong-ai,algorithm,image-recognition,nlp,research,game-theory,tensorflow"
580,"<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p>
",hypothetically  assume access infinite computing power  designs brute force algorithms find ai capable passing traditional tests  e g  turing  chinese room  mist  etc     ,hypothet assum access infinit comput power design brute forc algorithm find ai capabl pass tradit test e g ture chines room mist etc,"turing-test,strong-ai,ai-design","machine-learning,neural-networks,deep-learning,ai-design,algorithm,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,genetic-algorithms,research,strong-ai,image-recognition,computer-vision,training"
582,"<p>I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn.</p>
",aware could complex topic  however interested existing research projects studies people attempting succeeded teaching ai foreign language training teaching english books  reading  analysing understanding  knows foreign language rules  grammar  spelling  etc    way human would learn  language chinese  difficult even humans learn  ,awar could complex topic howev interest exist research project studi peopl attempt succeed teach ai foreign languag train teach english book read analys understand know foreign languag rule grammar spell etc way human would learn languag chines difficult even human learn,"research,machine-learning,self-learning,language-processing","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,natural-language,training,philosophy,algorithm,tensorflow,image-recognition,classification,nlp"
585,"<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>

<p>The three laws are:</p>

<ol>
<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>
<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>
<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>
</ol>

<hr>

<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>
",would possible put asimov three laws robotics ai   three laws    robot   accurately  ai  cannot harm human  inaction allow human harmed  robot must listen instructions given human  long conflict first law  robot must protect existence  conflict first two laws        knowledge  plot point one books  p ,would possibl put asimov three law robot ai three law robot accur ai harm human inact allow human harm robot must listen instruct given human long conflict first law robot must protect exist conflict first two law knowledg plot point one book p,"robotics,asimovs-laws","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,algorithm,image-recognition,convolutional-neural-networks,game-theory,strong-ai,nlp,genetic-algorithms,natural-language"
592,"<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p>

<p>Here is the example of application such interpretation in robot:</p>

<p><a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDram.png"" alt=""Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information""></a></p>

<p><sup>Image source: <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p>

<p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=""https://ai.stackexchange.com/a/1577/8"">Google car</a> does it (by determining certain patterns).</p>

<p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p>
",like investigate possibility achieving similar recognition honda asimo robotp    interpret positioning movement hand  including postures gestures based visual information   example application interpretation robot     image source  asimo featuring intelligence technology   technical information  pdf   basically recognition detect indicated location  posture recognition  respond wave  gesture recognition   also similar like google car  determining certain patterns    known asimo  would closest alternative postures gestures recognition achieve results  ,like investig possibl achiev similar recognit honda asimo robotp interpret posit movement hand includ postur gestur base visual inform exampl applic interpret robot imag sourc asimo featur intellig technolog technic inform pdf basic recognit detect indic locat postur recognit respond wave gestur recognit also similar like googl car determin certain pattern known asimo would closest altern postur gestur recognit achiev result,"image-recognition,robots,detecting-patterns","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,ai-design,image-recognition,reinforcement-learning,algorithm,classification,training,philosophy,game-ai,computer-vision,tensorflow,nlp"
595,"<p>For Example:</p>

<h2>Could you provide reasons why a sundial is <em>not</em> ""intelligent""?</h2>

<p>A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>

<h2>What properties of a self driving car would make it ""intelligent""?</h2>

<p>Where is the line between non intelligent matter and an intelligent system?</p>
",example   could provide reasons sundial  intelligent    sundial senses environment acts rationally  outputs time  also stores  percepts   numbers engineer wrote    properties self driving car would make  intelligent    line non intelligent matter intelligent system  ,exampl could provid reason sundial intellig sundial sens environ act ration output time also store percept number engin wrote properti self drive car would make intellig line non intellig matter intellig system,intelligence-testing,"neural-networks,machine-learning,deep-learning,ai-design,philosophy,reinforcement-learning,algorithm,convolutional-neural-networks,training,image-recognition,classification,game-ai,genetic-algorithms,tensorflow,game-theory"
601,"<p>We can read on <a href=""https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29"" rel=""nofollow"">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>

<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>

<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>
",read wikipedia page google built custom asic chip machine learning tailored tensorflow helps accelerate ai   since asic chips specially customized one particular use without ability change circuit  must fixed algorithm invoked   exactly acceleration ai using asic chips work algorithm cannot changed  part exactly accelerating  ,read wikipedia page googl built custom asic chip machin learn tailor tensorflow help acceler ai sinc asic chip special custom one particular use without abil chang circuit must fix algorithm invok exact acceler ai use asic chip work algorithm chang part exact acceler,"machine-learning,hardware","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,tensorflow,image-recognition,training,philosophy,genetic-algorithms,classification,nlp"
602,"<p>I was reading that the <a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">Valkyrie robot</a> was originally designed to 'carry out search and rescue missions'.</p>

<p>However there were some talks to send it to Mars to assist astronauts.</p>

<p>What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?</p>

<p>Refs:</p>

<ul>
<li><a href=""https://github.com/nasa-jsc-robotics"" rel=""nofollow"">NASA-JSC-Robotics at GitHub</a></li>
<li><a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""nofollow"">github.io page</a></li>
<li><a href=""https://gitlab.com/nasa-jsc-robotics/valkyrie"" rel=""nofollow"">gitlab page</a></li>
</ul>
",reading valkyrie robot originally designed  carry search rescue missions    however talks send mars assist astronauts   kind specific trainings tasks planned   able carry   refs    nasa jsc robotics github github io page gitlab page  ,read valkyri robot origin design carri search rescu mission howev talk send mar assist astronaut kind specif train task plan abl carri ref nasa jsc robot github github io page gitlab page,"robotics,nasa","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,tensorflow,training,classification,image-recognition,algorithm,game-ai,nlp,keras,deep-network"
605,"<p>Do scientists know by what mechanism biological brains/biological neural networks store data?</p>

<p>I was thinking about @kenorbs <a href=""https://ai.stackexchange.com/questions/1656/how-can-nanobot-implants-in-our-brains-connect-to-the-internet"">question</a> about implanting nanobots to build an AGI on top of human wetware. </p>

<p>I only have a vague notion that we store data in our brains by altering synapses? </p>

<p>Links, Criticism and Detailed Explanation welcome.</p>

<p>I also would love a decent description of how a vanilla Artificial Neural Network stores data. </p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>How is data stored in a biological Neural Network?</p></li>
<li><p>How is data stored in an Artificial Neural Network?</p></li>
</ol>
",scientists know mechanism biological brains biological neural networks store data   thinking  kenorbs question implanting nanobots build agi top human wetware    vague notion store data brains altering synapses    links  criticism detailed explanation welcome   also would love decent description vanilla artificial neural network stores data    questions    data stored biological neural network  data stored artificial neural network   ,scientist know mechan biolog brain biolog neural network store data think kenorb question implant nanobot build agi top human wetwar vagu notion store data brain alter synaps link critic detail explan welcom also would love decent descript vanilla artifici neural network store data question data store biolog neural network data store artifici neural network,"neural-networks,neuromorphic-computing","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,training,reinforcement-learning,classification,image-recognition,tensorflow,backpropagation,game-ai,recurrent-neural-networks,computer-vision,deep-network"
608,"<p>My understanding is that <em>Watson</em> is the name of the computer, and <em>DeepQA</em> is the name of the software or technology. They are both correlated.</p>

<p>Are there any computers/technologies other than <em>Watson</em> which <strong>are using <em>DeepQA</em></strong>? Or is <em>Watson</em> the only computer which implements that software/technology?</p>

<p><sup>This question is inspired by this <a href=""https://ai.meta.stackexchange.com/q/1177/8"">meta thread</a>.</sup></p>
",understanding watson name computer  deepqa name software technology  correlated   computers technologies watson using deepqa  watson computer implements software technology   question inspired meta thread  ,understand watson name comput deepqa name softwar technolog correl comput technolog watson use deepqa watson comput implement softwar technolog question inspir meta thread,watson,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,tensorflow,convolutional-neural-networks,algorithm,image-recognition,game-ai,training,philosophy,classification,computer-vision,backpropagation"
621,"<p>There is a study about <a href=""http://www.aclweb.org/anthology/P/P02/P02-1031.pdf"" rel=""noreferrer"">The Necessity of Parsing for Predicate Argument Recognition</a>, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.</p>

<p>What is it exactly and how does it work, briefly?</p>
",study necessity parsing predicate argument recognition  however find much information  predicate argument recognition  could explain   exactly work  briefly  ,studi necess pars predic argument recognit howev find much inform predic argument recognit could explain exact work briefli,"definitions,nlp,computational-linguistics","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,ai-design,image-recognition,reinforcement-learning,philosophy,algorithm,classification,training,computer-vision,game-ai,nlp,research"
628,"<p>The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.</p>

<p>Here is the <a href=""https://labs.wit.ai/demo/index.html"" rel=""nofollow"">demo site powered by Wit.ai</a>.</p>

<p>How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?</p>
",wit ai siri like voice interface parse messages predict actions perform   demo site powered wit ai   understand spoken sentences convert structured actionable data  basically  know  ,wit ai siri like voic interfac pars messag predict action perform demo site power wit ai understand spoken sentenc convert structur action data basic know,"language-processing,nlp,structured-data,voice-recognition","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,philosophy,tensorflow,game-theory,natural-language,training,strong-ai,classification"
629,"<p>In 2014 <a href=""https://techcrunch.com/2014/02/06/linkedin-snatches-up-data-savvy-job-search-startup-bright-com-for-120m-in-its-largest-acquisition-to-date/"" rel=""nofollow"">Linkedin acquired Bright.com</a>, for $120 million and it is using AI and big data algorithms to connect users.</p>

<blockquote>
  <p>Bright also throws in a little Klout, ranking people by a ?Bright score? which it uses to assess how strong the chemistry is between a user and a particular job.</p>
  
  <p>It also takes into account historical hiring patterns into its matching, along with account location, a user?s past experience and synonyms.</p>
</blockquote>

<p>In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?</p>
",     linkedin acquired bright com       million using ai big data algorithms connect users      bright also throws little klout  ranking people  bright score  uses assess strong chemistry user particular job       also takes account historical hiring patterns matching  along account location  user past experience synonyms    brief  known  based research papers  algorithm works aiming scoring  chemistry  users jobs  ,linkedin acquir bright com million use ai big data algorithm connect user bright also throw littl klout rank peopl bright score use assess strong chemistri user particular job also take account histor hire pattern match along account locat user past experi synonym brief known base research paper algorithm work aim score chemistri user job,social,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,game-ai,training,classification,image-recognition,genetic-algorithms,tensorflow,nlp,computer-vision"
630,"<p>According to this <a href=""http://mashable.com/2014/01/06/pinterest-acquires-visualgraph/"" rel=""nofollow"">article</a>, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.</p>

<p>How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?</p>

<p>In short, how do they predict the image categories? Based on what features?</p>
",according article  pinterest acquired visualgraph  image recognition visual search technology startup   pinterest apply visualgraph technology machine vision  image recognition visual search order classify images   short  predict image categories  based features  ,accord articl pinterest acquir visualgraph imag recognit visual search technolog startup pinterest appli visualgraph technolog machin vision imag recognit visual search order classifi imag short predict imag categori base featur,"image-recognition,classification,computer-vision","machine-learning,neural-networks,image-recognition,convolutional-neural-networks,deep-learning,classification,computer-vision,tensorflow,training,ai-design,algorithm,reinforcement-learning,keras,game-ai,object-recognition"
632,"<p>Wolfram Language Image Identification Project launched an <a href=""https://www.imageidentify.com/"" rel=""nofollow"">Image Identify site</a> demo which returns the top predicted tags for the photos.</p>

<p>How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?</p>
",wolfram language image identification project launched image identify site demo returns top predicted tags photos   work  briefly  mean type learning vision technologies used analyze  recognize understand content image  ,wolfram languag imag identif project launch imag identifi site demo return top predict tag photo work briefli mean type learn vision technolog use analyz recogn understand content imag,"image-recognition,deep-learning,classification","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,classification,tensorflow,computer-vision,ai-design,training,algorithm,keras,game-ai,natural-language"
634,"<p>I've <a href=""https://www.imageidentify.com/result/0lkzuttdxipub"" rel=""nofollow noreferrer"">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>

<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>

<p><a href=""https://i.stack.imgur.com/akquMm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/akquMm.png"" alt=""Wolfram&#39;s Image Identify: monocle/graffiti""></a></p>

<p>This could be the same problem as mentioned <a href=""https://ai.stackexchange.com/a/1533/8"">here</a>, that DNN are:</p>

<blockquote>
  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=""https://ai.stackexchange.com/a/1533/8"">2015</a></sup></p>
</blockquote>

<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>
",uploaded picture wolfram imageidentify graffiti wall  recognized  monocle   secondary guesses  primate    hominid    person   even close  graffiti   painting    design  methods teach convolutional neural network  cnn  reason aware bigger picture context  like mentioned graffiti   currently seems detecting literally depicted image  image actually     could problem mentioned  dnn      learning detect jaguars matching unique spots fur ignoring fact four legs        design  maybe better version cnn perform better  ,upload pictur wolfram imageidentifi graffiti wall recogn monocl secondari guess primat hominid person even close graffiti paint design method teach convolut neural network cnn reason awar bigger pictur context like mention graffiti current seem detect liter depict imag imag actual could problem mention dnn learn detect jaguar match uniqu spot fur ignor fact four leg design mayb better version cnn perform better,"image-recognition,classification,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,ai-design,classification,tensorflow,training,algorithm,computer-vision,game-ai,keras,deep-network"
637,"<p>An AI agent is often thought of having ""sensors"", ""a memory"", ""machine learning processors"" and ""reaction"" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>

<p>For example, <a href=""http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf"" rel=""nofollow"">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>

<blockquote>
  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its ""cognitive infrastructure"", where the latter is defined as the fuzzy set of ""intelligence-critical"" features of the system; and the intelligence-criticality of a system feature is defined as its ""feature quality,"" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>
</blockquote>

<p>However, this description of ""optimization of intelligence"" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>

<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>
",ai agent often thought  sensors    memory    machine learning processors   reaction  components  however  machine necessarily become self programming ai agent  beyond parts mentioned  elements details necessary make machine capable self programming ai agent   example  paper      declared solving optimization problem maximizing intelligence must feature self programming process  quoted      system said carry instance self programming undergoes learning regarding element  cognitive infrastructure   latter defined fuzzy set  intelligence critical  features system  intelligence criticality system feature defined  feature quality   considered perspective solving optimization problem maximizing intelligence multi feature system    however  description  optimization intelligence  vague  anyone give clear definition better summary necessary components self programming agents   question      closed beta  asker uid     ,ai agent often thought sensor memori machin learn processor reaction compon howev machin necessarili becom self program ai agent beyond part mention element detail necessari make machin capabl self program ai agent exampl paper declar solv optim problem maxim intellig must featur self program process quot system said carri instanc self program undergo learn regard element cognit infrastructur latter defin fuzzi set intellig critic featur system intellig critic system featur defin featur qualiti consid perspect solv optim problem maxim intellig multi featur system howev descript optim intellig vagu anyon give clear definit better summari necessari compon self program agent question close beta asker uid,"machine-learning,computer-programming","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,philosophy,algorithm,training,convolutional-neural-networks,game-ai,classification,image-recognition,multi-agent-systems,nlp,tensorflow"
643,"<p>In a <a href=""http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619"">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>

<blockquote>
  <p>The next step in achieving human-level ai is creating intelligent?but not autonomous?machines. The AI system in your car will get you safely home, but won?t choose another destination once you?ve gone inside. From there, we?ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it?s easy to imagine them inheriting human-like qualities?and flaws. </p>
</blockquote>

<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>
",recent wall street journal article  yann lecunn makes following statement      next step achieving human level ai creating intelligent autonomous machines  ai system car get safely home  choose another destination gone inside   add basic drives  along emotions moral values  create machines learn well brains  easy imagine inheriting human like qualities flaws     personally  generally taken position talking emotions artificial intelligences silly  would reason create ai experience emotions   obviously yann disagrees   question   end would served   ai need emotions serve useful tool    ,recent wall street journal articl yann lecunn make follow statement next step achiev human level ai creat intellig autonom machin ai system car get safe home choos anoth destin gone insid add basic drive along emot moral valu creat machin learn well brain easi imagin inherit human like qualiti flaw person general taken posit talk emot artifici intellig silli would reason creat ai experi emot obvious yann disagre question end would serv ai need emot serv use tool,"philosophy,emotional-intelligence","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,philosophy,game-ai,convolutional-neural-networks,algorithm,image-recognition,tensorflow,strong-ai,genetic-algorithms,training,game-theory"
644,"<p>Inspired by <a href=""https://ai.stackexchange.com/q/1481/8"">this discussion</a> about recognizing human actions, I have found the <a href=""https://github.com/harishrithish7/Fall-Detection"" rel=""nofollow noreferrer"">Fall-Detection</a> project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.</p>

<p>My question is, are there any existing real-life implementations or research projects <strong>which specifically use live video feed from the surveillance cameras in order to detect crime</strong> using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?</p>

<p>For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.</p>
",inspired discussion recognizing human actions  found fall detection project detects humans falling ground cctv camera feed  consider alerting hospital authorities   question  existing real life implementations research projects specifically use live video feed surveillance cameras order detect crime using convnets  similar approaches    work  briefly  automatically inform police crime details happened   example car accidents  physical assaults  robberies  violent disturbances  weapon attacks  etc  ,inspir discuss recogn human action found fall detect project detect human fall ground cctv camera feed consid alert hospit author question exist real life implement research project specif use live video feed surveil camera order detect crime use convnet similar approach work briefli automat inform polic crime detail happen exampl car accid physic assault robberi violent disturb weapon attack etc,"convolutional-neural-networks,computer-vision,action-recognition","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,image-recognition,game-ai,algorithm,tensorflow,training,classification,philosophy,computer-vision,nlp"
648,"<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.</p>

<p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p>

<p>Let's assume that the user entered the following input:</p>

<pre><code>A, B, C, and D
</code></pre>

<p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p>

<p>And also, let's say that <code>A and B</code> was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C and D</code> where <code>C</code> doesn't exist in the data-table, but there is a possibility that <code>D</code> exists.</p>

<p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data-table. So, <code>C</code> gets a score of 0%.</p>

<p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p>

<p>Summary of generating the output:</p>

<pre><code>If A and B were entered and exist, then output = 100%
If D was entered and existed but C was not, then output = 90%
If all entered don't exist, then output = 0%
</code></pre>

<p>What techniques would be used to produce this system?</p>
",trying come right algorithm system user enters symptoms system predict determine likelihood selected symptoms associated existing system  associating  result output specific disease symptoms   system comprised series diseases assigned specific symptoms  also exist system   let assume user entered following input    b  c    first thing system check associate symptom  case represented alphabetical letters  individually data table symptoms already exist  cases input exist  system report send feedback   also  let say b data table       sure valid exist system able give disease based input  let say input c c exist data table  possibility exists   give score       maybe something lower  let say       c exist data table   c gets score      therefore  system kind association prediction techniques rules output result judging user input   summary generating output   b entered exist  output        entered existed c  output       entered exist  output        techniques would used produce system  ,tri come right algorithm system user enter symptom system predict determin likelihood select symptom associ exist system associ result output specif diseas symptom system compris seri diseas assign specif symptom also exist system let assum user enter follow input b c first thing system check associ symptom case repres alphabet letter individu data tabl symptom alreadi exist case input exist system report send feedback also let say b data tabl sure valid exist system abl give diseas base input let say input c c exist data tabl possibl exist give score mayb someth lower let say c exist data tabl c get score therefor system kind associ predict techniqu rule output result judg user input summari generat output b enter exist output enter exist c output enter exist output techniqu would use produc system,"algorithm,machine-learning,prediction","neural-networks,machine-learning,deep-learning,ai-design,algorithm,convolutional-neural-networks,reinforcement-learning,classification,training,tensorflow,game-theory,decision-theory,backpropagation,game-ai,image-recognition"
649,"<p>I have gone through the <a href=""https://en.wikipedia.org/wiki/Statistical_relational_learning"">wikipedia explanation of SRL</a>. But, it only confused me more:</p>

<blockquote>
  <p>Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.</p>
</blockquote>

<p>Can someone give a more dumbed down explanation of the same, preferably with an example?</p>
",gone wikipedia explanation srl   confused      statistical relational learning  srl  subdiscipline artificial intelligence machine learning concerned domain models exhibit uncertainty  dealt using statistical methods  complex  relational structure    someone give dumbed explanation  preferably example  ,gone wikipedia explan srl confus statist relat learn srl subdisciplin artifici intellig machin learn concern domain model exhibit uncertainti dealt use statist method complex relat structur someon give dumb explan prefer exampl,"definitions,statistical-ai","machine-learning,neural-networks,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,tensorflow,algorithm,classification,game-ai,training,philosophy,image-recognition,keras,q-learning"
653,"<p>The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?</p>

<p>I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?</p>

<p>The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality.</p>
",obvious solution ensure training data balanced   particular case impossible  corrections one perform scenario   know training data heavily biased towards particular class  say  cannot change  moreover  labels noisy  conditioned piece information  anything tweaking training process  something else  correct bias training data   data comes experiment  electron microscope   cannot collect data  always going biased way  alternatively biased also option  sorry unable provide details due confidentiality  ,obvious solut ensur train data balanc particular case imposs correct one perform scenario know train data heavili bias toward particular class say chang moreov label noisi condit piec inform anyth tweak train process someth els correct bias train data data come experi electron microscop collect data alway go bias way altern bias also option sorri unabl provid detail due confidenti,"neural-networks,research,deep-network,training","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,training,tensorflow,classification,ai-design,reinforcement-learning,image-recognition,keras,computer-vision,python,algorithm,game-ai"
655,"<p><strong>Note:</strong> I wanted to ask a meta-post first to see if this site was supposed to be used only for AI-related questions, or if AI-related questions such as this were allowed, too, but apparently you need to have asked five actual questions first.</p>

<hr>

<p>I'm going to be entering a masters computer science program in the fall, and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications. While I have a math and CS background, I have almost no biology/neuroscience background, and my linguistics background is limited to the random research I've done in my spare time to satiate my curiosities.</p>

<p>What are good non-math and CS related topics to study for these fields? </p>
",note  wanted ask meta post first see site supposed used ai related questions  ai related questions allowed   apparently need asked five actual questions first     going entering masters computer science program fall  wanted move towards concentration computational neuroscience linguistics ai development applications  math cs background  almost biology neuroscience background  linguistics background limited random research done spare time satiate curiosities   good non math cs related topics study fields   ,note want ask meta post first see site suppos use ai relat question ai relat question allow appar need ask five actual question first go enter master comput scienc program fall want move toward concentr comput neurosci linguist ai develop applic math cs background almost biolog neurosci background linguist background limit random research done spare time satiat curios good non math cs relat topic studi field,"research,machine-learning,neuromorphic-computing,computational-linguistics","machine-learning,neural-networks,deep-learning,ai-design,game-ai,reinforcement-learning,convolutional-neural-networks,philosophy,algorithm,image-recognition,training,tensorflow,genetic-algorithms,strong-ai,natural-language"
658,"<p><a href=""http://www.alicebot.org/articles/wallace/eliza.html"" rel=""nofollow"">From Eliza to A.L.I.C.E.</a>:</p>

<blockquote>
  <p>Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as ""Doctor"") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a ""real"" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.</p>
</blockquote>

<p>Wikipedia's article on the <a href=""https://en.wikipedia.org/wiki/ELIZA_effect"" rel=""nofollow"">""ELIZA Effect""</a>:</p>

<blockquote>
  <p>Though designed strictly as a mechanism to support ""natural language conversation"" with a computer, ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output. As Weizenbaum later wrote, <strong>""I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.""</strong> Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, <em>even when they consciously knew that ELIZA did not simulate emotion.</em></p>
</blockquote>

<p>ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as <a href=""http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0"" rel=""nofollow"">Xiaoice</a>. But I would like to know what <em>exactly</em> led to such a simple program like ELIZA to be so successful in the first place.</p>

<p>This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.</p>
",eliza l c e       weizenbaum tells us shocked experience releasing eliza  also known  doctor   nontechnical staff mit ai lab  secretaries nontechnical administrative staff thought machine  real  therapist  spent hours revealing personal problems program  weizenbaum informed secretary  course  access logs conversations  reacted outrage invasion privacy  weizenbaum shocked similar incidents find simple program could easily deceive naive user revealing personal information    wikipedia article  eliza effect       though designed strictly mechanism support  natural language conversation  computer  eliza doctor script found surprisingly successful eliciting emotional responses users  course interacting program  began ascribe understanding motivation program output  weizenbaum later wrote   realized     extremely short exposures relatively simple computer program could induce powerful delusional thinking quite normal people   indeed  eliza code designed evoke reaction first place  upon observation  researchers discovered users unconsciously assuming eliza questions implied interest emotional involvement topics discussed  even consciously knew eliza simulate emotion    eliza  despite simplicity  incredibly successful task tricking human beings  even knew eliza bot would still talk  obviously  eliza served inspiration various  intelligent chatbots  xiaoice  would like know exactly led simple program like eliza successful first place   useful knowledge programmer since simple program one would easily maintainable  ,eliza l c e weizenbaum tell us shock experi releas eliza also known doctor nontechn staff mit ai lab secretari nontechn administr staff thought machin real therapist spent hour reveal person problem program weizenbaum inform secretari cours access log convers react outrag invas privaci weizenbaum shock similar incid find simpl program could easili deceiv naiv user reveal person inform wikipedia articl eliza effect though design strict mechan support natur languag convers comput eliza doctor script found surpris success elicit emot respons user cours interact program began ascrib understand motiv program output weizenbaum later wrote realiz extrem short exposur relat simpl comput program could induc power delusion think quit normal peopl inde eliza code design evok reaction first place upon observ research discov user unconsci assum eliza question impli interest emot involv topic discuss even conscious knew eliza simul emot eliza despit simplic incred success task trick human even knew eliza bot would still talk obvious eliza serv inspir various intellig chatbot xiaoic would like know exact led simpl program like eliza success first place use knowledg programm sinc simpl program one would easili maintain,"history,turing-test,emotional-intelligence,chat-bots","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,philosophy,game-ai,image-recognition,classification,natural-language,training,genetic-algorithms,nlp"
670,"<p>What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?</p>

<p>So far I know of <a href=""http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html"">Sir David King's report done for UK government</a>.</p>
",regulations already place regarding artificial general intelligences  reports recommendations prepared official government authorities already published   far know sir david king report done uk government  ,regul alreadi place regard artifici general intellig report recommend prepar offici govern author alreadi publish far know sir david king report done uk govern,"agi,legal","machine-learning,neural-networks,deep-learning,ai-design,philosophy,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,research,definitions,strong-ai,image-recognition,nlp,natural-language"
671,"<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite). This way we are introduced quickly to Value Iteration, Q-Learning, and the like.</p>

<p>However the most interesting applications (say, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces. I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p>

<p>What areas do I need to know or study to understand these cases in depth?</p>
",introductions field mdps reinforcement learning focus exclusively domains space action variables integers  finite   way introduced quickly value iteration  q learning  like   however interesting applications  say  flying helicopters  rl mdps involve continuous state space action spaces  like go beyond basic introductions focus cases sure get    areas need know study understand cases depth  ,introduct field mdps reinforc learn focus exclus domain space action variabl integ finit way introduc quick valu iter q learn like howev interest applic say fli helicopt rl mdps involv continu state space action space like go beyond basic introduct focus case sure get area need know studi understand case depth,"research,reinforcement-learning,control-problem","machine-learning,neural-networks,deep-learning,reinforcement-learning,convolutional-neural-networks,tensorflow,ai-design,q-learning,algorithm,game-ai,training,classification,image-recognition,genetic-algorithms,backpropagation"
678,"<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p>
",someone explain difference machine learning deep learning  possible learn deep learning without knowing machine learning  ,someon explain differ machin learn deep learn possibl learn deep learn without know machin learn,"machine-learning,deep-learning","machine-learning,neural-networks,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,self-learning,tensorflow,classification,q-learning,algorithm,training,game-ai,genetic-algorithms,unsupervised-learning"
686,"<p>By new, unseen examples; I mean like the animals in <a href=""https://en.wikipedia.org/wiki/No_Man%27s_Sky"" rel=""noreferrer"">No Man's Sky</a>. </p>

<p>A couple of images of the animals are:
<a href=""https://i.stack.imgur.com/zS0rX.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zS0rX.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ir1Qt.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ir1Qt.jpg"" alt=""enter image description here""></a></p>

<p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p>
",new  unseen examples  mean like animals man sky    couple images animals       upon playing game  curious good ai generating visual characters examples  ,new unseen exampl mean like anim man sky coupl imag anim upon play game curious good ai generat visual charact exampl,"research,image-recognition","machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,game-ai,image-recognition,tensorflow,algorithm,classification,training,philosophy,genetic-algorithms,computer-vision"
687,"<p>I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?</p>
",wanted know differences hyper heuristics meta heuristics  main applications  problems suited solved hyper heuristics  ,want know differ hyper heurist meta heurist main applic problem suit solv hyper heurist,"definitions,optimization","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,reinforcement-learning,algorithm,ai-design,game-ai,image-recognition,classification,training,tensorflow,computer-vision,genetic-algorithms,deep-network"
691,"<p>What is the difference between agent function and agent program with respect to percept sequence?</p>

<p>In the book <em>""Artificial Intelligence: A modern approach""</em>,</p>

<blockquote>
  <p>The agent function, notionally speaking, takes as input the entire
  percept sequence up to that point, whereas the agent program takes the
  current percept only.</p>
</blockquote>

<p>Why does the agent program only take current percept. Isn't it just implementation of the agent function?</p>
",difference agent function agent program respect percept sequence   book  artificial intelligence  modern approach       agent function  notionally speaking  takes input entire   percept sequence point  whereas agent program takes   current percept    agent program take current percept  implementation agent function  ,differ agent function agent program respect percept sequenc book artifici intellig modern approach agent function notion speak take input entir percept sequenc point wherea agent program take current percept agent program take current percept implement agent function,"definitions,models,intelligent-agent,reinforcement-learning","neural-networks,machine-learning,reinforcement-learning,deep-learning,ai-design,multi-agent-systems,game-ai,convolutional-neural-networks,algorithm,philosophy,intelligent-agent,training,game-theory,tensorflow,genetic-algorithms"
693,"<p>Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?</p>
",currently studies simulate gradual  sudden  implementation ais general work force  ,current studi simul gradual sudden implement ai general work forc,"research,implementation","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,strong-ai,algorithm,game-theory,genetic-algorithms,tensorflow,research,training"
699,"<p>In <a href=""https://en.wikipedia.org/wiki/Portal_2"" rel=""noreferrer"">Portal 2</a> we see that AI's can be ""killed"" by thinking about a paradox.</p>

<p><a href=""https://i.stack.imgur.com/wkUSC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wkUSC.png"" alt=""Portal Paradox Poster""></a></p>

<p>I assume this works by forcing the AI into an infinite loop which would essentially ""freeze"" the computer's consciousness.</p>

<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>
",portal   see ai  killed  thinking paradox     assume works forcing ai infinite loop would essentially  freeze  computer consciousness   questions  would confuse ai technology today point destroying      could possible future  ,portal see ai kill think paradox assum work forc ai infinit loop would essenti freez comput conscious question would confus ai technolog today point destroy could possibl futur,"decision-theory,death","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,algorithm,convolutional-neural-networks,strong-ai,image-recognition,game-theory,genetic-algorithms,research,training"
705,"<p>In the 1950's, there were widely-held beliefs that ""Artificial Intelligence"" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's ""Official History of the Perceptron Controversy"", or let say 2001: Space Odyssey).</p>

<p>When did it become clear that devising programs that master games like chess resulted in software designs that only applied to games like the ones for which they were programmed? Who was the first person to recognize the distinction between human-like general intelligence and domain specific intelligence?</p>

<p>(thanks to Douglas Daseeco for a better way to phrase this question)</p>
",      widely held beliefs  artificial intelligence  quickly become self conscious smart enough win chess humans  various people suggested time frames e g     years  see olazaran  official history perceptron controversy   let say       space odyssey    become clear devising programs master games like chess resulted software designs applied games like ones programmed  first person recognize distinction human like general intelligence domain specific intelligence    thanks douglas daseeco better way phrase question  ,wide held belief artifici intellig quick becom self conscious smart enough win chess human various peopl suggest time frame e g year see olazaran offici histori perceptron controversi let say space odyssey becom clear devis program master game like chess result softwar design appli game like one program first person recogn distinct human like general intellig domain specif intellig thank dougla daseeco better way phrase question,history,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,game-ai,philosophy,convolutional-neural-networks,algorithm,training,image-recognition,tensorflow,classification,research,game-theory"
706,"<p>We hear a lot today about how <a href=""http://deeplearning4j.org/thoughtvectors"" rel=""nofollow"">thought vectors</a> are the <a href=""http://www.extremetech.com/extreme/206521-thought-vectors-could-revolutionize-artificial-intelligence"" rel=""nofollow"">Next Big Thing in AI</a>, and how they serve as the underlying representation of thought/knowledge in ANN's.  But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI?  Could thought vectors be the ""substrate"" that binds together probabilistic approaches to AI and approaches that are rooted in logic?  </p>
",hear lot today thought vectors next big thing ai  serve underlying representation thought knowledge ann   one use thought vectors regimes  especially including symbolic logic   gofai   could thought vectors  substrate  binds together probabilistic approaches ai approaches rooted logic    ,hear lot today thought vector next big thing ai serv represent thought knowledg ann one use thought vector regim especi includ symbol logic gofai could thought vector substrat bind togeth probabilist approach ai approach root logic,"neural-networks,gofai,logic,thought-vectors","machine-learning,neural-networks,ai-design,deep-learning,game-ai,reinforcement-learning,image-recognition,convolutional-neural-networks,philosophy,algorithm,training,classification,natural-language,strong-ai,tensorflow"
713,"<p>A system makes a decision basing on a large number of <em>varied</em> factors, following a ""live"" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.</p>

<p>The individual decisions can be recorded as a kind of structure:</p>

<ul>
<li>decision function</li>
<li>node to activate if decision is positive</li>
<li>node to activate if decision is negative</li>
</ul>

<p>and a node can be another decision record, or a conclusion.</p>

<p>This isn't entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.</p>

<p>There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It's perfectly sufficient for the machine.</p>

<p>The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: <strong>debugging the AI learning process.</strong></p>

<p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>

<p>a non-working example of the answer is <a href=""https://en.wikipedia.org/wiki/Concept_map"" rel=""nofollow"">Concept map</a> - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?</p>
",system makes decision basing large number varied factors  following  live  decision tree   one  independently  subsystem  updated new decisions  new situations   individual decisions recorded kind structure    decision function node activate decision positive node activate decision negative   node another decision record  conclusion   entirely binary tree  many decisions may lead conclusion   node two children  may many parents   absolutely problem storing tree memory   database records entries map  list  perfectly sufficient machine   problem building subsystem expands decision tree   particular  human operator understand structure built  able tune  guide  fix  adjust  debugging ai learning process   question  represent data human readable way  emphasizes flow graph   non working example answer concept map   case goes far  thirty nodes  becomes jumbled mess  especially number cross connections  multiple parents  becomes significant  maybe exists way laying slicing make clearer     ,system make decis base larg number vari factor follow live decis tree one independ subsystem updat new decis new situat individu decis record kind structur decis function node activ decis posit node activ decis negat node anoth decis record conclus entir binari tree mani decis may lead conclus node two children may mani parent absolut problem store tree memori databas record entri map list perfect suffici machin problem build subsystem expand decis tree particular human oper understand structur built abl tune guid fix adjust debug ai learn process question repres data human readabl way emphas flow graph non work exampl answer concept map case goe far thirti node becom jumbl mess especi number cross connect multipl parent becom signific mayb exist way lay slice make clearer,"machine-learning,decision-theory,implementation","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,classification,genetic-algorithms,philosophy,image-recognition,training,tensorflow,backpropagation"
714,"<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p>

<p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones.  </p>

<p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p>
",currently working childes corpus trying create classifier distinguishes children suffer specific language impairment  sli  typically developing  td    readings noticed really convincing set features distinguish two discovered yet  came upon idea trying create feature learning algorithm could potentially make better ones     possible  suggest approach  reading done  feature learning done image processing  another problem dataset potentially small make work       unless find way get transcripts children  ,current work child corpus tri creat classifi distinguish children suffer specif languag impair sli typic develop td read notic realli convinc set featur distinguish two discov yet came upon idea tri creat featur learn algorithm could potenti make better one possibl suggest approach read done featur learn done imag process anoth problem dataset potenti small make work unless find way get transcript children,"research,deep-learning,classification,language-processing,training","machine-learning,neural-networks,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,algorithm,image-recognition,classification,game-ai,tensorflow,training,genetic-algorithms,computer-vision,philosophy"
722,"<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>

<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=""http://www.yudkowsky.net/singularity/aibox/"" rel=""noreferrer"">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>

<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>
",ai box  physical  barrier preventing ai using much environment accomplish final goal  example  ai given task check  say       cases mathematical conjecture fast possible  might decide would better also take control computers ai help    however  transhuman ai might able talk human human lets box  fact  eliezer yudowsky conducted experiment twice  played ai twice convinced gatekeeper let box  however  want reveal methods used get box   questions  conducted similiar experiments    known methods used get experiments  ,ai box physic barrier prevent ai use much environ accomplish final goal exampl ai given task check say case mathemat conjectur fast possibl might decid would better also take control comput ai help howev transhuman ai might abl talk human human let box fact eliez yudowski conduct experi twice play ai twice convinc gatekeep let box howev want reveal method use get box question conduct similiar experi known method use get experi,ai-box,"machine-learning,neural-networks,deep-learning,ai-design,game-ai,reinforcement-learning,convolutional-neural-networks,philosophy,algorithm,image-recognition,tensorflow,training,strong-ai,genetic-algorithms,game-theory"
734,"<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>

<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>

<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>

<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>
",ai systems today capable machines  recently area natural language processing response exploding innovation  well fundamental algorithmic structure ai machines   asking  given recent breakthroughs  ai systems developed able  preferably measure success  knowingly lie humans facts knows   note  asking goes beyond canonical discussions turing test  asking machines  understand  facts formulate lie fact  perhaps using facts produce believable  cover  part lie   e g   cia supercomputer stolen spies try use computer things  computer keeps saying missing dependencies though really gives correct looking wrong answers knowingly  gives incorrect location person  knowing person frequents place moment  sophisticated  course  ,ai system today capabl machin recent area natur languag process respons explod innov well fundament algorithm structur ai machin ask given recent breakthrough ai system develop abl prefer measur success know lie human fact know note ask goe beyond canon discuss ture test ask machin understand fact formul lie fact perhap use fact produc believ cover part lie e g cia supercomput stolen spi tri use comput thing comput keep say miss depend though realli give correct look wrong answer know give incorrect locat person know person frequent place moment sophist cours,"nlp,human-like","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,game-ai,philosophy,image-recognition,training,classification,nlp,natural-language,tensorflow"
737,"<p><em>""An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.""</em> (<a href=""https://simple.wikipedia.org/wiki/Constructed_language"" rel=""noreferrer"">Source: Simply English Wikipedia</a>)</p>

<p>My question is, could an AI make construct it's own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p>

<p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=""https://en.wikipedia.org/wiki/Esperanto"" rel=""noreferrer"">Esperanto</a>)?</p>
", artificial constructed language  sometimes called conlang  language created person small group  instead formed naturally part culture    source  simply english wikipedia   question  could ai make construct natural language  words  conjugations grammar rules  basically  language humans could use speak   preferably communicate abstract  high level concepts    techniques could ai use  could based existing natural languages would connections existing natural languages  could design language easier learn existing languages  even esperanto   ,artifici construct languag sometim call conlang languag creat person small group instead form natur part cultur sourc simpli english wikipedia question could ai make construct natur languag word conjug grammar rule basic languag human could use speak prefer communic abstract high level concept techniqu could ai use could base exist natur languag would connect exist natur languag could design languag easier learn exist languag even esperanto,natural-language,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,natural-language,game-ai,convolutional-neural-networks,algorithm,nlp,philosophy,image-recognition,training,classification,genetic-algorithms"
742,"<p>I want to start with a scenario that got me thinking about how well MCTS can perform:
Let's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that <em>all</em> moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and <em>not</em> grow the search tree towards this move and also rate this subtree very badly? 
I know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.
Maybe someone can tell me if this is a correct evaluation on my part.</p>

<p>Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). </p>
",want start scenario got thinking well mcts perform  let assume move yet added search tree  layers moves deep  play move game basically  however let also assume moves could taken instead given game state bad  sake argument let say      possible moves one good  good  rest bad  mcts fail recognize grow search tree towards move also rate subtree badly   know mcts eventually converges minimax  eventually build whole tree enough memory   know move good even though many bad possiblities  guess practice something one rely  maybe someone tell correct evaluation part   apart special scenario also like know scenarios mcts perform badly  extraordinary well    ,want start scenario got think well mcts perform let assum move yet ad search tree layer move deep play move game basic howev let also assum move could taken instead given game state bad sake argument let say possibl move one good good rest bad mcts fail recogn grow search tree toward move also rate subtre bad know mcts eventu converg minimax eventu build whole tree enough memori know move good even though mani bad possibl guess practic someth one reli mayb someon tell correct evalu part apart special scenario also like know scenario mcts perform bad extraordinari well,"gaming,monte-carlo-search","neural-networks,machine-learning,deep-learning,reinforcement-learning,game-ai,ai-design,convolutional-neural-networks,algorithm,tensorflow,training,q-learning,image-recognition,combinatorial-games,classification,philosophy"
750,"<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>
",reading nonsense ai would turn world supercomputer solve problem thought needed solve  ai  procedural programming stuck loop nonsense  ai would need evolve organise neurons  stuck hardcode becomes intelligent writing code  ,read nonsens ai would turn world supercomput solv problem thought need solv ai procedur program stuck loop nonsens ai would need evolv organis neuron stuck hardcod becom intellig write code,philosophy,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,algorithm,reinforcement-learning,convolutional-neural-networks,genetic-algorithms,strong-ai,game-theory,image-recognition,natural-language,classification"
751,"<p>I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv).</p>
",process learning much chatbots cui applications possible trying find information major players field   mean execs  developers  academics  designers  etc  cutting edge things  examples could david marcus  vp messaging products facebook  adam cheyer  vp engineering viv   ,process learn much chatbot cui applic possibl tri find inform major player field mean exec develop academ design etc cut edg thing exampl could david marcus vp messag product facebook adam cheyer vp engin viv,chat-bots,"machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,game-ai,algorithm,image-recognition,classification,tensorflow,training,philosophy,nlp,combinatorial-games"
757,"<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=""http://www.scientificamerican.com/article/thinking-hard-calories/"">12.6 watts</a>)?</p>

<p>Let assume one cycle per second, which seems to roughly match the <a href=""http://www.jneurosci.org/content/31/45/16217.full"">firing rate of biological neurons</a>.</p>
",big artificial neural networks run  either full train backprop cycle evaluating network outputs  total energy budget computation equivalent human brain energy budget       watts    let assume one cycle per second  seems roughly match firing rate biological neurons  ,big artifici neural network run either full train backprop cycl evalu network output total energi budget comput equival human brain energi budget watt let assum one cycl per second seem rough match fire rate biolog neuron,"neural-networks,human-like","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,training,game-ai,classification,backpropagation,image-recognition,algorithm,recurrent-neural-networks,artificial-neuron"
760,"<blockquote>
  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=""http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf"" rel=""nofollow noreferrer"">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>
</blockquote>

<p>43 years later...</p>

<blockquote>
  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=""https://area51.meta.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539"">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>
</blockquote>

<p>AI is a label that is applied to a ""very mixed bunch of activities"". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>

<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its ""big tent"" nature?</p>
",   artificial intelligence rather pernicious label attach mixed bunch activities  one could argue sooner forget better  would disastrous conclude ai bad thing supported  would disastrous conclude good thing privileged access money tap  former would tend penalise well based efforts make computers complicated things programmed  latter would great waste resources  ai refer anything definite enough coherent policy way    dr  r   needham  commentary lighthill report sutherland reply            years later        already strong demand engineers scientists working artificial intelligence many fields mention  many  expertise making real time systems controlling trains make know anything robotics  analyzing human behavior detect crime virtually nothing common self driving cars  beyond cs pattern recognition building blocks   never going demand someone broad sense areas without deep expertise  never going someone     phds work  tl dr    ai branch  tree    matthew read  comment area    stackexchange         ai label applied  mixed bunch activities   unifying feature activities fact deal machines fashion  since many ways use machine  field output may seem rather incoherent incongruent  seem make sense ai field collapse entirely  instead replaced multitude specialized fields really interact one another  sir james lighthill appeared supported sort approach within      report state artificial intelligence research   yet  today  artificial intelligence se exist  still talk ai unified  coherent field study  happen  ai survive  despite  big tent  nature  ,artifici intellig rather pernici label attach mix bunch activ one could argu sooner forget better would disastr conclud ai bad thing support would disastr conclud good thing privileg access money tap former would tend penalis well base effort make comput complic thing program latter would great wast resourc ai refer anyth definit enough coher polici way dr r needham commentari lighthil report sutherland repli year later alreadi strong demand engin scientist work artifici intellig mani field mention mani expertis make real time system control train make know anyth robot analyz human behavior detect crime virtual noth common self drive car beyond cs pattern recognit build block never go demand someon broad sens area without deep expertis never go someon phds work tl dr ai branch tree matthew read comment area stackexchang ai label appli mix bunch activ unifi featur activ fact deal machin fashion sinc mani way use machin field output may seem rather incoher incongru seem make sens ai field collaps entir instead replac multitud special field realli interact one anoth sir jame lighthil appear support sort approach within report state artifici intellig research yet today artifici intellig se exist still talk ai unifi coher field studi happen ai surviv despit big tent natur,history,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,philosophy,game-ai,algorithm,image-recognition,training,tensorflow,genetic-algorithms,classification,strong-ai"
762,"<p>Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. </p>
",let suppose legacy system source code system mainframe written cobol  way using machine learning learn inputs outputs way executables work  analysis could lead develop rest   soap webservice substitute legacy system opinion   ,let suppos legaci system sourc code system mainfram written cobol way use machin learn learn input output way execut work analysi could lead develop rest soap webservic substitut legaci system opinion,machine-learning,"neural-networks,machine-learning,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,algorithm,training,classification,tensorflow,image-recognition,game-ai,backpropagation,genetic-algorithms,nlp"
767,"<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">GOFAI</a>.</p>

<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p>

<p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p>

<p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p>
",sometimes understand people cognitive science try avoid term artificial intelligence  feeling get need put distance gofai   another impression get cognitive science trying find human intelligence       mind  works    would use artificial intelligence make tests experiments  test ideas forth     artificial intelligence   research tool cognitive science   difference artificial intelligence cognitive science  ,sometim understand peopl cognit scienc tri avoid term artifici intellig feel get need put distanc gofai anoth impress get cognit scienc tri find human intellig mind work would use artifici intellig make test experi test idea forth artifici intellig research tool cognit scienc differ artifici intellig cognit scienc,"terminology,cognitive-science","neural-networks,machine-learning,deep-learning,ai-design,philosophy,convolutional-neural-networks,reinforcement-learning,algorithm,game-ai,image-recognition,tensorflow,classification,training,genetic-algorithms,strong-ai"
771,"<p>Just for fun, I am trying to develop a neural network.</p>

<p>Now, for backpropagation I saw two techniques.</p>

<p>The first one is used <a href=""http://courses.cs.washington.edu/courses/cse599/01wi/admin/Assignments/bpn.html"" rel=""nofollow noreferrer"">here</a> and in many other places too.</p>

<p>What it does is:</p>

<ul>
<li>It computes the error for each output neuron.</li>
<li>It backpropagates it into the network (calculating an error for each inner neuron).</li>
<li>It updates the weights with the formula: <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D%20%3D%20k%20%5Ccdot%20E_%7Bl&plus;1%2Cn%7D%20%5Ccdot%20N_%7Bl%2Cm%7D"" alt=""""> (where <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D"" alt=""""> is the change in weight, <img src=""https://latex.codecogs.com/gif.latex?k"" alt=""""> the learning speed, <img src=""https://latex.codecogs.com/gif.latex?E_%7Bl&plus;1%2Cn%7D"" alt=""""> the error of the neuron receiving the input from the synapse and <img src=""https://latex.codecogs.com/gif.latex?N_%7Bl%2Cm%7D"" alt=""""> being the output sent on the synapse).</li>
<li>It repeats for each entry of the dataset, as many times as required.</li>
</ul>

<p>However, the neural network proposed in <a href=""https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR"" rel=""nofollow noreferrer"">this tutorial</a> (also available on GitHub) uses a different technique:</p>

<ul>
<li>It uses an error function (the other method does have an error function, but it does not use it for training).</li>
<li>It has another function which can compute the final error starting from the weights.</li>
<li>It minimizes that function (through gradient descent).</li>
</ul>

<p>Now, which method should be used?</p>

<p>I think the first one is the most used one (because I saw different examples using it), but does it work as well?</p>

<p>In particular, I don't know:</p>

<ul>
<li>Isn't it more subject to local minimums (since it doesn't use quadratic functions)?</li>
<li>Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?</li>
</ul>

<p>Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.</p>

<p>Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?</p>
",fun  trying develop neural network    backpropagation saw two techniques   first one used many places      computes error output neuron  backpropagates network  calculating error inner neuron   updates weights formula     change weight   learning speed   error neuron receiving input synapse  output sent synapse   repeats entry dataset  many times required    however  neural network proposed tutorial  also available github  uses different technique    uses error function  method error function  use training   another function compute final error starting weights  minimizes function  gradient descent      method used   think first one used one  saw different examples using   work well   particular  know    subject local minimums  since use quadratic functions   since variation weight influenced output value output neuron  entries dataset happen produce higher values neurons  output ones  influence weights entries     prefer first technique  find simpler implement easier think   though  problems mentioned  hope   actual reason use second method  ,fun tri develop neural network backpropag saw two techniqu first one use mani place comput error output neuron backpropag network calcul error inner neuron updat weight formula chang weight learn speed error neuron receiv input synaps output sent synaps repeat entri dataset mani time requir howev neural network propos tutori also avail github use differ techniqu use error function method error function use train anoth function comput final error start weight minim function gradient descent method use think first one use one saw differ exampl use work well particular know subject local minimum sinc use quadrat function sinc variat weight influenc output valu output neuron entri dataset happen produc higher valu neuron output one influenc weight entri prefer first techniqu find simpler implement easier think though problem mention hope actual reason use second method,"neural-networks,machine-learning,backpropagation","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,backpropagation,training,tensorflow,game-ai,algorithm,classification,image-recognition,genetic-algorithms,artificial-neuron"
773,"<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually ""doing"". Thus, it may make more sense to use ""human-like"" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>

<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=""https://www.landley.net/history/mirror/jargon.html#Anthropomorphization"">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>

<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>

<ol>
<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>
<li><em>Very Technical</em> - The algorithm converts each article into a ""bag-of-words"", and then compare the ""bag-of-words"" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>
</ol>

<p>Obviously, #2 may be more ""technically correct"" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>

<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer ""reads"" the article, we can then focus on using the algorithm in real-world scenarios.</p>

<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>

<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>
",english language well suited talking artificial intelligence  makes difficult humans communicate ai actually    thus  may make sense use  human like  terms describe actions machinery  even internal properties machinery resemble internal properties humanity   anthropomorphic language used lot technology  see hacker dictionary definition anthropomorphization  attempts justify computer programmers  use anthromporhic terms describing technology   ai continues advance  may useful consider tradeoffs using anthropomorphic language communicating technical audiences non technical audiences  get good handle ai even describe   suppose want develop algorithm display list related articles  two ways explain algorithm works layman    anthropomorphic   algorithm reads articles website  display articles similar article looking  technical   algorithm converts article  bag words   compare  bag words  article determine articles share common words  articles share words bags ones displayed user    obviously     may  technically correct      detailing implementation algorithm  makes easier someone understand fix algorithm produces output disagree heavily      readable  elegant  easier understand  provides general sense algorithm  instead algorithm  abstracting away implementation details computer  reads  article  focus using algorithm real world scenarios    therefore  prefer use anthropomorphic language emphasized statement        p   answer depends audience speaking  non technical audience might prefer     technical audience may prefer      let know well  ,english languag well suit talk artifici intellig make difficult human communic ai actual thus may make sens use human like term describ action machineri even intern properti machineri resembl intern properti human anthropomorph languag use lot technolog see hacker dictionari definit anthropomorph attempt justifi comput programm use anthromporh term describ technolog ai continu advanc may use consid tradeoff use anthropomorph languag communic technic audienc non technic audienc get good handl ai even describ suppos want develop algorithm display list relat articl two way explain algorithm work layman anthropomorph algorithm read articl websit display articl similar articl look technic algorithm convert articl bag word compar bag word articl determin articl share common word articl share word bag one display user obvious may technic correct detail implement algorithm make easier someon understand fix algorithm produc output disagre heavili readabl eleg easier understand provid general sens algorithm instead algorithm abstract away implement detail comput read articl focus use algorithm real world scenario therefor prefer use anthropomorph languag emphas statement p answer depend audienc speak non technic audienc might prefer technic audienc may prefer let know well,philosophy,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,game-ai,image-recognition,natural-language,philosophy,training,tensorflow,classification,genetic-algorithms"
778,"<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p>

<p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE which uses CD as representation for the story to generate.</p>
",roger schank interesting work language processing conceptual dependency  cd       moved somewhat field  education days  useful applications natural language generation  babel   story generation  tailspin  areas  often involving planning episodes rather individual sentences   anybody else continued use cd variants thereof  aware projects  apart hovy pauline uses cd representation story generate  ,roger schank interest work languag process conceptu depend cd move somewhat field educ day use applic natur languag generat babel stori generat tailspin area often involv plan episod rather individu sentenc anybodi els continu use cd variant thereof awar project apart hovi paulin use cd represent stori generat,"nlp,knowledge-representation","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,game-ai,natural-language,algorithm,training,tensorflow,image-recognition,nlp,classification,genetic-algorithms"
779,"<p>I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing.</p>
",wanting get started learning artificial intelligence know almost nothing coding anything  question  would best way get started learning artificial intelligence  learn kind coding language kind concept need know getting started  kind looking best way get started literally know nothing  ,want get start learn artifici intellig know almost noth code anyth question would best way get start learn artifici intellig learn kind code languag kind concept need know get start kind look best way get start liter know noth,research,"machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,algorithm,tensorflow,classification,training,game-ai,philosophy,genetic-algorithms,image-recognition,natural-language"
787,"<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>

<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>
",studying local search algorithms greedy hill climbing  stochastic hill climbing  simulated annealing etc  noticed methods take little memory compared systematic search techniques   local search algorithms make use memory give significantly better answers algorithms use little memory  crossing local maxima   also  way combine local search systematic search algorithms get best worlds  ,studi local search algorithm greedi hill climb stochast hill climb simul anneal etc notic method take littl memori compar systemat search techniqu local search algorithm make use memori give signific better answer algorithm use littl memori cross local maxima also way combin local search systemat search algorithm get best world,search,"machine-learning,neural-networks,deep-learning,algorithm,reinforcement-learning,ai-design,convolutional-neural-networks,game-ai,genetic-algorithms,training,image-recognition,classification,tensorflow,nlp,backpropagation"
792,"<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p>

<p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p>

<p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p>

<p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p>

<p>because : </p>

<ol>
<li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li>
<li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li>
<li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li>
</ol>
",know every program positive negative points  know maybe  net programming languages best ai programming   prefer  net programming languages experiences would like know ai program one better  c c   c  vb    one languages faster stable running different queries self learning    make summary  think c   best ai programming  net also c  used projects  python recommended others option view         complex language every single move need find library import project  library date working new released python versions  people say easy language learn use    start create library  language could hardest language world    create program using library every single option project  like lego game  sure  think cheap programming language find good program created language    ,know everi program posit negat point know mayb net program languag best ai program prefer net program languag experi would like know ai program one better c c c vb one languag faster stabl run differ queri self learn make summari think c best ai program net also c use project python recommend option view complex languag everi singl move need find librari import project librari date work new releas python version peopl say easi languag learn use start creat librari languag could hardest languag world creat program use librari everi singl option project like lego game sure think cheap program languag find good program creat languag,intelligent-agent,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,algorithm,natural-language,tensorflow,training,image-recognition,genetic-algorithms,philosophy,nlp"
793,"<p>When I visit this site, I find the word ""search"" appears quite often. </p>

<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>
",visit site  find word  search  appears quite often    important  kinds search algorithms used artificial intelligence   improve result ai  ,visit site find word search appear quit often import kind search algorithm use artifici intellig improv result ai,search,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,algorithm,philosophy,game-ai,convolutional-neural-networks,tensorflow,strong-ai,natural-language,training,classification,genetic-algorithms"
801,"<p>Considering the answers of <a href=""https://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain"">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>

<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>

<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>

<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>

<p>According to <a href=""https://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist"">this</a> post, ""software rewirable"" analogous circuits, essentially ""analogous FPGAs"" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=""https://en.wikipedia.org/wiki/Application-specific_integrated_circuit"" rel=""nofollow noreferrer"">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>

<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>

<p>Could this idea work? Maybe there is even active research/development into this direction?</p>
",considering answers question  emulating human brain current computing capacity currently impossible  far   note      decades ago  similar calculations similar results   clock frequency modern cpus seem stopped  currently miniaturization     mobile use   ram cache improvement multi core paralellization main lines development   ok  case analogous chips  case nn  big problem  accurate  nn would adapt minor manufacturing differences learning phase  single analogous wire substitute complex integer multiplication division unit  whole surface analogous printed circuit could work parallel   according post   software rewirable  analogous circuits  essentially  analogous fpgas  already exist  although capacity fpgas highly capacity asics size  maybe analogous chips neural networks could also exist   suspect  correct  maybe even real human brain model far  would still require massively parallel system costly analogous nn chips  seems impossible   could idea work  maybe even active research development direction  ,consid answer question emul human brain current comput capac current imposs far note decad ago similar calcul similar result clock frequenc modern cpus seem stop current miniatur mobil use ram cach improv multi core paralel main line develop ok case analog chip case nn big problem accur nn would adapt minor manufactur differ learn phase singl analog wire substitut complex integ multipl divis unit whole surfac analog print circuit could work parallel accord post softwar rewir analog circuit essenti analog fpgas alreadi exist although capac fpgas high capac asic size mayb analog chip neural network could also exist suspect correct mayb even real human brain model far would still requir massiv parallel system cost analog nn chip seem imposs could idea work mayb even activ research develop direct,neural-networks,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,game-ai,algorithm,tensorflow,image-recognition,training,classification,philosophy,nlp,genetic-algorithms"
809,"<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p>

<p>They're certainly efficient at <a href=""https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html"" rel=""nofollow"">compressing images</a>.</p>

<p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p>

<p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p>

<p>Could some kind of lossy <a href=""https://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow"">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p>
",conceptually speaking  artificial neural networks highly distributed  lossy compression schemes   certainly efficient compressing images   brains  least  neocortex  compartmentalized  highly distributed  lossy databases    salient features rnns cnns necessary given lossy compression scheme order extract semantic relations  matter large number dimensions variables    could kind lossy bloom filter purposed kinds problems anns applied  ,conceptu speak artifici neural network high distribut lossi compress scheme certain effici compress imag brain least neocortex compartment high distribut lossi databas salient featur rnns cnns necessari given lossi compress scheme order extract semant relat matter larg number dimens variabl could kind lossi bloom filter purpos kind problem ann appli,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,classification,ai-design,algorithm,tensorflow,training,game-ai,computer-vision,genetic-algorithms,deep-network"
811,"<p>Consciousness <a href=""http://www.iep.utm.edu/consciou/"" rel=""nofollow noreferrer"">is challenging to define</a>, but for this question let's define it as ""actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine."" Humans, of course, have minds; for normal computers, all the things they ""see"" are just more data. One could alternatively say that humans are <a href=""https://philosophy.stackexchange.com/a/4687"">sentient</a>, while traditional computers are not.</p>

<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>
",consciousness challenging define  question let define  actually experiencing sensory input opposed putting bunch data inanimate machine   humans  course  minds  normal computers  things  see  data  one could alternatively say humans sentient  traditional computers   setting aside question whether possible build sentient machine  actually make difference ai sentient  words  tasks made impossible   difficult   lack sentience  ,conscious challeng defin question let defin actual experienc sensori input oppos put bunch data inanim machin human cours mind normal comput thing see data one could altern say human sentient tradit comput set asid question whether possibl build sentient machin actual make differ ai sentient word task made imposs difficult lack sentienc,philosophy,"machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,philosophy,algorithm,game-ai,image-recognition,training,classification,tensorflow,computer-vision,nlp"
817,"<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). 
From a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. </p>

<p>Is anyone of note still working on GOFAI?    </p>
",lot textbooks introductory lectures typically split ai connectionism gofai  good old fashioned ai    purely technical perspective seems connectionism grown machine learning data science  nobody talks gofai  symbolic ai expert systems    anyone note still working gofai      ,lot textbook introductori lectur typic split ai connection gofai good old fashion ai pure technic perspect seem connection grown machin learn data scienc nobodi talk gofai symbol ai expert system anyon note still work gofai,"gofai,symbolic-computing","machine-learning,neural-networks,ai-design,deep-learning,reinforcement-learning,game-ai,philosophy,convolutional-neural-networks,strong-ai,training,algorithm,tensorflow,research,image-recognition,genetic-algorithms"
820,"<p>I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)</p>

<p>The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.</p>

<p>Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.</p>

<p>Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)</p>

<p>PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management).</p>
",recently finished course rl david silver  yt  thought trying simple application unity game engine  built simple labyrint ball want teach ball get point point b avoiding obstacles fire  place get burnt big negative reward   problem encountered designing whole thing  programming wise   correct  least good  way representing position  space  continuous thought representing feature vector consisting    left  right  posx  posy  direction whether pressing button moving direction binary  actions want  pos floats       representing normalized position one corner plane whole map  would accompanied vector w would represent weights adjusted using gradient descent   question  work   asking   reasons  one sure posx posy since   multiply weights vector could resulting reward anything    second reason sure actions part features  mean  makes sense could easily wrong since beginner   thanks lot guys advance  questions think problem described deeply enough ask comments edit question      ps  could code way think right  also want get gasp designing applications paper coding  project management   ,recent finish cours rl david silver yt thought tri simpl applic uniti game engin built simpl labyrint ball want teach ball get point point b avoid obstacl fire place get burnt big negat reward problem encount design whole thing program wise correct least good way repres posit space continu thought repres featur vector consist left right posx posi direct whether press button move direct binari action want pos float repres normal posit one corner plane whole map would accompani vector w would repres weight adjust use gradient descent question work ask reason one sure posx posi sinc multipli weight vector could result reward anyth second reason sure action part featur mean make sens could easili wrong sinc beginn thank lot guy advanc question think problem describ deepli enough ask comment edit question ps could code way think right also want get gasp design applic paper code project manag,"reinforcement-learning,models,implementation,q-learning","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,ai-design,algorithm,game-ai,tensorflow,image-recognition,classification,training,q-learning,genetic-algorithms,backpropagation"
824,"<p>Currently I work as a java developer, But very much interested in learning Artificial Intelligence.
Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.
Is there any special technologies i have to learn or something else.</p>
",currently work java developer  much interested learning artificial intelligence  anybody tell steps follow learn artificial intelligence considering fact new  special technologies learn something else  ,current work java develop much interest learn artifici intellig anybodi tell step follow learn artifici intellig consid fact new special technolog learn someth els,"machine-learning,self-learning,learning-algorithms,ultraintelligent-machine","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,philosophy,tensorflow,q-learning,self-learning,algorithm,game-ai,classification,genetic-algorithms,training"
825,"<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p>
",self recognition seems item designers trying integrate artificial intelligence  generally recognized method machine  would one test capacity   turing test  ,self recognit seem item design tri integr artifici intellig general recogn method machin would one test capac ture test,intelligence-testing,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,philosophy,reinforcement-learning,training,algorithm,classification,image-recognition,tensorflow,game-ai,nlp,research"
833,"<p>I know that deepmind used deep Q learning (<a href=""https://deepmind.com/research/dqn/"" rel=""nofollow"">DQN</a>) for its Atari game AI. It used a <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">conv neural network</a> (CNN) to approximate <code>Q(s,a)</code> from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?</p>

<p>Here are the steps that I believe are happening inside DQN:</p>

<blockquote>
  <p>1) A game picture (a state) is send to CNN as input value</p>
  
  <p>2) CNN predicts an output as action (eg:left, right, shoot, etc)</p>
  
  <p>3) Simulator applies the predicted action and moves to new game state</p>
  
  <p>4) repeat step 1</p>
</blockquote>

<p>The problem with my above logic is in <strong>step 2</strong>. CNN is used for predicting an action, but when is CNN trained for prediction? </p>

<p>I would prefer if you used less math for explanation.</p>

<p>EDIT</p>

<p>I want to add some more questions regarding the same topic</p>

<p>1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?</p>

<p>2) How many output the neural network has and how action is determined from those outputs?</p>
",know deepmind used deep q learning  dqn  atari game ai  used conv neural network  cnn  approximate q  pixels instead q table  want know dqn converted input action  many output cnn  train neural network prediction   steps believe happening inside dqn         game picture  state  send cnn input value         cnn predicts output action  eg left  right  shoot  etc          simulator applies predicted action moves new game state         repeat step     problem logic step    cnn used predicting action  cnn trained prediction    would prefer used less math explanation   edit  want add questions regarding topic     reward passed neural network  neural network knows whether output action obtained positive negative reward      many output neural network action determined outputs  ,know deepmind use deep q learn dqn atari game ai use conv neural network cnn approxim q pixel instead q tabl want know dqn convert input action mani output cnn train neural network predict step believ happen insid dqn game pictur state send cnn input valu cnn predict output action eg left right shoot etc simul appli predict action move new game state repeat step problem logic step cnn use predict action cnn train predict would prefer use less math explan edit want add question regard topic reward pass neural network neural network know whether output action obtain posit negat reward mani output neural network action determin output,"deep-network,deep-learning,convolutional-neural-networks,gaming","neural-networks,machine-learning,deep-learning,reinforcement-learning,convolutional-neural-networks,tensorflow,q-learning,ai-design,game-ai,training,classification,algorithm,backpropagation,image-recognition,keras"
834,"<p>In the lecture, there was a statement:</p>

<blockquote>
  <p>""Recurrent neural networks with multiple hidden layers are just a
  special case that has some of the hidden to hidden connections
  missing.""</p>
</blockquote>

<p>I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?</p>
",lecture  statement       recurrent neural networks multiple hidden layers   special case hidden hidden connections   missing     understand recurrent means connections previous layer layer well  visualization available easily understand statement  ,lectur statement recurr neural network multipl hidden layer special case hidden hidden connect miss understand recurr mean connect previous layer layer well visual avail easili understand statement,"hidden-layers,recurrent-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,backpropagation,tensorflow,reinforcement-learning,algorithm,classification,recurrent-neural-networks,game-ai,image-recognition,ai-design,genetic-algorithms,deep-network"
835,"<p>Inattentional Blindness is common in humans (see: <a href=""https://en.wikipedia.org/wiki/Inattentional_blindness"" rel=""nofollow"">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>
",inattentional blindness common humans  see  https   en wikipedia org wiki inattentional blindness    could also common machines built artificial vision  ,inattent blind common human see https en wikipedia org wiki inattent blind could also common machin built artifici vision,image-recognition,"machine-learning,neural-networks,deep-learning,ai-design,philosophy,reinforcement-learning,game-ai,image-recognition,algorithm,convolutional-neural-networks,nlp,singularity,research,game-theory,ultraintelligent-machine"
836,"<p>My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.</p>

<p>I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.</p>

<p><strong>Question:</strong> Are neural networks <a href=""http://mathworld.wolfram.com/LipschitzFunction.html"" rel=""nofollow"">Lipschitz</a> functions?</p>
",question regarding standard dense connected feed forward neural networks sigmoidal activation   studying bayesian optimization hyper parameter selection neural networks  doubt effective method  wan delve little deeper maths   question  neural networks lipschitz functions  ,question regard standard dens connect feed forward neural network sigmoid activ studi bayesian optim hyper paramet select neural network doubt effect method wan delv littl deeper math question neural network lipschitz function,"neural-networks,optimization,math","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,training,ai-design,classification,tensorflow,backpropagation,game-ai,recurrent-neural-networks,genetic-algorithms,image-recognition,deep-network"
840,"<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>
",one actually kill machine  problems defining life  also problems defining death  also true artificial life artificial intelligence  ,one actual kill machin problem defin life also problem defin death also true artifici life artifici intellig,"philosophy,death","machine-learning,neural-networks,deep-learning,ai-design,philosophy,algorithm,convolutional-neural-networks,reinforcement-learning,genetic-algorithms,image-recognition,classification,definitions,game-ai,tensorflow,training"
841,"<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p>
",generally  people classified aggressive  type  passive  could programming ai systems cause aggressive passive behavior ais  ,general peopl classifi aggress type passiv could program ai system caus aggress passiv behavior ai,"philosophy,emotional-intelligence,human-like","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,algorithm,strong-ai,classification,reinforcement-learning,convolutional-neural-networks,image-recognition,research,natural-language,genetic-algorithms"
849,"<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p>
",assuming mankind eventually create artificial humans  put equal effort humans relate artificial human  expect return  happening real time place ai trucks cars road  people right question  maybe court  ai machine breaks law  ,assum mankind eventu creat artifici human put equal effort human relat artifici human expect return happen real time place ai truck car road peopl right question mayb court ai machin break law,"philosophy,legal","machine-learning,neural-networks,ai-design,philosophy,deep-learning,game-ai,reinforcement-learning,algorithm,image-recognition,strong-ai,convolutional-neural-networks,game-theory,research,genetic-algorithms,self-learning"
850,"<p>AI death is still unclear a concept, as it may take several forms and allow for ""coming back from the dead"". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p>

<p>""Somehow forbid"" is the topic of this question. There will probably be rules, like ""AI social laws"", that can conclude an AI should ""die"" or ""be sentenced to the absence of progress"" (a jail). Then who or what could manage that AI's state?</p>
",ai death still unclear concept  may take several forms allow  coming back dead   example  ai could somehow forbidden anything  permission execute   infringed laws    somehow forbid  topic question  probably rules  like  ai social laws   conclude ai  die   sentenced absence progress   jail   could manage ai state  ,ai death still unclear concept may take sever form allow come back dead exampl ai could somehow forbidden anyth permiss execut infring law somehow forbid topic question probabl rule like ai social law conclud ai die sentenc absenc progress jail could manag ai state,death,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,strong-ai,algorithm,natural-language,research,convolutional-neural-networks,game-theory,genetic-algorithms,image-recognition"
855,"<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=""http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg"" rel=""nofollow noreferrer"">ducks on the road</a>?</p>

<p><a href=""https://i.stack.imgur.com/a0PVLm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a0PVLm.jpg"" alt=""ducks on the road""></a></p>
",self driving cars deal snow  heavy rain  weather conditions like  deal unusual events  ducks road    ,self drive car deal snow heavi rain weather condit like deal unusu event duck road,"self-driving,cars","neural-networks,machine-learning,deep-learning,algorithm,ai-design,training,image-recognition,convolutional-neural-networks,self-driving,reinforcement-learning,backpropagation,classification,philosophy,tensorflow,game-ai"
862,"<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>
",people trying answer question neural network  however  anyone came thoughts make neural network ask questions  instead answer questions  example  cnn decide category object belongs  ask question help classification  ,peopl tri answer question neural network howev anyon came thought make neural network ask question instead answer question exampl cnn decid categori object belong ask question help classif,"neural-networks,deep-learning","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,classification,image-recognition,nlp,training,game-ai,tensorflow,algorithm,genetic-algorithms,backpropagation"
864,"<p>The Mars Exploration Rover (MER) <em><a href=""http://www.nasa.gov/mp4/618340main_mer20120124-320-jpl.mp4"" rel=""nofollow"">Opportunity</a></em> landed on Mars on January 25, 2004. The rover was originally designed for a 90 <strong>Sol mission</strong> (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.</p>

<p>How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?</p>
",mars exploration rover  mer  opportunity landed mars january           rover originally designed    sol mission  sol  one martian day  slightly longer earth day    hours    minutes   mission extended several times  machine still trekking    years red planet   working    years  anyone please explain smart rover  ai concepts behind  ,mar explor rover mer opportun land mar januari rover origin design sol mission sol one martian day slight longer earth day hour minut mission extend sever time machin still trekk year red planet work year anyon pleas explain smart rover ai concept behind,"control-problem,robotics,nasa","machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,philosophy,reinforcement-learning,game-ai,algorithm,image-recognition,training,tensorflow,research,strong-ai,genetic-algorithms"
870,"<p>I have used OpenCV to train Haar cascades to detect face and other patterns. However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results. But OpenCV doesn't have a good documentation of how to train hogs, I have googled a bit and found results that includes SVM and others.</p>

<p>OpenCV also has versioning problem where they move certain classes or functions somewhere else.</p>

<p>Are there any other techniques/method that I can use to train and detect objects and patterns? Preferably with proper documentation and basic tutorial/examples. Language preference: C#, Java, C++, Python</p>
",used opencv train haar cascades detect face patterns  however later realized haar tends give lot false positives learned hog would give accurate results  opencv good documentation train hogs  googled bit found results includes svm others   opencv also versioning problem move certain classes functions somewhere else   techniques method use train detect objects patterns  preferably proper documentation basic tutorial examples  language preference  c   java  c    python ,use opencv train haar cascad detect face pattern howev later realiz haar tend give lot fals posit learn hog would give accur result opencv good document train hog googl bit found result includ svm opencv also version problem move certain class function somewher els techniqu method use train detect object pattern prefer proper document basic tutori exampl languag prefer c java c python,"object-recognition,detecting-patterns","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,tensorflow,classification,algorithm,image-recognition,game-ai,keras,computer-vision,q-learning"
872,"<p>Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! </p>
",future robots machines going use stack exchange communities teach  ongoing projects  imagine bot memory q amp communities   ,futur robot machin go use stack exchang communiti teach ongo project imagin bot memori q amp communiti,"ai-design,self-learning,knowledge-representation","machine-learning,neural-networks,deep-learning,reinforcement-learning,q-learning,ai-design,tensorflow,convolutional-neural-networks,game-ai,algorithm,training,image-recognition,philosophy,nlp,classification"
873,"<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p>
",mankind create machines work  could also create  passion  within machines better work using artificial intelligence  would passion cause machine better job  could measure quantity quality passion comparing outputs machine    machines passion  without  ,mankind creat machin work could also creat passion within machin better work use artifici intellig would passion caus machin better job could measur quantiti qualiti passion compar output machin machin passion without,"philosophy,emotional-intelligence","machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,algorithm,philosophy,image-recognition,game-ai,training,classification,definitions,genetic-algorithms,singularity"
874,"<p>Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?</p>
",ai systems created could recognize  recognize intelligence systems  make intelligent decisions systems  mankind seems making progress self recognition seen evidence one system recognizing systems able compare intelligence systems  could accomplished  ,ai system creat could recogn recogn intellig system make intellig decis system mankind seem make progress self recognit seen evid one system recogn system abl compar intellig system could accomplish,philosophy,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,algorithm,reinforcement-learning,image-recognition,game-ai,training,convolutional-neural-networks,strong-ai,natural-language,research,computer-vision"
877,"<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p>

<p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>
",iq used measure intelligence machines  humans  point time would iq intelligent ai systems  iq  best compare intelligence machine  one machine another    question asking measure iq machine  iq preferred  general  method measuring intelligence artificial intelligence compare accepted method measuring intelligence humans  many people may understand relevance turing test intelligent new car  types intelligent machines  ,iq use measur intellig machin human point time would iq intellig ai system iq best compar intellig machin one machin anoth question ask measur iq machin iq prefer general method measur intellig artifici intellig compar accept method measur intellig human mani peopl may understand relev ture test intellig new car type intellig machin,philosophy,"machine-learning,neural-networks,deep-learning,philosophy,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,game-ai,definitions,strong-ai,image-recognition,singularity,training,classification"
878,"<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>

<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>

<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>

<p>Thanks in advance for any ideas.</p>

<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>
",think ais would work  realised think way ai could taught language  child tends learn language associations language pictures object  e g  people saying word dog around dog  later realising  people say dog car learn means      however  text based ai use method learn  access sort input device   way could come programming every word  rule  english language  whatever language meant  speak    however would  potentially  take years   anyone ideas could done  done already    thanks advance ideas   btw  context  using ai mean artificial intelligence system near human intelligence  prior knowledge language  ,think ai would work realis think way ai could taught languag child tend learn languag associ languag pictur object e g peopl say word dog around dog later realis peopl say dog car learn mean howev text base ai use method learn access sort input devic way could come program everi word rule english languag whatev languag meant speak howev would potenti take year anyon idea could done done alreadi thank advanc idea btw context use ai mean artifici intellig system near human intellig prior knowledg languag,natural-language,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,natural-language,game-ai,philosophy,algorithm,image-recognition,training,nlp,genetic-algorithms,tensorflow"
884,"<ul>
<li>Would AI be a self-propogating iteration in which the previous AI is
destroyed by a more optimised AI child?  </li>
<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>
</ul>
", would ai self propogating iteration previous ai destroyed optimised ai child    would ai branches ai warning create new ai   ,would ai self propog iter previous ai destroy optimis ai child would ai branch ai warn creat new ai,ai-design,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,strong-ai,game-theory,reinforcement-learning,algorithm,research,genetic-algorithms,training,natural-language,convolutional-neural-networks"
886,"<blockquote>
  <p>Shortly about <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer""><strong>deep learning</strong> (for reference)</a>:</p>
  
  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by
  using a deep graph with multiple processing layers, composed of
  multiple linear and non-linear transformations.</em></p>
  
  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent
  neural networks have been applied to fields like computer vision,
  automatic speech recognition, natural language processing, audio
  recognition and bioinformatics where they have been shown to produce
  state-of-the-art results on various tasks.</em></p>
</blockquote>

<hr>

<p><strong>My question:</strong></p>

<p>Can <a href=""https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures"" rel=""noreferrer"">deep neural networks</a> or <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""noreferrer"">convolutional deep neural networks</a> be viewed as <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""noreferrer"">ensemble-based</a> method of machine learning? Or it is different approaches?</p>
",   shortly deep learning  reference        deep learning branch machine learning based set algorithms attempt model high level abstractions data   using deep graph multiple processing layers  composed   multiple linear non linear transformations       various deep learning architectures deep neural networks  convolutional deep neural networks  deep belief networks recurrent   neural networks applied fields like computer vision    automatic speech recognition  natural language processing  audio   recognition bioinformatics shown produce   state art results various tasks      question   deep neural networks convolutional deep neural networks viewed ensemble based method machine learning  different approaches  ,short deep learn refer deep learn branch machin learn base set algorithm attempt model high level abstract data use deep graph multipl process layer compos multipl linear non linear transform various deep learn architectur deep neural network convolut deep neural network deep belief network recurr neural network appli field like comput vision automat speech recognit natur languag process audio recognit bioinformat shown produc state art result various task question deep neural network convolut deep neural network view ensembl base method machin learn differ approach,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,classification,training,game-ai,image-recognition,backpropagation,deep-network,genetic-algorithms,algorithm"
890,"<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p>
",convolutional neural networks summarily better pattern recognition existing image processing libraries use cnn  still hard outstanding problems image processing seem beyond capability  ,convolut neural network summarili better pattern recognit exist imag process librari use cnn still hard outstand problem imag process seem beyond capabl,"image-recognition,comparison,convolutional-neural-networks","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,reinforcement-learning,classification,training,ai-design,tensorflow,game-ai,computer-vision,algorithm,keras,genetic-algorithms"
893,"<p>I have been messing around in <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""noreferrer"">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p>
",messing around tensorflow playground  one input data sets spiral  matter input parameters choose  matter wide deep neural network make  cannot fit spiral  data scientists fit data shape  ,mess around tensorflow playground one input data set spiral matter input paramet choos matter wide deep neural network make fit spiral data scientist fit data shape,"neural-networks,classification,tensorflow","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,tensorflow,ai-design,classification,training,reinforcement-learning,keras,backpropagation,algorithm,genetic-algorithms,game-ai,image-recognition"
895,"<p>A ""general intelligence"" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The ""AGI"" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of ""teaching"" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>

<p>Contrast that to a ""narrow intelligence"". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>

<p>A ""general intelligence"" seems to be more flexible than a ""narrow intelligence"". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>

<p>A ""narrow intelligence"" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time ""learning"" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>

<p>Is my ""thinking"" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>
", general intelligence  may capable learning lot different things  possessing capability equal actually   agi  must learn   learning process take time  want agi drive car play go  find way  teaching   keep mind never built agis  know long training process  would safe assume pessimistic estimates   contrast  narrow intelligence   narrow ai already knows drive car play go  programmed excellent one specific task  need worry training machine  already pre trained    general intelligence  seems flexible  narrow intelligence   could buy agi drive car play go  willing training  even teach new trick  bake cake  worry unexpected tasks coming  since agi eventually figure  given enough training time  would wait long time though    narrow intelligence  appears efficient assigned task  due programmed specifically task  knows exactly  waste time  learning   unlike agi buddy   instead buying one agi handle bunch different tasks poorly  would rather buy bunch specialized narrow ais  narrow ai    drives cars  narrow ai    plays go  narrow ai    bake cakes  etc  said  brittle approach  since unexpected task comes  none narrow ais would able handle  willing accept risk though    thinking  correct  trade flexibility  agi  efficiency  narrow ai   like described  theoretically possible agi flexible efficient  ,general intellig may capabl learn lot differ thing possess capabl equal actual agi must learn learn process take time want agi drive car play go find way teach keep mind never built agi know long train process would safe assum pessimist estim contrast narrow intellig narrow ai alreadi know drive car play go program excel one specif task need worri train machin alreadi pre train general intellig seem flexibl narrow intellig could buy agi drive car play go train even teach new trick bake cake worri unexpect task come sinc agi eventu figur given enough train time would wait long time though narrow intellig appear effici assign task due program specif task know exact wast time learn unlik agi buddi instead buy one agi handl bunch differ task poor would rather buy bunch special narrow ai narrow ai drive car narrow ai play go narrow ai bake cake etc said brittl approach sinc unexpect task come none narrow ai would abl handl accept risk though think correct trade flexibl agi effici narrow ai like describ theoret possibl agi flexibl effici,"ai-design,strong-ai,weak-ai","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-ai,philosophy,training,tensorflow,algorithm,image-recognition,classification,genetic-algorithms,strong-ai"
900,"<p>Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?</p>
",neural network nn  system architecture used storing retrieving information  example  store whole avatar movie hd format inside neural network retrieve without loss  neural network needed  searched web came across lstm rnn understanding lstm stores pattern content  nn exist explain  ,neural network nn system architectur use store retriev inform exampl store whole avatar movi hd format insid neural network retriev without loss neural network need search web came across lstm rnn understand lstm store pattern content nn exist explain,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,training,classification,image-recognition,recurrent-neural-networks,game-ai,backpropagation,genetic-algorithms,algorithm"
901,"<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=""http://image-net.org/challenges/LSVRC/2015/results"" rel=""nofollow noreferrer"">""Large Scale Visual Recognition Challenge 2015"" (ILSVRC2015)</a> in all five main tracks:</p>

<blockquote>
  <ul>
  <li><em>ImageNet Classification: ?Ultra-deep? (quote Yann) 152-layer nets</em> </li>
  <li><em>ImageNet Detection: 16% better than 2nd</em></li>
  <li><em>ImageNet Localization: 27% better than 2nd</em></li>
  <li><em>COCO Detection: 11% better than 2nd</em></li>
  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em>
  <em>Source:</em> <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow noreferrer""><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li>
  </ul>
</blockquote>

<p>This work is described in the following article:</p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1512.03385"" rel=""nofollow noreferrer""><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p>
</blockquote>

<hr>

<p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p>

<blockquote>
  <p><a href=""https://arxiv.org/pdf/1603.05027.pdf"" rel=""nofollow noreferrer"">""<em>Identity Mappings in Deep Residual Networks (2016)</em>""</a></p>
</blockquote>

<p>state that <strong>depth</strong> plays a key role:</p>

<blockquote>
  <p><em>""<strong>We obtain these results via a simple but essential concept ? going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>""</em></p>
</blockquote>

<p>It is emphasized in their <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""nofollow noreferrer"">presentation</a> also (deeper - better):<br> </p>

<blockquote>
  <p><em>- ""A deeper model should not have higher training error.""<br> 
  - ""Deeper ResNets have lower training error, and also lower test error.""<br> 
  - ""Deeper ResNets have lower error.""<br>
  - ""All benefit more from deeper features ? cumulative gains!""<br>
  - ""Deeper is still better.""</em></p>
</blockquote>

<p>Here is the sctructure of 34-layer residual (for reference):
<a href=""https://i.stack.imgur.com/L8m0X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L8m0X.png"" alt=""enter image description here""></a></p>

<hr>

<p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p>

<blockquote>
  <p><a href=""https://arxiv.org/abs/1605.06431"" rel=""nofollow noreferrer""><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p>
</blockquote>

<p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. 
There is a picture in the article. I attach it with explanation:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/PGhK2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PGhK2.jpg"" alt=""enter image description here""></a> Residual Networks are
  conventionally shown as (a), which is a natural representation of
  Equation (1). When we expand this formulation to Equation (6), we
  obtain an unraveled view of a 3-block residual network (b). From this
  view, it is apparent that residual networks have O(2^n) implicit paths
  connecting input and output and that adding a block doubles the number
  of paths.</p>
</blockquote>

<p>In conclusion of the article it is stated:</p>

<blockquote>
  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.
  Residual networks push the limits of network multiplicity, not network
  depth. Our proposed unraveled view and the lesion study show that
  residual networks are an implicit ensemble of exponentially many
  networks. If most of the paths that contribute gradient are very short
  compared to the overall depth of the network, <strong>increased depth</strong>
  alone <strong>can?t be the key characteristic</strong> of residual networks. We now
  believe that <strong>multiplicity</strong>, the network?s expressability in the
  terms of the number of paths, plays <strong>a key role</strong>.</p>
</blockquote>

<p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p>

<hr>

<p><strong>My question:</strong><br>
Should we think of deep ResNets as ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?</p>
",question architecture deep residual networks  resnets   model   st places  large scale visual recognition challenge        ilsvrc      five main tracks         imagenet classification   ultra deep   quote yann      layer nets    imagenet detection      better  nd   imagenet localization      better  nd   coco detection      better  nd   coco segmentation      better  nd   source  msra   ilsvrc  amp  coco      competitions  presentation    nd slide       work described following article      deep residual learning image recognition        pdf      microsoft research team  developers resnets  kaiming  xiangyu zhang  shaoqing ren  jian sun  article       identity mappings deep residual networks           state depth plays key role       obtain results via simple essential concept   going deeper  results demonstrate potential pushing limits depth     emphasized presentation also  deeper   better           deeper model higher training error         deeper resnets lower training error  also lower test error         deeper resnets lower error        benefit deeper features   cumulative gains        deeper still better     sctructure    layer residual  reference       recently found one theory introduces novel interpretation residual networks showing exponential ensembles      residual networks exponential ensembles relatively shallow networks          deep resnets described many shallow networks whose outputs pooled various depths   picture article  attach explanation       residual networks   conventionally shown    natural representation   equation      expand formulation equation        obtain unraveled view   block residual network  b     view  apparent residual networks   n  implicit paths   connecting input output adding block doubles number   paths    conclusion article stated      depth  ensemble makes residual networks strong    residual networks push limits network multiplicity  network   depth  proposed unraveled view lesion study show   residual networks implicit ensemble exponentially many   networks  paths contribute gradient short   compared overall depth network  increased depth   alone key characteristic residual networks    believe multiplicity  network expressability   terms number paths  plays key role    recent theory confirmed refuted  happens sometimes theories refuted articles withdrawn     question  think deep resnets ensemble  ensemble depth makes residual networks strong  possible even developers quite perceive model represent key concept  ,question architectur deep residu network resnet model st place larg scale visual recognit challeng ilsvrc five main track imagenet classif ultra deep quot yann layer net imagenet detect better nd imagenet local better nd coco detect better nd coco segment better nd sourc msra ilsvrc amp coco competit present nd slide work describ follow articl deep residu learn imag recognit pdf microsoft research team develop resnet kaim xiangyu zhang shaoq ren jian sun articl ident map deep residu network state depth play key role obtain result via simpl essenti concept go deeper result demonstr potenti push limit depth emphas present also deeper better deeper model higher train error deeper resnet lower train error also lower test error deeper resnet lower error benefit deeper featur cumul gain deeper still better sctructur layer residu refer recent found one theori introduc novel interpret residu network show exponenti ensembl residu network exponenti ensembl relat shallow network deep resnet describ mani shallow network whose output pool various depth pictur articl attach explan residu network convent shown natur represent equat expand formul equat obtain unravel view block residu network b view appar residu network n implicit path connect input output ad block doubl number path conclus articl state depth ensembl make residu network strong residu network push limit network multipl network depth propos unravel view lesion studi show residu network implicit ensembl exponenti mani network path contribut gradient short compar overal depth network increas depth alon key characterist residu network believ multipl network express term number path play key role recent theori confirm refut happen sometim theori refut articl withdrawn question think deep resnet ensembl ensembl depth make residu network strong possibl even develop quit perceiv model repres key concept,"neural-networks,machine-learning,deep-network,deep-learning,pattern-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,tensorflow,ai-design,training,classification,image-recognition,game-ai,backpropagation,keras,genetic-algorithms,recurrent-neural-networks"
904,"<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p>
",attempt trying learn neural network machine learning trying create simple neural network trained recognise one word given string  contains one word   effect one feed string containing trained word spelled wrong network would able still recognise word  anybody help pseudo code start code  general explanation read like   articles   example projects still clue ,attempt tri learn neural network machin learn tri creat simpl neural network train recognis one word given string contain one word effect one feed string contain train word spell wrong network would abl still recognis word anybodi help pseudo code start code general explan read like articl exampl project still clue,"neural-networks,machine-learning","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,classification,training,algorithm,image-recognition,game-ai,natural-language,recurrent-neural-networks,backpropagation"
907,"<p>In 2004 <a href=""https://en.wikipedia.org/wiki/Jeff_Hawkins"">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=""https://en.wikipedia.org/wiki/On_Intelligence"">On Intelligence</a>, in which he details a theory how the human neocortex works. </p>

<p>This theory is called <a href=""https://en.wikipedia.org/wiki/Memory-prediction_framework"">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=""http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full"">in this paper</a>).</p>

<p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p>

<p>Hawkins founded <a href=""https://en.wikipedia.org/wiki/Numenta"">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. </p>

<p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? </p>
",     jeff hawkins  inventor palm pilot  published interesting book called intelligence  details theory human neocortex works    theory called memory prediction framework striking features  example bottom  feedforward   also top information processing ability make simultaneous  discrete predictions different future scenarios  described paper    promise memory prediction framework unsupervised generation stable high level representations future possibilities  something would revolutionise probably whole bunch ai research areas   hawkins founded company proceeded implement ideas  unfortunately ten years later promise ideas still unfulfilled  far implementation used anomaly detection  kind opposite really want  instead extracting understanding  extract instances artificial cortex understand    question way hawkins framework falls short  concrete conceptual problems far prevent theory working practice   ,jeff hawkin inventor palm pilot publish interest book call intellig detail theori human neocortex work theori call memori predict framework strike featur exampl bottom feedforward also top inform process abil make simultan discret predict differ futur scenario describ paper promis memori predict framework unsupervis generat stabl high level represent futur possibl someth would revolutionis probabl whole bunch ai research area hawkin found compani proceed implement idea unfortun ten year later promis idea still unfulfil far implement use anomali detect kind opposit realli want instead extract understand extract instanc artifici cortex understand question way hawkin framework fall short concret conceptu problem far prevent theori work practic,unsupervised-learning,"neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,algorithm,game-ai,philosophy,image-recognition,classification,tensorflow,training,game-theory,genetic-algorithms"
909,"<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>

<p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>

<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>

<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p>

<p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>
",far tell  neural networks fixed number neurons input layer   neural networks used context like example nlp  sentences blocks text varying sizes fed network  varying input size reconciled fixed size input layer network  words  network made flexible enough deal input might anywhere one word multiple pages text   assumption fixed number input neurons wrong new input neurons added removed network match input size see ever trained   give example nlp  lots problems inherently unpredictable input size  interested general approach dealing   edit  images  clear downsample fixed size  text seems impossible approach since adding removing text changes meaning original input  ,far tell neural network fix number neuron input layer neural network use context like exampl nlp sentenc block text vari size fed network vari input size reconcil fix size input layer network word network made flexibl enough deal input might anywher one word multipl page text assumpt fix number input neuron wrong new input neuron ad remov network match input size see ever train give exampl nlp lot problem inher unpredict input size interest general approach deal edit imag clear downsampl fix size text seem imposs approach sinc ad remov text chang mean origin input,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,implementation","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,tensorflow,reinforcement-learning,ai-design,classification,training,keras,image-recognition,backpropagation,algorithm,game-ai,recurrent-neural-networks"
912,"<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being ""primed"" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br>
 I think what we have currently is ""artificial thinking"" which is abstracted a long way from what is described above. so my question is ""are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? "" </p>
",estimation two minds manage speak dialectic series interrupts  thus one time one systems controlling master inhabits consciousness  subordinate system controls context constantly  primed  senses subordinate systems experience conscious thought process  see thinking fast slow daniel kahneman   thus thought process constantly driven one  similarly system works node community standalone thing   think currently  artificial thinking  abstracted long way described  question  artificial intelligence systems internal dialectical approach drivers conceived develop within community nodes     ,estim two mind manag speak dialect seri interrupt thus one time one system control master inhabit conscious subordin system control context constant prime sens subordin system experi conscious thought process see think fast slow daniel kahneman thus thought process constant driven one similar system work node communiti standalon thing think current artifici think abstract long way describ question artifici intellig system intern dialect approach driver conceiv develop within communiti node,philosophy,"neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,algorithm,convolutional-neural-networks,philosophy,image-recognition,game-ai,training,classification,genetic-algorithms,backpropagation,nlp"
918,"<p>In the recent PC game <em><a href=""http://www.theturingtestgame.com/"">The Turing Test</a></em>, the AI (""TOM"") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to ""<a href=""https://en.wikipedia.org/wiki/Lateral_thinking"">think laterally</a>."" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce ""ethically suboptimal"" solutions, like chopping off an arm to leave on a pressure plate.</p>

<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>
",recent pc game turing test  ai   tom   needs help ava get puzzle rooms  tom says unable solve puzzles allowed  think laterally   specifically  says would thought throw box window solve first room  creators  story goes  turned capability thinking could produce  ethically suboptimal  solutions  like chopping arm leave pressure plate   would creative puzzle solving abilities need removed ai keep results reasonable  could get benefits lateral thinking without losing arm  ,recent pc game ture test ai tom need help ava get puzzl room tom say unabl solv puzzl allow think later specif say would thought throw box window solv first room creator stori goe turn capabl think could produc ethic suboptim solut like chop arm leav pressur plate would creativ puzzl solv abil need remov ai keep result reason could get benefit later think without lose arm,"agi,problem-solving","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,philosophy,image-recognition,tensorflow,training,classification,genetic-algorithms,game-theory"
919,"<p>In the recent <a href=""https://live.newscientist.com/"" rel=""nofollow"">festival of science</a>, there was a talk given by researcher <a href=""https://live.newscientist.com/mike-cook/"" rel=""nofollow"">Mike Cook</a> about:</p>

<blockquote>
  <p><a href=""http://www.gamesbyangelina.org/"" rel=""nofollow"">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p>
</blockquote>

<p>So the aim of Angelina AI is basically to design videogames.</p>

<p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p>
",recent festival science  talk given researcher mike cook      angelina  ai game designer invented game mechanics  made games news stories  first ai enter game jam    aim angelina ai basically design videogames   briefly  exactly angelina design new games  work behind scenes  ,recent festiv scienc talk given research mike cook angelina ai game design invent game mechan made game news stori first ai enter game jam aim angelina ai basic design videogam briefli exact angelina design new game work behind scene,"algorithm,gaming","machine-learning,neural-networks,game-ai,ai-design,deep-learning,reinforcement-learning,tensorflow,philosophy,game-theory,combinatorial-games,gaming,convolutional-neural-networks,research,q-learning,strong-ai"
921,"<p>I understand that neural networks model biological neurons.  Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells.  As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such.  </p>

<p>Recent discoveries on how the brain works reveal the importance of calcium within the cells.  See <a href=""http://link.springer.com/article/10.1007/BF01794675"" rel=""noreferrer"">http://link.springer.com/article/10.1007/BF01794675</a> for more information.  To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.</p>

<p>From my study of neural networks, there does not seem to be a calcium equivalent.  Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time.   I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).</p>

<p>Does anyone know of such a network (or training algorithm)?  If so, do these networks perform better than a network that is pre-configured?</p>
",understand neural networks model biological neurons   node network represents neuron cell connections nodes represent connections cells   nature  neuron fires electrical signal connected neurons based kind threshold function mimics     recent discoveries brain works reveal importance calcium within cells   see http   link springer com article         bf         information   summarize  calcium affects regulation  stimulation transmission electrical activity well destruction neurones   study neural networks  seem calcium equivalent   one would imply functions  connections weights artificial network configured training execution process change time    understand back propagation used train weights  seen anything trains function connections  although zero weight could imply connection    anyone know network  training algorithm     networks perform better network pre configured  ,understand neural network model biolog neuron node network repres neuron cell connect node repres connect cell natur neuron fire electr signal connect neuron base kind threshold function mimic recent discoveri brain work reveal import calcium within cell see http link springer com articl bf inform summar calcium affect regul stimul transmiss electr activ well destruct neuron studi neural network seem calcium equival one would impli function connect weight artifici network configur train execut process chang time understand back propag use train weight seen anyth train function connect although zero weight could impli connect anyon know network train algorithm network perform better network pre configur,"philosophy,unsupervised-learning,neurons","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,tensorflow,classification,backpropagation,game-ai,image-recognition,genetic-algorithms,keras,algorithm"
926,"<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>

<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>
",freshman machine learning  know   kinds problems life  problems humans solve problems solve  problems humans solve  always try best write algorithm tell machine follow step step  finally machine acts like people   curious problems humans solve  humans sum get algorithm  means know solve problem   machine solve problem   machine sum get algorithm based large amount problem data  ,freshman machin learn know kind problem life problem human solv problem solv problem human solv alway tri best write algorithm tell machin follow step step final machin act like peopl curious problem human solv human sum get algorithm mean know solv problem machin solv problem machin sum get algorithm base larg amount problem data,"machine-learning,algorithm","machine-learning,neural-networks,deep-learning,reinforcement-learning,algorithm,ai-design,convolutional-neural-networks,classification,tensorflow,philosophy,training,image-recognition,game-ai,genetic-algorithms,nlp"
929,"<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>

<hr>

<p>feel free to skip the problem explanation:</p>

<blockquote>
  <p>A friend is writing a path-finding algorithm - an autopilot for an
  (off-road) vehicle in a computer game. This is a pretty classic
  problem - he finds a viable, not necessarily optimal but ""good enough""
  route using the A* algorithm, by taking the terrain layout and vehicle
  capabilities into account, and modifying a direct (straight) line path
  to account for these. The whole map is known a'priori and invariant,
  though the start and destination are arbitrary (user-chosen) and the
  path is not guaranteed to exist at all.</p>
  
  <p>This cookie-cutter approach comes with a twist: limited storage space.
  We can afford some more volatile memory on start, but we should free
  most of it once the route has been found. The travel may take days -
  of real time too, so the path must be saved to disk, and the space in
  the save file for custom data like this is severely limited. Too
  limited to save all the waypoints - even after culling trivial
  solution waypoints ('continue straight ahead'), and by a rather large
  margin, order of 20% the size of our data set.</p>
  
  <p>A solution we came up with is to calculate the route once on start,
  then 'forget' all the trivial and 90% of the non-trivial waypoints.
  This both serves as a proof that a solution exists, and provides a set
  of points reaching which, in sequence, guarantees the route will take
  us to the destination.</p>
  
  <p>Once the vehicle reaches a waypoint, the route to the next one is
  calculated again, from scratch. It's known to exist and be correct
  (because we did it once, and it was correct), it doesn't put too much
  strain on the CPU and the memory (it's only about 10% the total route
  length) and it doesn't need to go into permanent storage (restarting
  from any point along the path is just a subset of the solution
  connecting two saved waypoints).</p>
</blockquote>

<hr>

<p>Now for the actual question:</p>

<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>

<p>Is this a heuristic approach?</p>

<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>
",question nomenclature   already algorithm solution  sure whether qualifies utilizing heuristics     feel free skip problem explanation      friend writing path finding algorithm   autopilot    road  vehicle computer game  pretty classic   problem   finds viable  necessarily optimal  good enough    route using  algorithm  taking terrain layout vehicle   capabilities account  modifying direct  straight  line path   account  whole map known priori invariant    though start destination arbitrary  user chosen    path guaranteed exist       cookie cutter approach comes twist  limited storage space    afford volatile memory start  free   route found  travel may take days     real time  path must saved disk  space   save file custom data like severely limited    limited save waypoints   even culling trivial   solution waypoints   continue straight ahead    rather large   margin  order     size data set       solution came calculate route start     forget  trivial     non trivial waypoints    serves proof solution exists  provides set   points reaching  sequence  guarantees route take   us destination       vehicle reaches waypoint  route next one   calculated  scratch  known exist correct     correct   put much   strain cpu memory      total route   length  need go permanent storage  restarting   point along path subset solution   connecting two saved waypoints       actual question   pathfinding algorithm follows sparse set waypoints nearly sufficient route  allow easy  efficient  calculation actual route  simultaneously guarantying existence  subset full solution    heuristic approach    understand  normally  heuristics guarantee existence solution  merely suggest likely candidates  case   hints  taken straight actual working solution  thus doubts   ,question nomenclatur alreadi algorithm solut sure whether qualifi util heurist feel free skip problem explan friend write path find algorithm autopilot road vehicl comput game pretti classic problem find viabl necessarili optim good enough rout use algorithm take terrain layout vehicl capabl account modifi direct straight line path account whole map known priori invari though start destin arbitrari user chosen path guarante exist cooki cutter approach come twist limit storag space afford volatil memori start free rout found travel may take day real time path must save disk space save file custom data like sever limit limit save waypoint even cull trivial solut waypoint continu straight ahead rather larg margin order size data set solut came calcul rout start forget trivial non trivial waypoint serv proof solut exist provid set point reach sequenc guarante rout take us destin vehicl reach waypoint rout next one calcul scratch known exist correct correct put much strain cpu memori total rout length need go perman storag restart point along path subset solut connect two save waypoint actual question pathfind algorithm follow spars set waypoint near suffici rout allow easi effici calcul actual rout simultan guaranti exist subset full solut heurist approach understand normal heurist guarante exist solut mere suggest like candid case hint taken straight actual work solut thus doubt,"heuristics,path-planning","machine-learning,neural-networks,deep-learning,reinforcement-learning,algorithm,ai-design,convolutional-neural-networks,tensorflow,training,game-ai,image-recognition,classification,genetic-algorithms,philosophy,computer-vision"
930,"<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p>

<p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p>
",understand neural network trained recognise certain features image  faces  cars        inputs image pixels  output set boolean values indicating objects recognised image   really get  using approach detect features detect face example  go back original image determine location boundaries detected face  achieved  achieved based recognition algorithm  separate algorithm used locate face  seems unlikely since find face  needs recognised image  reason using nn first place  ,understand neural network train recognis certain featur imag face car input imag pixel output set boolean valu indic object recognis imag realli get use approach detect featur detect face exampl go back origin imag determin locat boundari detect face achiev achiev base recognit algorithm separ algorithm use locat face seem unlik sinc find face need recognis imag reason use nn first place,"neural-networks,deep-learning,convolutional-neural-networks,computer-vision","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,image-recognition,classification,reinforcement-learning,tensorflow,training,ai-design,algorithm,computer-vision,game-ai,keras,backpropagation"
933,"<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain?s architecture? What?s the constraint ?</p>
",said ai assess scenarios decide ai best suited construct new ai new tasks  sufficient time would ai developed suite ais powerful specialized tasks  versatile whole  much like brain architecture  constraint   ,said ai assess scenario decid ai best suit construct new ai new task suffici time would ai develop suit ai power special task versatil whole much like brain architectur constraint,"neural-networks,philosophy,agi","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,strong-ai,reinforcement-learning,algorithm,convolutional-neural-networks,game-theory,research,genetic-algorithms,natural-language,training"
940,"<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>
",ai progressing drastically  imagine tell fired robot take place  jobs never automated  ,ai progress drastic imagin tell fire robot take place job never autom,"philosophy,robots","machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,strong-ai,algorithm,game-theory,research,genetic-algorithms,image-recognition,natural-language,nlp"
945,"<p>I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.
As a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).
Maybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)</p>

<p>My question now is: What kind of machine learning would be best to achieve this?</p>
",want program writes like human  want font  instead  intelligent  program produce different result trained different sets generate different handwritings  training set would like parts handwritten text  saved list paths  like vector graphics   maybe means simplify things  could flatten paths consecutive straight lines  program receives string text produces list paths  vector graphic  whatever easier work   question  kind machine learning would best achieve  ,want program write like human want font instead intellig program produc differ result train differ set generat differ handwrit train set would like part handwritten text save list path like vector graphic mayb mean simplifi thing could flatten path consecut straight line program receiv string text produc list path vector graphic whatev easier work question kind machin learn would best achiev,"machine-learning,training","machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,training,tensorflow,algorithm,image-recognition,classification,game-ai,nlp,natural-language,philosophy"
946,"<p>I'm wondering how feasible it is to create a machine that can separate clothing from a basket.</p>

<p>At the most basic level it would distinguish between tops, pants, button downs and socks</p>

<p>Programmatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:</p>

<ol>
<li>pick up an item</li>
<li>lay it in such a way that is recognizable </li>
<li>deduce whether it is a top, button down, etc.</li>
<li>sort it accordingly</li>
</ol>

<p>If this sounds ridiculous please let me know...</p>

<p>If it is possible :</p>

<p>would this be based on some sort of computer vision?
or only a well trained neural network?</p>

<p>Any insight is much appreciated!</p>
",wondering feasible create machine separate clothing basket   basic level would distinguish tops  pants  button downs socks  programmatically  image would require training neural network recognize items  real time becomes exponentially difficult small space fast rate    pick item lay way recognizable  deduce whether top  button  etc  sort accordingly   sounds ridiculous please let know     possible    would based sort computer vision  well trained neural network   insight much appreciated  ,wonder feasibl creat machin separ cloth basket basic level would distinguish top pant button sock programmat imag would requir train neural network recogn item real time becom exponenti difficult small space fast rate pick item lay way recogniz deduc whether top button etc sort accord sound ridicul pleas let know possibl would base sort comput vision well train neural network insight much appreci,"neural-networks,image-recognition","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,training,image-recognition,classification,tensorflow,algorithm,game-ai,computer-vision,genetic-algorithms,recurrent-neural-networks"
947,"<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the ""DIKW pyramid"" where they add another step after knowledge, namely wisdom.
They define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p>

<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would ""wisdom"" be defined in AI? And since we have KR languages, how would we represent ""wisdom"" as they define it?</p>

<p>Any references would be welcome?</p>
",years dealing  teaching  knowledge representation knowledge representation languages  discovered another community  information systems  something called  dikw pyramid  add another step knowledge  namely wisdom  define data simply symbols  information answer   knowledge answer   wisdom answer     question  anyone done connection ai calls data information knowledge notions information systems  particular  would  wisdom  defined ai  since kr languages  would represent  wisdom  define   references would welcome  ,year deal teach knowledg represent knowledg represent languag discov anoth communiti inform system someth call dikw pyramid add anoth step knowledg name wisdom defin data simpli symbol inform answer knowledg answer wisdom answer question anyon done connect ai call data inform knowledg notion inform system particular would wisdom defin ai sinc kr languag would repres wisdom defin refer would welcom,knowledge-representation,"machine-learning,neural-networks,ai-design,deep-learning,reinforcement-learning,philosophy,algorithm,convolutional-neural-networks,image-recognition,classification,training,natural-language,game-ai,nlp,research"
954,"<p><a href=""https://en.wikipedia.org/wiki/AI_effect"" rel=""noreferrer"">According to Wikipedia</a>...</p>

<blockquote>
  <p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p>
  
  <p>Pamela McCorduck writes: ""It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something?play good checkers, solve simple but relatively informal problems?there was chorus of critics to say, 'that's not thinking'.""[1] AI researcher Rodney Brooks complains ""Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'""[2]</p>
</blockquote>

<p>The Wikipedia page then proposes several different reasons that could explain why onlookers might ""discount"" AI programs. However, those reasons seem to imply that the humans are making a mistake in ""discounting"" the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in ""discounting"" the behavior of AI programs.</p>

<p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p>

<p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p>

<p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p>

<p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p>

<p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p>

<p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define ""intelligence"", not due to people dismissing programs as not being ""intelligent"".</p>

<p>Is this argument valid or correct? And if not, why not?</p>
",according wikipedia        ai effect occurs onlookers discount behavior artificial intelligence program arguing real intelligence       pamela mccorduck writes   part history field artificial intelligence every time somebody figured make computer something play good checkers  solve simple relatively informal problems chorus critics say   thinking       ai researcher rodney brooks complains  every time figure piece  stops magical  say   oh  computation         wikipedia page proposes several different reasons could explain onlookers might  discount  ai programs  however  reasons seem imply humans making mistake  discounting  behavior ai programs   ai programs might actually  intelligent  want make alternate argument  humans making mistake   discounting  behavior ai programs   consider following situation  want build machine x  x trait  like intelligence   able evaluate intuitively whether machine x criteria  good definition x actually  identify whether something x   however  think people x  build machine  surely  built machine x   building machine  examine see machine x   machine lacks x  machine cool  really want machine x  go back drawing board think new idea reach x   writing whiteboard couple hours  realize people x z  course  try build new machine z  yes  z  must x   building machine z  check see x   return back drawing board  cycle repeats repeats     essentially  humans attempting determine whether entity intelligence via proxy measurements  proxy measurements potentially faulty  possible meet proxy measurements without ever actually intelligence   know define intelligence design test accurately measure  unlikely us build machine intelligence  ai effect occurs humans know define  intelligence   due people dismissing programs  intelligent    argument valid correct    ,accord wikipedia ai effect occur onlook discount behavior artifici intellig program argu real intellig pamela mccorduck write part histori field artifici intellig everi time somebodi figur make comput someth play good checker solv simpl relat inform problem chorus critic say think ai research rodney brook complain everi time figur piec stop magic say oh comput wikipedia page propos sever differ reason could explain onlook might discount ai program howev reason seem impli human make mistak discount behavior ai program ai program might actual intellig want make altern argument human make mistak discount behavior ai program consid follow situat want build machin x x trait like intellig abl evalu intuit whether machin x criteria good definit x actual identifi whether someth x howev think peopl x build machin sure built machin x build machin examin see machin x machin lack x machin cool realli want machin x go back draw board think new idea reach x write whiteboard coupl hour realiz peopl x z cours tri build new machin z yes z must x build machin z check see x return back draw board cycl repeat repeat essenti human attempt determin whether entiti intellig via proxi measur proxi measur potenti faulti possibl meet proxi measur without ever actual intellig know defin intellig design test accur measur unlik us build machin intellig ai effect occur human know defin intellig due peopl dismiss program intellig argument valid correct,intelligence-testing,"machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,tensorflow,reinforcement-learning,philosophy,keras,game-ai,image-recognition,algorithm,nlp,classification,q-learning"
956,"<p>Here is one of the most serious questions, about the artificial intelligence.<br>
How will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy.<br>
<br> A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.<br>
<br>It will follow the ethics of its creator, the man who originally designed it<br>
 But how to teach a computer something we don't have the right answer.<br>
 People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. 
<br>Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others.   </p>

<p>Who will learn the machine a humanity?   :)</p>
",one serious questions  artificial intelligence  machine know difference right wrong  good bad  respect  dignity  faith empathy   machine recognize correct incorrect  right wrong  depend originally designed  follow ethics creator  man originally designed  teach computer something right answer   people selfish  jealous  self confident  able understand sorrows  pains beliefs  understand different religions  different traditions beliefs   creating ai might breakthrough one nation  one race  one ethnic religious group  others      learn machine humanity       ,one serious question artifici intellig machin know differ right wrong good bad respect digniti faith empathi machin recogn correct incorrect right wrong depend origin design follow ethic creator man origin design teach comput someth right answer peopl selfish jealous self confid abl understand sorrow pain belief understand differ religion differ tradit belief creat ai might breakthrough one nation one race one ethnic religi group learn machin human,"ethics,value-alignment","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,philosophy,game-ai,algorithm,image-recognition,training,classification,tensorflow,backpropagation,computer-vision"
960,"<p>Can someone suggest step by step approach to learn AI rather than study a stack of book for long time.[ I'm not denying that books are great helper but what after that ]</p>

<p>Thanks in Advance.</p>
",someone suggest step step approach learn ai rather study stack book long time   denying books great helper    thanks advance  ,someon suggest step step approach learn ai rather studi stack book long time deni book great helper thank advanc,"ai-design,training","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,tensorflow,q-learning,convolutional-neural-networks,game-ai,algorithm,training,philosophy,backpropagation,classification,genetic-algorithms"
961,"<p><a href=""https://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt"">From this SE question</a>:</p>

<blockquote>
  <p>Will be AI able to adapt, to different environments and changes.</p>
</blockquote>

<p>This is my attempt at interpreting that question.</p>

<p>Evolutionary algorithms are useful for solving optimization problems...by measuring the ""fitness"" of various probable solutions and then  of an algorithm through the process of natural selection.</p>

<p>Suppose, the ""fitness calculation""/""environment"" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p>
",se question      ai able adapt  different environments changes    attempt interpreting question   evolutionary algorithms useful solving optimization problems   measuring  fitness  various probable solutions  algorithm process natural selection   suppose   fitness calculation   environment  changed mid training  could easily happen real life scenarios people may desire different solutions different times   would evolutionary algorithms able respond effectively change  ,se question ai abl adapt differ environ chang attempt interpret question evolutionari algorithm use solv optim problem measur fit various probabl solut algorithm process natur select suppos fit calcul environ chang mid train could easili happen real life scenario peopl may desir differ solut differ time would evolutionari algorithm abl respond effect chang,evolutionary-algorithms,"machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,convolutional-neural-networks,game-ai,training,genetic-algorithms,classification,image-recognition,tensorflow,philosophy,nlp"
966,"<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p>

<p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p>

<p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p>

<p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p>
",propose strategy ask strategy tested genetic algorithms past  exactly know find discussion   classic example genetic algorithm would population certain amount simulation time evaluate breeding  proceed next generation   would isolate small part population simulation process keep evolving little island time rest population continues evolve normally  could united rest population end simulation would go trough  breed population continue    super important part natural evolution probably know actually works genetic programming  ,propos strategi ask strategi test genet algorithm past exact know find discuss classic exampl genet algorithm would popul certain amount simul time evalu breed proceed next generat would isol small part popul simul process keep evolv littl island time rest popul continu evolv normal could unit rest popul end simul would go trough breed popul continu super import part natur evolut probabl know actual work genet program,genetic-algorithms,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,algorithm,genetic-algorithms,classification,game-ai,game-theory,tensorflow,philosophy,training,image-recognition"
970,"<p>Obviously this is hypothetical, but is true? I know ""perfect fitness function"" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p>
",obviously hypothetical  true  know  perfect fitness function  bit hand wavy  mean perfect way measure completion problem  ,obvious hypothet true know perfect fit function bit hand wavi mean perfect way measur complet problem,genetic-programming,"neural-networks,machine-learning,deep-learning,reinforcement-learning,ai-design,convolutional-neural-networks,algorithm,genetic-algorithms,training,classification,tensorflow,game-ai,image-recognition,game-theory,deep-network"
974,"<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>

<p>I mainly know of AI being used in games or robotics fields. But can it be useful in ""standard"" application development?</p>
",curious artificial intelligence  everyday job develop standard applications  like websites basic functionalities like user subscription  file upload  forms saved database      mainly know ai used games robotics fields  useful  standard  application development  ,curious artifici intellig everyday job develop standard applic like websit basic function like user subscript file upload form save databas main know ai use game robot field use standard applic develop,applications,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,training,tensorflow,image-recognition,philosophy,classification,research,game-theory"
981,"<p>I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner?  Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? </p>
",heard ai solve math problems  possible create  logic system  equivalent humans solve mathematics called  beautiful  manner   ai find beauty mathematics solve problems using brute force  please provide examples work done   ,heard ai solv math problem possibl creat logic system equival human solv mathemat call beauti manner ai find beauti mathemat solv problem use brute forc pleas provid exampl work done,research,"machine-learning,neural-networks,deep-learning,ai-design,algorithm,reinforcement-learning,game-ai,convolutional-neural-networks,philosophy,image-recognition,training,classification,tensorflow,genetic-algorithms,strong-ai"
984,"<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p>
",trying gain intuition beyond definitions  possible dimension  appreciate references read  ,tri gain intuit beyond definit possibl dimens appreci refer read,"machine-learning,models","machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,algorithm,game-ai,image-recognition,classification,philosophy,tensorflow,research,training,artificial-neuron"
985,"<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the ""highest level of abstraction"" that we've observed?</p>
",seems deep neural networks making improvements largely add nodes connections  able put together abstract concepts  know  starting pixels  start recognize high level objects like cat faces  chairs  written words  network ever shown learned abstract concept physical object   highest level abstraction  observed  ,seem deep neural network make improv larg add node connect abl put togeth abstract concept know start pixel start recogn high level object like cat face chair written word network ever shown learn abstract concept physic object highest level abstract observ,deep-learning,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,tensorflow,image-recognition,algorithm,game-ai,training,classification,backpropagation,genetic-algorithms,computer-vision"
988,"<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>

<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>
",bit confused definition life  ai systems called  living   things  even communicate one another    formed call cells   see  cells collection several chemical processes turn non living like ai formed several lines code  ,bit confus definit life ai system call live thing even communic one anoth form call cell see cell collect sever chemic process turn non live like ai form sever line code,"research,philosophy","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,image-recognition,algorithm,training,strong-ai,tensorflow,genetic-algorithms,game-theory"
992,"<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>

<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>

<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>

<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>

<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>
",interested mostly application ai gaming  case adjusts way answer  general answers welcome well   reading neural networks combining genetic algorithms  high level understanding neural networks used produce result inputs  genetic algorithm employed constantly adjust weights neural network good answer found   concept genetic algorithm randomly mutating weights inputs neural network makes sense  understand would applied respect gaming   example  simple enemy ai want adapt players play style  good opportunity implement ai genetic algorithm combined neural network   different suitable applications  one go deciding encode problem way mutated genetic algorithm serve suitable inputs neural network  actually  neural networks always designed signals    ,interest applic ai game case adjust way answer general answer welcom well read neural network combin genet algorithm high level understand neural network use produc result input genet algorithm employ constant adjust weight neural network good answer found concept genet algorithm random mutat weight input neural network make sens understand would appli respect game exampl simpl enemi ai want adapt player play style good opportun implement ai genet algorithm combin neural network differ suitabl applic one go decid encod problem way mutat genet algorithm serv suitabl input neural network actual neural network alway design signal,"neural-networks,gaming,genetic-algorithms","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,game-ai,training,algorithm,genetic-algorithms,classification,tensorflow,backpropagation,image-recognition,recurrent-neural-networks"
993,"<p>I have seen an AI create a game it self, AI act as a lawyer, call center etc.</p>

<p>There are many problems (Example for mobile development)</p>

<pre><code>1. New api/technology or even new language every year.
2. New design
3. New hardware
4. Good code architecture, design pattern
5. Security
6. Image/Animation optimization
7. Automate testing
</code></pre>

<p>etc.</p>

<p>I wonder that AI can help developer solve that problems.</p>

<p>1.1 May be I want to get the location then AI suggest the best api for specific platform.</p>

<p>1.2 AI help to refactoring and optimizing the code</p>

<ol start=""2"">
<li><p>Help on design e.g. golden ratio, Material theme color</p></li>
<li><p>Suggest or determine the limit of the hardware e.g. screen size, ram</p></li>
<li><p>Can convert to another design pattern </p></li>
<li><p>Help to waring the latest vulnerable and automate pentest etc.</p></li>
<li><p>Help to optimize image by learning how much can we reduce the image size while people still ok with it.</p></li>
<li><p>Generate automate-testing</p></li>
</ol>

<p>Is there any solution existed?</p>

<p>If not, what can we do?</p>
",seen ai create game self  ai act lawyer  call center etc   many problems  example mobile development      new api technology even new language every year     new design    new hardware    good code architecture  design pattern    security    image animation optimization    automate testing   etc   wonder ai help developer solve problems       may want get location ai suggest best api specific platform       ai help refactoring optimizing code   help design e g  golden ratio  material theme color suggest determine limit hardware e g  screen size  ram convert another design pattern  help waring latest vulnerable automate pentest etc  help optimize image learning much reduce image size people still ok  generate automate testing   solution existed     ,seen ai creat game self ai act lawyer call center etc mani problem exampl mobil develop new api technolog even new languag everi year new design new hardwar good code architectur design pattern secur imag anim optim autom test etc wonder ai help develop solv problem may want get locat ai suggest best api specif platform ai help refactor optim code help design e g golden ratio materi theme color suggest determin limit hardwar e g screen size ram convert anoth design pattern help ware latest vulner autom pentest etc help optim imag learn much reduc imag size peopl still ok generat autom test solut exist,"intelligence-testing,security","machine-learning,neural-networks,deep-learning,ai-design,convolutional-neural-networks,reinforcement-learning,image-recognition,tensorflow,game-ai,algorithm,classification,training,philosophy,computer-vision,genetic-algorithms"
994,"<p>There are AI creating game, content and more.</p>

<p>I'm thinking on how can AI develop mobile app itself?</p>

<p>The computer languages might easy for AI to learn.</p>

<p>AI can learn a lot from good open source project in github.</p>

<p>The trend prediction can help AI to select the topic for creating a great apps.</p>

<p>There are lots of details to let AI create a great apps. </p>
",ai creating game  content   thinking ai develop mobile app   computer languages might easy ai learn   ai learn lot good open source project github   trend prediction help ai select topic creating great apps   lots details let ai create great apps   ,ai creat game content think ai develop mobil app comput languag might easi ai learn ai learn lot good open sourc project github trend predict help ai select topic creat great app lot detail let ai creat great app,computer-programming,"machine-learning,neural-networks,ai-design,deep-learning,game-ai,reinforcement-learning,philosophy,strong-ai,algorithm,convolutional-neural-networks,game-theory,genetic-algorithms,tensorflow,research,training"
997,"<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p>

<p>And was wondering if there are any simple data sets to test a MLP ANN ?
i.e. small number of inputs and outputs</p>

<p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p>

<p>Maybe start slightly more complex than XOR ?</p>
",new topic  think figured implement multi level perceptron mlp  ann   wondering simple data sets test mlp ann   e  small number inputs outputs  getting expected results uci cancer  hoping someone could save time point data used    maybe start slightly complex xor   ,new topic think figur implement multi level perceptron mlp ann wonder simpl data set test mlp ann e small number input output get expect result uci cancer hope someon could save time point data use mayb start slight complex xor,neural-networks,"neural-networks,machine-learning,deep-learning,convolutional-neural-networks,ai-design,reinforcement-learning,tensorflow,training,classification,algorithm,game-ai,backpropagation,image-recognition,keras,genetic-algorithms"
999,"<p>The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?</p>

<p>I am just looking for a clear definition of what most people have in mind when they refer to AI.</p>
",concept intrinsically related building sort media ai exists  may think digital computer  programmed use language act way cannot distinguished human   media really mater  unconventional computation paradigms   certain control limits ai matter  synthetic biology ultimate goal building biological systems scratch   would synthetic brain  potentially introduced synthetic human  constitute ai   looking clear definition people mind refer ai  ,concept intrins relat build sort media ai exist may think digit comput program use languag act way distinguish human media realli mater unconvent comput paradigm certain control limit ai matter synthet biolog ultim goal build biolog system scratch would synthet brain potenti introduc synthet human constitut ai look clear definit peopl mind refer ai,definitions,"machine-learning,neural-networks,ai-design,deep-learning,philosophy,game-ai,reinforcement-learning,algorithm,convolutional-neural-networks,strong-ai,image-recognition,natural-language,game-theory,genetic-algorithms,research"
1000,"<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>

<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>
",autonomous cars related artificial intelligence  would presume artificial intelligence able copy human state mind perform tasks way  autonomous car rule based machines operates due environment  self aware  cannot choose good way act never experienced situation   know many people often mention autonomous cars speaking ai  really convinced related  either strict understanding ai  ,autonom car relat artifici intellig would presum artifici intellig abl copi human state mind perform task way autonom car rule base machin oper due environ self awar choos good way act never experienc situat know mani peopl often mention autonom car speak ai realli convinc relat either strict understand ai,"self-driving,strong-ai,cars,weak-ai","machine-learning,neural-networks,ai-design,deep-learning,philosophy,reinforcement-learning,game-ai,algorithm,convolutional-neural-networks,image-recognition,strong-ai,training,genetic-algorithms,game-theory,natural-language"
1001,"<p>What are the advantages of having self-driving cars?</p>

<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>

<p>Are we really interested in this?</p>
",advantages self driving cars   able cars traffic time  also make people choose use cars  traffic public health actually become worse   really interested  ,advantag self drive car abl car traffic time also make peopl choos use car traffic public health actual becom wors realli interest,"research,self-driving,cars","neural-networks,machine-learning,deep-learning,ai-design,convolutional-neural-networks,algorithm,reinforcement-learning,image-recognition,training,game-ai,genetic-algorithms,tensorflow,philosophy,self-driving,classification"
1004,"<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>

<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>
",lots sci fi  seems ai becomes sentient  terminator  peter f hamilton si  commonwealth saga   etc    however  interested whether actually plausible  whether ai could actually break free form controlled us  possible  whether research sort complexity   processing power ai would need able  ,lot sci fi seem ai becom sentient termin peter f hamilton si commonwealth saga etc howev interest whether actual plausibl whether ai could actual break free form control us possibl whether research sort complex process power ai would need abl,"machine-learning,deep-learning","machine-learning,neural-networks,ai-design,deep-learning,game-ai,philosophy,reinforcement-learning,convolutional-neural-networks,algorithm,strong-ai,game-theory,image-recognition,genetic-algorithms,research,training"
1014,"<p>Deepmind just published a <a href=""http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html"" rel=""noreferrer"">paper</a> about a <a href=""https://deepmind.com/blog/differentiable-neural-computers/"" rel=""noreferrer"">""differentiable neural computer""</a>, which basically <em>combines a neural network with a memory</em>. </p>

<p>The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (<a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""noreferrer"">LSTMs</a> are one try to slow down this degradation of short term memories, but it still happens.)</p>

<p>Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation. My question is why this approach should scale. Shouldn't a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation?</p>
",deepmind published paper  differentiable neural computer   basically combines neural network memory    idea teach neural network create recall useful explicit memories certain task  complements abilities neural network well  nns store knowledge implicitly weights information used work single task stored activation network degrades quickly information add   lstms one try slow degradation short term memories  still happens     instead keeping necessary information activation  presumably keep addresses memory slots specific information activation  also subject degradation  question approach scale  somewhat higher number task specific information overwhelm networks capability keeping addresses appropriate memory slots activation  ,deepmind publish paper differenti neural comput basic combin neural network memori idea teach neural network creat recal use explicit memori certain task complement abil neural network well nns store knowledg implicit weight inform use work singl task store activ network degrad quick inform add lstms one tri slow degrad short term memori still happen instead keep necessari inform activ presum keep address memori slot specif inform activ also subject degrad question approach scale somewhat higher number task specif inform overwhelm network capabl keep address appropri memori slot activ,"deep-learning,ai-design","neural-networks,machine-learning,deep-learning,convolutional-neural-networks,reinforcement-learning,ai-design,training,classification,tensorflow,image-recognition,game-ai,backpropagation,keras,deep-network,genetic-algorithms"
1015,"<p>What could be an algorithm that determines whether an AI ( algorithm ) is 
AI Complete or not ?
How does one proceed to program it ?</p>

<p>edit : question edited due to some misinterpretation in the first answer !</p>
",could algorithm determines whether ai   algorithm    ai complete   one proceed program    edit   question edited due misinterpretation first answer   ,could algorithm determin whether ai algorithm ai complet one proceed program edit question edit due misinterpret first answer,"machine-learning,deep-learning,learning-theory,incompleteness-theorems","machine-learning,neural-networks,ai-design,deep-learning,algorithm,philosophy,game-ai,reinforcement-learning,convolutional-neural-networks,genetic-algorithms,nlp,strong-ai,image-recognition,classification,training"
1022,"<p><a href=""https://www.national.co.uk/tech-powers-google-car/"" rel=""nofollow"">This slideshow</a> documents some of the technologies used in Google's self-driving car.</p>

<p>It mentions radar.</p>

<p>Why does Google use radar? Doesn't LIDAR do everything radar can do? In particular, are there technical advantages with radar regarding object detection and tracking?</p>

<p>To clarify the relationship with AI: how do radar sensors contribute to self-driving algorithms in ways that LIDAR sensors do not?</p>

<p>The premise is AI algorithms are influenced by inputs, which are governed by sensors. For instance, if self-driving cars relied solely on cameras, this constraint would alter their AI algorithms and performance.</p>
",slideshow documents technologies used google self driving car   mentions radar   google use radar  lidar everything radar  particular  technical advantages radar regarding object detection tracking   clarify relationship ai  radar sensors contribute self driving algorithms ways lidar sensors   premise ai algorithms influenced inputs  governed sensors  instance  self driving cars relied solely cameras  constraint would alter ai algorithms performance  ,slideshow document technolog use googl self drive car mention radar googl use radar lidar everyth radar particular technic advantag radar regard object detect track clarifi relationship ai radar sensor contribut self drive algorithm way lidar sensor premis ai algorithm influenc input govern sensor instanc self drive car reli sole camera constraint would alter ai algorithm perform,cars,"neural-networks,machine-learning,deep-learning,ai-design,algorithm,reinforcement-learning,training,game-ai,convolutional-neural-networks,image-recognition,philosophy,genetic-algorithms,classification,tensorflow,strong-ai"
1025,"<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>

<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>

<p>Thanks. </p>
",sometimes  always commercialization technology  low hanging fruits early applications  trouble coming examples applications would apply conscious ai   per conscious would propose expanded strict definition  state awake aware one surroundings along capability self aware   thanks   ,sometim alway commerci technolog low hang fruit earli applic troubl come exampl applic would appli conscious ai per conscious would propos expand strict definit state awak awar one surround along capabl self awar thank,"object-recognition,pattern-recognition","neural-networks,machine-learning,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,training,philosophy,classification,image-recognition,game-theory,genetic-algorithms,tensorflow"
1031,"<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>
",machine learning allows system self automated sense predict future state based learned far  question  machine learning techniques way making system develop domain knowledge  ,machin learn allow system self autom sens predict futur state base learn far question machin learn techniqu way make system develop domain knowledg,"machine-learning,knowledge-representation","machine-learning,neural-networks,deep-learning,reinforcement-learning,ai-design,algorithm,convolutional-neural-networks,training,tensorflow,game-ai,classification,image-recognition,philosophy,nlp,self-learning"
1033,"<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a $1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a $1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p>

<p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p>
",age spiritual machines         ray kurzweil predicted             computing device would able perform trillion operations per second  additionally  claimed             computing device would approximately equal computational ability human brain  due moore law exponential growth    kurzweil first prediction come true  pace second prediction come true   many years  ,age spiritu machin ray kurzweil predict comput devic would abl perform trillion oper per second addit claim comput devic would approxim equal comput abil human brain due moor law exponenti growth kurzweil first predict come true pace second predict come true mani year,hypercomputation,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,convolutional-neural-networks,game-theory,philosophy,game-ai,algorithm,decision-theory,image-recognition,classification,training,tensorflow"
1038,"<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>

<p><a href=""https://i.stack.imgur.com/aQ61J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aQ61J.png"" alt=""enter image description here""></a> </p>

<p>The AI snakes must meet the following requirements:  </p>

<ul>
<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>
<li>The AI snakes must move on a sphere</li>
</ul>

<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>
",creating snake game unity would like implement ai snakes wander around globe avoiding collision snakes globe  possible would also like make ai snakes purposefully trap snakes snakes would collide die       ai snakes must meet following requirements      must move certain way  snake controlled user using arrow keys keyboard  therefor would also like ai snakes move using form input  ai snakes must move sphere   know  creating artificial intelligence easy task would like know open source projects use accomplishing task  ,creat snake game uniti would like implement ai snake wander around globe avoid collis snake globe possibl would also like make ai snake purpos trap snake snake would collid die ai snake must meet follow requir must move certain way snake control user use arrow key keyboard therefor would also like ai snake move use form input ai snake must move sphere know creat artifici intellig easi task would like know open sourc project use accomplish task,gaming,"machine-learning,neural-networks,deep-learning,ai-design,reinforcement-learning,game-ai,convolutional-neural-networks,algorithm,genetic-algorithms,philosophy,training,image-recognition,tensorflow,classification,game-theory"
